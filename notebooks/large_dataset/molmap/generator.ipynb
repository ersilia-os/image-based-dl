{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolMap with generator (dual-path approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Set the file name of your data'\n",
    "data_name = \"CYP450.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../../../data/\" + data_name, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up (adapt according to your data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inactive (0): 7429\n",
      "Active (1): 2621\n"
     ]
    }
   ],
   "source": [
    "# keep desired columns\n",
    "data = data[['smiles', 'label_2c9']]\n",
    "\n",
    "# drop molecules with NaN activity\n",
    "data = data.dropna(subset = [\"label_2c9\"])\n",
    "\n",
    "# set SMILES\n",
    "smi = data['smiles'].tolist()\n",
    "\n",
    "# set Y\n",
    "Y = pd.get_dummies(data['label_2c9']).values\n",
    "\n",
    "# number of active and inactive molecules\n",
    "print(\"Inactive (0):\", Y[:,1].tolist().count(0))\n",
    "print(\"Active (1):\", Y[:,1].tolist().count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do MolMap: from MolDs & FFs to Fmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from molmap import MolMap\n",
    "from molmap import feature\n",
    "from ondiskxy import MatrixWriter\n",
    "from ondiskxy.utils import chunker\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "dir_path_X1 = \"../../../files/\" + data_name + \"/molmap/X1/X1\"\n",
    "dir_path_X2 = \"../../../files/\" + data_name + \"/molmap/X2/X2\"\n",
    "\n",
    "if not path.isdir(dir_path_X1) or not path.isdir(dir_path_X2):\n",
    "    'If it doesn´t exist, we compute it and save it to disk.'\n",
    "    # compute MolDs\n",
    "    mp1 = MolMap(ftype='descriptor', metric='cosine',)\n",
    "    mp1.fit(verbose=0, method='umap', min_dist=0.1, n_neighbors=15)\n",
    "    \n",
    "    # compute FFs\n",
    "    bitsinfo = feature.fingerprint.Extraction().bitsinfo\n",
    "    flist = bitsinfo[bitsinfo.Subtypes.isin(['PubChemFP', 'MACCSFP', 'PharmacoErGFP'])].IDs.tolist()\n",
    "    mp2 = MolMap(ftype = 'fingerprint', fmap_type = 'scatter', flist = flist) \n",
    "    mp2.fit(method = 'umap',  min_dist = 0.1, n_neighbors = 15, verbose = 0)\n",
    "\n",
    "    # get Fmaps and save them in chunks to disk \n",
    "    writer = MatrixWriter(dir_path=dir_path_X1, max_file_size_mb=1000)\n",
    "    for smiles_chunk in chunker(smi, 10000):\n",
    "        X1 = mp1.batch_transform(smiles_chunk)\n",
    "        writer.append(X1)\n",
    "        \n",
    "    writer = MatrixWriter(dir_path=dir_path_X2, max_file_size_mb=1000)\n",
    "    for smiles_chunk in chunker(smi, 10000):\n",
    "        X2 = mp2.batch_transform(smiles_chunk)\n",
    "        writer.append(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get indices for train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8040 1005 1005\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../src\")\n",
    "from utils import Rdsplit\n",
    "\n",
    "train_idx, valid_idx, test_idx = Rdsplit(data, random_state = 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get X (train, val and test)**\n",
    "\n",
    "First, if we don't have our X1 and X2 matrices saved in one single file in disk, we do an assembler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler\n",
    "from ondiskxy.hdf5 import Hdf5Assembler\n",
    "\n",
    "path_X1 = \"../../../files/\" + data_name + \"/molmap/X1/X1.h5\"\n",
    "path_X2 = \"../../../files/\" + data_name + \"/molmap/X2/X2.h5\"\n",
    "\n",
    "if not path.isfile(path_X1) or not path.isfile(path_X2):\n",
    "    'If it doesn´t exist, we compute it and save it to disk.'\n",
    "    # assembler X1\n",
    "    assembler = Hdf5Assembler(dir_path_X1)\n",
    "    assembler.assemble(path_X1)\n",
    "    \n",
    "    # assembler X2\n",
    "    assembler = Hdf5Assembler(dir_path_X2)\n",
    "    assembler.assemble(path_X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we access to this matrices X1 and X2 and we keep only the needed indexes for computing our train, val and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_by_index \n",
    "from ondiskxy import filter_by_index\n",
    "\n",
    "path_trainX1 = \"../../../files/\" + data_name + \"/molmap/X1/trainX1.h5\"\n",
    "path_trainX2 = \"../../../files/\" + data_name + \"/molmap/X2/trainX2.h5\"\n",
    "path_validX1 = \"../../../files/\" + data_name + \"/molmap/X1/validX1.h5\"\n",
    "path_validX2 = \"../../../files/\" + data_name + \"/molmap/X2/validX2.h5\"\n",
    "path_testX1 = \"../../../files/\" + data_name + \"/molmap/X1/testX1.h5\"\n",
    "path_testX2 = \"../../../files/\" + data_name + \"/molmap/X2/testX2.h5\"\n",
    "\n",
    "if not path.isfile(path_trainX1):\n",
    "    'If it doesn´t exist, we compute it and save it to disk.' \n",
    "    filter_by_index(train_idx, path_X1, path_trainX1)\n",
    "    filter_by_index(train_idx, path_X2, path_trainX2)\n",
    "    filter_by_index(valid_idx, path_X1, path_validX1)\n",
    "    filter_by_index(valid_idx, path_X2, path_validX2)\n",
    "    filter_by_index(test_idx, path_X1, path_testX1)\n",
    "    filter_by_index(test_idx, path_X2, path_testX2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Y (train, val and test)**\n",
    "\n",
    "(obtained directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = Y[train_idx]\n",
    "validY = Y[valid_idx]\n",
    "testY = Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(path_validX1, \"r\") as f:\n",
    "    validX1 = np.array(f[\"data\"])\n",
    "    \n",
    "with h5py.File(path_validX2, \"r\") as f:\n",
    "    validX2 = np.array(f[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X1: (37, 37, 13)\n",
      "dim X1: (37, 37)\n",
      "n channels X1: 13\n",
      "shape X2: (72, 72, 3)\n",
      "dim X2: (72, 72)\n",
      "n channels X2: 3\n",
      "n classes: 2\n"
     ]
    }
   ],
   "source": [
    "shapeX1 = validX1.shape[1:]\n",
    "dimX1 = shapeX1[:-1]\n",
    "nchannelsX1 = shapeX1[-1]\n",
    "\n",
    "shapeX2 = validX2.shape[1:]\n",
    "dimX2 = shapeX2[:-1]\n",
    "nchannelsX2 = shapeX2[-1]\n",
    "\n",
    "nclasses = validY.shape[1]\n",
    "\n",
    "print(\"shape X1:\", shapeX1)\n",
    "print(\"dim X1:\", dimX1)\n",
    "print(\"n channels X1:\", nchannelsX1)\n",
    "print(\"shape X2:\", shapeX2)\n",
    "print(\"dim X2:\", dimX2)\n",
    "print(\"n channels X2:\", nchannelsX2)\n",
    "print(\"n classes:\", nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get generators for train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import Hdf5Generator\n",
    "\n",
    "XY_train_generator = Hdf5Generator(path_trainX1, dimX1, nchannelsX1, path_trainX2, dimX2, nchannelsX2, trainY[:,1], nclasses, batch_size=32, sample_weight=True, shuffle=True)\n",
    "XY_valid_generator = Hdf5Generator(path_validX1, dimX1, nchannelsX1, path_validX2, dimX2, nchannelsX2, validY[:,1], nclasses, batch_size=32, shuffle=True)\n",
    "XY_test_generator = Hdf5Generator(path_testX1, dimX1, nchannelsX1, path_testX2, dimX2, nchannelsX2, testY[:,1], nclasses, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build model (MultiClassEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClassEstimator_largedata(callbacks_path='../../../files/CYP450.csv.gz/molmap/checkpoints.h5',\n",
      "                              epochs=100, fmap_shape1=(37, 37, 13),\n",
      "                              fmap_shape2=(72, 72, 3), gpuid='0', n_outputs=2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../../../tools/bidd-molmap/molmap/model\")\n",
    "from model import MultiClassEstimator_largedata\n",
    "\n",
    "clf = MultiClassEstimator_largedata(n_outputs=trainY.shape[1], \n",
    "                          fmap_shape1 = shapeX1,\n",
    "                          fmap_shape2 = shapeX2,\n",
    "                          metric='ROC', \n",
    "                          dense_layers = [128, 64],  gpuid = 0, epochs = 100,\n",
    "                          callbacks_path = \"../../../files/\" + data_name + \"/molmap/checkpoints.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fb3ec29f598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48702, saving model to ../../../files/CYP450.csv.gz/molmap/checkpoints.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48702 to 0.39244, saving model to ../../../files/CYP450.csv.gz/molmap/checkpoints.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.39244\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.39244\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.39244 to 0.37524, saving model to ../../../files/CYP450.csv.gz/molmap/checkpoints.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.37524 to 0.37139, saving model to ../../../files/CYP450.csv.gz/molmap/checkpoints.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.37139\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.37139\n"
     ]
    }
   ],
   "source": [
    "from molmap.model import save_model, load_model\n",
    "\n",
    "dir_path_model = \"../../../files/\" + data_name + \"/molmap/model\"\n",
    "path_history = \"../../../files/\" + data_name + \"/molmap/history.pkl\"\n",
    "\n",
    "if path.isdir(dir_path_model):\n",
    "    with open(path_history, 'rb') as f:\n",
    "        history_dict = pickle.load(f)\n",
    "    clf = load_model(dir_path_model)\n",
    "\n",
    "else: \n",
    "    history_clf = clf.fit(XY_train_generator, XY_valid_generator) \n",
    "    save_model(clf, dir_path_model)\n",
    "    history_dict = history_clf.history\n",
    "    with open(path_history, 'wb') as f:\n",
    "        pickle.dump(history_dict, f)\n",
    "\n",
    "# history_clf = clf.fit(XY_train_generator, XY_valid_generator) \n",
    "# history_dict = history_clf.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_dict)[['accuracy', 'val_accuracy']].plot(title=\"Performance Learning Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_dict)[['loss', 'val_loss']].plot(title=\"Optimization Learning Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predict probabilities\n",
    "df_pred = pd.DataFrame([testY[:, 1], clf.predict_proba(testX)[:,1]]).T\n",
    "df_pred.columns=['y_true', 'y_pred_prob']\n",
    "\n",
    "# distributions for each class\n",
    "dist0 = df_pred[df_pred['y_true'] == 0]['y_pred_prob'].tolist()\n",
    "dist1 = df_pred[df_pred['y_true'] == 1]['y_pred_prob'].tolist()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title('Predicted probabilities distribution for each class')\n",
    "plt.hist(dist0,histtype='step', label=\"Inactive (0)\", color='r')  \n",
    "plt.hist(dist1,histtype='step', label=\"Active (1)\", color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(df_pred['y_true'], df_pred['y_pred_prob'], n_bins=10)\n",
    "\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', c='cadetblue')\n",
    "\n",
    "# plot model reliability\n",
    "plt.title('Calibration curve')\n",
    "plt.plot(mpv, fop, marker='.', c='deeppink')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on test set: AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = clf.score(testX, testY) \n",
    "print(round(auc,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, threshold = metrics.roc_curve(df_pred['y_true'], df_pred['y_pred_prob'])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(XY_test_generator)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b953d02031940799c70de464297ad7062f1169bc628a7b2471c13167aae5287"
  },
  "kernelspec": {
   "display_name": "molmap",
   "language": "python",
   "name": "molmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
