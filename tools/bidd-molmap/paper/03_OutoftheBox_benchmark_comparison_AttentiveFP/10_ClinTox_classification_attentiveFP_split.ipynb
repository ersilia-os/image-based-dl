{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [23:50:32] Enabling RDKit 2019.09.2 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attentiveFP_idx(df):\n",
    "    \"\"\" attentiveFP dataset\"\"\"\n",
    "    train, valid,test = load('./split_and_data/10_ClinTox_attentiveFP.data')\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train), len(valid), len(test)))\n",
    "    train_idx = df[df.smiles.isin(train.smiles)].index\n",
    "    valid_idx = df[df.smiles.isin(valid.smiles)].index\n",
    "    test_idx = df[df.smiles.isin(test.smiles)].index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: ClinTox number of split times: 3\n",
      "training set: 1180, valid set: 148, test set 148\n",
      "training set: 1180, valid set: 148, test set 148\n"
     ]
    }
   ],
   "source": [
    "task_name = 'ClinTox'\n",
    "\n",
    "from chembench import load_data\n",
    "df, _ = load_data(task_name)\n",
    "\n",
    "train_idx, valid_idx, test_idx = get_attentiveFP_idx(df) \n",
    "len(train_idx), len(valid_idx), len(test_idx)\n",
    "\n",
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_dir = '../02_OutofTheBox_benchmark_comparison_DMPNN/tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)\n",
    "\n",
    "\n",
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [128] #2 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 148 148\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "trainX = (X1[train_idx], X2[train_idx])\n",
    "trainY = Y[train_idx]\n",
    "\n",
    "validX = (X1[valid_idx], X2[valid_idx])\n",
    "validY = Y[valid_idx]\n",
    "\n",
    "testX = (X1[test_idx], X2[test_idx])\n",
    "testY = Y[test_idx]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, loss: 0.6764 - val_loss: 0.7995; auc: 0.7247 - val_auc: 0.6461                                                                                                    \n",
      "epoch: 0002, loss: 0.6554 - val_loss: 0.8018; auc: 0.7581 - val_auc: 0.6661                                                                                                    \n",
      "epoch: 0003, loss: 0.6333 - val_loss: 0.7836; auc: 0.7704 - val_auc: 0.6883                                                                                                    \n",
      "epoch: 0004, loss: 0.6206 - val_loss: 0.7815; auc: 0.7886 - val_auc: 0.7032                                                                                                    \n",
      "epoch: 0005, loss: 0.6048 - val_loss: 0.7850; auc: 0.8001 - val_auc: 0.7188                                                                                                    \n",
      "epoch: 0006, loss: 0.5939 - val_loss: 0.8174; auc: 0.8146 - val_auc: 0.7245                                                                                                    \n",
      "epoch: 0007, loss: 0.5811 - val_loss: 0.7388; auc: 0.8143 - val_auc: 0.7339                                                                                                    \n",
      "epoch: 0008, loss: 0.5769 - val_loss: 0.8113; auc: 0.8265 - val_auc: 0.7344                                                                                                    \n",
      "epoch: 0009, loss: 0.5590 - val_loss: 0.7839; auc: 0.8252 - val_auc: 0.7346                                                                                                    \n",
      "epoch: 0010, loss: 0.5703 - val_loss: 0.7241; auc: 0.8214 - val_auc: 0.7329                                                                                                    \n",
      "epoch: 0011, loss: 0.5548 - val_loss: 0.9026; auc: 0.8380 - val_auc: 0.7310                                                                                                    \n",
      "epoch: 0012, loss: 0.5545 - val_loss: 0.7471; auc: 0.8345 - val_auc: 0.7334                                                                                                    \n",
      "epoch: 0013, loss: 0.5295 - val_loss: 0.7593; auc: 0.8424 - val_auc: 0.7308                                                                                                    \n",
      "epoch: 0014, loss: 0.5170 - val_loss: 0.7598; auc: 0.8488 - val_auc: 0.7342                                                                                                    \n",
      "epoch: 0015, loss: 0.5250 - val_loss: 0.8073; auc: 0.8577 - val_auc: 0.7329                                                                                                    \n",
      "epoch: 0016, loss: 0.5100 - val_loss: 0.8822; auc: 0.8652 - val_auc: 0.7320                                                                                                    \n",
      "epoch: 0017, loss: 0.5096 - val_loss: 0.7100; auc: 0.8595 - val_auc: 0.7362                                                                                                    \n",
      "epoch: 0018, loss: 0.5113 - val_loss: 0.8410; auc: 0.8761 - val_auc: 0.7408                                                                                                    \n",
      "epoch: 0019, loss: 0.4788 - val_loss: 0.7465; auc: 0.8736 - val_auc: 0.7391                                                                                                    \n",
      "epoch: 0020, loss: 0.4800 - val_loss: 0.7241; auc: 0.8760 - val_auc: 0.7413                                                                                                    \n",
      "epoch: 0021, loss: 0.4672 - val_loss: 0.9216; auc: 0.8898 - val_auc: 0.7426                                                                                                    \n",
      "epoch: 0022, loss: 0.4609 - val_loss: 0.7610; auc: 0.8902 - val_auc: 0.7443                                                                                                    \n",
      "epoch: 0023, loss: 0.4568 - val_loss: 0.6988; auc: 0.8882 - val_auc: 0.7470                                                                                                    \n",
      "epoch: 0024, loss: 0.4790 - val_loss: 1.0209; auc: 0.9092 - val_auc: 0.7482                                                                                                    \n",
      "epoch: 0025, loss: 0.4549 - val_loss: 0.7483; auc: 0.9040 - val_auc: 0.7497                                                                                                    \n",
      "epoch: 0026, loss: 0.4239 - val_loss: 0.7935; auc: 0.9092 - val_auc: 0.7565                                                                                                    \n",
      "epoch: 0027, loss: 0.4116 - val_loss: 0.7603; auc: 0.9114 - val_auc: 0.7578                                                                                                    \n",
      "epoch: 0028, loss: 0.4060 - val_loss: 0.7499; auc: 0.9145 - val_auc: 0.7597                                                                                                    \n",
      "epoch: 0029, loss: 0.3936 - val_loss: 0.7818; auc: 0.9213 - val_auc: 0.7620                                                                                                    \n",
      "epoch: 0030, loss: 0.3893 - val_loss: 0.8007; auc: 0.9286 - val_auc: 0.7644                                                                                                    \n",
      "epoch: 0031, loss: 0.3771 - val_loss: 0.8991; auc: 0.9340 - val_auc: 0.7654                                                                                                    \n",
      "epoch: 0032, loss: 0.3687 - val_loss: 0.8225; auc: 0.9381 - val_auc: 0.7684                                                                                                    \n",
      "epoch: 0033, loss: 0.3541 - val_loss: 0.8577; auc: 0.9420 - val_auc: 0.7703                                                                                                    \n",
      "epoch: 0034, loss: 0.3489 - val_loss: 0.7706; auc: 0.9409 - val_auc: 0.7707                                                                                                    \n",
      "epoch: 0035, loss: 0.3350 - val_loss: 0.9093; auc: 0.9504 - val_auc: 0.7753                                                                                                    \n",
      "epoch: 0036, loss: 0.3340 - val_loss: 1.0024; auc: 0.9554 - val_auc: 0.7775                                                                                                    \n",
      "epoch: 0037, loss: 0.3339 - val_loss: 0.7850; auc: 0.9496 - val_auc: 0.7781                                                                                                    \n",
      "epoch: 0038, loss: 0.3186 - val_loss: 0.6741; auc: 0.9482 - val_auc: 0.7781                                                                                                    \n",
      "epoch: 0039, loss: 0.3208 - val_loss: 0.7655; auc: 0.9583 - val_auc: 0.7809                                                                                                    \n",
      "epoch: 0040, loss: 0.2929 - val_loss: 0.7833; auc: 0.9637 - val_auc: 0.7876                                                                                                    \n",
      "epoch: 0041, loss: 0.2971 - val_loss: 0.7623; auc: 0.9667 - val_auc: 0.7897                                                                                                    \n",
      "epoch: 0042, loss: 0.2716 - val_loss: 0.8596; auc: 0.9703 - val_auc: 0.7935                                                                                                    \n",
      "epoch: 0043, loss: 0.2586 - val_loss: 0.8380; auc: 0.9710 - val_auc: 0.7927                                                                                                    \n",
      "epoch: 0044, loss: 0.2518 - val_loss: 0.7452; auc: 0.9702 - val_auc: 0.7960                                                                                                    \n",
      "epoch: 0045, loss: 0.2405 - val_loss: 0.9029; auc: 0.9758 - val_auc: 0.7958                                                                                                    \n",
      "epoch: 0046, loss: 0.2319 - val_loss: 0.9025; auc: 0.9763 - val_auc: 0.7968                                                                                                    \n",
      "epoch: 0047, loss: 0.2346 - val_loss: 0.7446; auc: 0.9753 - val_auc: 0.7998                                                                                                    \n",
      "epoch: 0048, loss: 0.2251 - val_loss: 0.7717; auc: 0.9808 - val_auc: 0.8027                                                                                                    \n",
      "epoch: 0049, loss: 0.2226 - val_loss: 0.7179; auc: 0.9791 - val_auc: 0.8041                                                                                                    \n",
      "epoch: 0050, loss: 0.2177 - val_loss: 0.7999; auc: 0.9818 - val_auc: 0.8063                                                                                                    \n",
      "epoch: 0051, loss: 0.2042 - val_loss: 0.9741; auc: 0.9843 - val_auc: 0.8044                                                                                                    \n",
      "epoch: 0052, loss: 0.1879 - val_loss: 1.0671; auc: 0.9863 - val_auc: 0.8035                                                                                                    \n",
      "epoch: 0053, loss: 0.1774 - val_loss: 0.8443; auc: 0.9855 - val_auc: 0.8079                                                                                                    \n",
      "epoch: 0054, loss: 0.1756 - val_loss: 0.9870; auc: 0.9873 - val_auc: 0.8073                                                                                                    \n",
      "epoch: 0055, loss: 0.1695 - val_loss: 1.1170; auc: 0.9891 - val_auc: 0.8064                                                                                                    \n",
      "epoch: 0056, loss: 0.1600 - val_loss: 0.9308; auc: 0.9885 - val_auc: 0.8100                                                                                                    \n",
      "epoch: 0057, loss: 0.1591 - val_loss: 1.1256; auc: 0.9903 - val_auc: 0.8068                                                                                                    \n",
      "epoch: 0058, loss: 0.1551 - val_loss: 1.0676; auc: 0.9904 - val_auc: 0.8083                                                                                                    \n",
      "epoch: 0059, loss: 0.1431 - val_loss: 1.1592; auc: 0.9911 - val_auc: 0.8058                                                                                                    \n",
      "epoch: 0060, loss: 0.1526 - val_loss: 1.4897; auc: 0.9923 - val_auc: 0.7981                                                                                                    \n",
      "epoch: 0061, loss: 0.1471 - val_loss: 1.3926; auc: 0.9924 - val_auc: 0.8052                                                                                                    \n",
      "epoch: 0062, loss: 0.1390 - val_loss: 1.1947; auc: 0.9921 - val_auc: 0.7994                                                                                                    \n",
      "epoch: 0063, loss: 0.1279 - val_loss: 1.0097; auc: 0.9923 - val_auc: 0.8068                                                                                                    \n",
      "epoch: 0064, loss: 0.1238 - val_loss: 1.1462; auc: 0.9929 - val_auc: 0.8074                                                                                                    \n",
      "epoch: 0065, loss: 0.1196 - val_loss: 0.9458; auc: 0.9927 - val_auc: 0.8138                                                                                                    \n",
      "epoch: 0066, loss: 0.1191 - val_loss: 1.2350; auc: 0.9934 - val_auc: 0.8089                                                                                                    \n",
      "epoch: 0067, loss: 0.1093 - val_loss: 1.3027; auc: 0.9937 - val_auc: 0.8079                                                                                                    \n",
      "epoch: 0068, loss: 0.1052 - val_loss: 1.1098; auc: 0.9936 - val_auc: 0.8084                                                                                                    \n",
      "epoch: 0069, loss: 0.1064 - val_loss: 1.2444; auc: 0.9939 - val_auc: 0.8073                                                                                                    \n",
      "epoch: 0070, loss: 0.1014 - val_loss: 1.2974; auc: 0.9941 - val_auc: 0.8045                                                                                                    \n",
      "epoch: 0071, loss: 0.0962 - val_loss: 1.7811; auc: 0.9942 - val_auc: 0.7959                                                                                                    \n",
      "epoch: 0072, loss: 0.1025 - val_loss: 1.4439; auc: 0.9944 - val_auc: 0.8037                                                                                                    \n",
      "epoch: 0073, loss: 0.0940 - val_loss: 1.1506; auc: 0.9946 - val_auc: 0.8161                                                                                                    \n",
      "epoch: 0074, loss: 0.0961 - val_loss: 1.1748; auc: 0.9947 - val_auc: 0.8115                                                                                                    \n",
      "epoch: 0075, loss: 0.0900 - val_loss: 1.3912; auc: 0.9946 - val_auc: 0.8093                                                                                                    \n",
      "epoch: 0076, loss: 0.0843 - val_loss: 1.4935; auc: 0.9945 - val_auc: 0.8092                                                                                                    \n",
      "epoch: 0077, loss: 0.0825 - val_loss: 1.2151; auc: 0.9950 - val_auc: 0.8122                                                                                                    \n",
      "epoch: 0078, loss: 0.0869 - val_loss: 1.0877; auc: 0.9948 - val_auc: 0.8118                                                                                                    \n",
      "epoch: 0079, loss: 0.0928 - val_loss: 0.9872; auc: 0.9942 - val_auc: 0.8135                                                                                                    \n",
      "epoch: 0080, loss: 0.0973 - val_loss: 1.1638; auc: 0.9947 - val_auc: 0.8113                                                                                                    \n",
      "epoch: 0081, loss: 0.0838 - val_loss: 1.0497; auc: 0.9945 - val_auc: 0.8137                                                                                                    \n",
      "epoch: 0082, loss: 0.0819 - val_loss: 1.3549; auc: 0.9948 - val_auc: 0.8100                                                                                                    \n",
      "epoch: 0083, loss: 0.0797 - val_loss: 1.2182; auc: 0.9951 - val_auc: 0.8101                                                                                                    \n",
      "epoch: 0084, loss: 0.0754 - val_loss: 1.4552; auc: 0.9955 - val_auc: 0.8009                                                                                                    \n",
      "epoch: 0085, loss: 0.0716 - val_loss: 1.5235; auc: 0.9958 - val_auc: 0.8039                                                                                                    \n",
      "epoch: 0086, loss: 0.0671 - val_loss: 1.6779; auc: 0.9960 - val_auc: 0.8018                                                                                                    \n",
      "epoch: 0087, loss: 0.0693 - val_loss: 1.6682; auc: 0.9960 - val_auc: 0.8027                                                                                                    \n",
      "epoch: 0088, loss: 0.0694 - val_loss: 1.6530; auc: 0.9961 - val_auc: 0.8043                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00088: early stopping\n",
      "Train on 1180 samples, validate on 148 samples\n",
      "Epoch 1/38\n",
      "1180/1180 [==============================] - 5s 4ms/sample - loss: 0.6790 - val_loss: 0.8496\n",
      "Epoch 2/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.6571 - val_loss: 0.8104\n",
      "Epoch 3/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.6379 - val_loss: 0.8136\n",
      "Epoch 4/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.6314 - val_loss: 0.7940\n",
      "Epoch 5/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.6131 - val_loss: 0.7814\n",
      "Epoch 6/38\n",
      "1180/1180 [==============================] - 2s 1ms/sample - loss: 0.5980 - val_loss: 0.7832\n",
      "Epoch 7/38\n",
      "1180/1180 [==============================] - 1s 804us/sample - loss: 0.5928 - val_loss: 0.7577\n",
      "Epoch 8/38\n",
      "1180/1180 [==============================] - 1s 807us/sample - loss: 0.5723 - val_loss: 0.8229\n",
      "Epoch 9/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5613 - val_loss: 0.7325\n",
      "Epoch 10/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5547 - val_loss: 0.8729\n",
      "Epoch 11/38\n",
      "1180/1180 [==============================] - 1s 957us/sample - loss: 0.5526 - val_loss: 0.7753\n",
      "Epoch 12/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5318 - val_loss: 0.7330\n",
      "Epoch 13/38\n",
      "1180/1180 [==============================] - 1s 955us/sample - loss: 0.5269 - val_loss: 0.9054\n",
      "Epoch 14/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5377 - val_loss: 0.7989\n",
      "Epoch 15/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5153 - val_loss: 0.7170\n",
      "Epoch 16/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4966 - val_loss: 0.8103\n",
      "Epoch 17/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4853 - val_loss: 0.7519\n",
      "Epoch 18/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4778 - val_loss: 0.8023\n",
      "Epoch 19/38\n",
      "1180/1180 [==============================] - 2s 1ms/sample - loss: 0.4664 - val_loss: 0.7760\n",
      "Epoch 20/38\n",
      "1180/1180 [==============================] - 1s 782us/sample - loss: 0.4574 - val_loss: 0.7363\n",
      "Epoch 21/38\n",
      "1180/1180 [==============================] - 1s 741us/sample - loss: 0.4533 - val_loss: 0.8717\n",
      "Epoch 22/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4445 - val_loss: 0.8817\n",
      "Epoch 23/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4587 - val_loss: 0.7233\n",
      "Epoch 24/38\n",
      "1180/1180 [==============================] - 1s 996us/sample - loss: 0.4169 - val_loss: 0.7683\n",
      "Epoch 25/38\n",
      "1180/1180 [==============================] - 1s 981us/sample - loss: 0.4061 - val_loss: 0.7748\n",
      "Epoch 26/38\n",
      "1180/1180 [==============================] - 1s 924us/sample - loss: 0.4014 - val_loss: 0.6661\n",
      "Epoch 27/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4288 - val_loss: 0.8072\n",
      "Epoch 28/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3778 - val_loss: 0.8343\n",
      "Epoch 29/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3800 - val_loss: 0.8574\n",
      "Epoch 30/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3616 - val_loss: 0.7744\n",
      "Epoch 31/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3523 - val_loss: 0.9650\n",
      "Epoch 32/38\n",
      "1180/1180 [==============================] - 2s 1ms/sample - loss: 0.3447 - val_loss: 0.7320\n",
      "Epoch 33/38\n",
      "1180/1180 [==============================] - 1s 786us/sample - loss: 0.3343 - val_loss: 0.6395\n",
      "Epoch 34/38\n",
      "1180/1180 [==============================] - 1s 792us/sample - loss: 0.3752 - val_loss: 0.6895\n",
      "Epoch 35/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3489 - val_loss: 0.8289\n",
      "Epoch 36/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.2993 - val_loss: 0.7906\n",
      "Epoch 37/38\n",
      "1180/1180 [==============================] - 1s 987us/sample - loss: 0.2890 - val_loss: 0.8409\n",
      "Epoch 38/38\n",
      "1180/1180 [==============================] - 1s 966us/sample - loss: 0.2875 - val_loss: 0.7750\n",
      "Train on 1180 samples, validate on 148 samples\n",
      "Epoch 1/38\n",
      "1180/1180 [==============================] - 4s 3ms/sample - loss: 0.6790 - val_loss: 0.8498\n",
      "Epoch 2/38\n",
      "1180/1180 [==============================] - 1s 931us/sample - loss: 0.6571 - val_loss: 0.8107\n",
      "Epoch 3/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.6379 - val_loss: 0.8130\n",
      "Epoch 4/38\n",
      "1180/1180 [==============================] - 1s 990us/sample - loss: 0.6315 - val_loss: 0.7948\n",
      "Epoch 5/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.6140 - val_loss: 0.7836\n",
      "Epoch 6/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5986 - val_loss: 0.7826\n",
      "Epoch 7/38\n",
      "1180/1180 [==============================] - 1s 959us/sample - loss: 0.5935 - val_loss: 0.7587\n",
      "Epoch 8/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5729 - val_loss: 0.8234\n",
      "Epoch 9/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5622 - val_loss: 0.7333\n",
      "Epoch 10/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.5551 - val_loss: 0.8754\n",
      "Epoch 11/38\n",
      "1180/1180 [==============================] - 1s 993us/sample - loss: 0.5532 - val_loss: 0.7741\n",
      "Epoch 12/38\n",
      "1180/1180 [==============================] - 2s 1ms/sample - loss: 0.5318 - val_loss: 0.7332\n",
      "Epoch 13/38\n",
      "1180/1180 [==============================] - 1s 902us/sample - loss: 0.5272 - val_loss: 0.9082\n",
      "Epoch 14/38\n",
      "1180/1180 [==============================] - 1s 721us/sample - loss: 0.5378 - val_loss: 0.7991\n",
      "Epoch 15/38\n",
      "1180/1180 [==============================] - 1s 922us/sample - loss: 0.5154 - val_loss: 0.7176\n",
      "Epoch 16/38\n",
      "1180/1180 [==============================] - 1s 919us/sample - loss: 0.4969 - val_loss: 0.8108\n",
      "Epoch 17/38\n",
      "1180/1180 [==============================] - 1s 987us/sample - loss: 0.4856 - val_loss: 0.7510\n",
      "Epoch 18/38\n",
      "1180/1180 [==============================] - 1s 953us/sample - loss: 0.4781 - val_loss: 0.8047\n",
      "Epoch 19/38\n",
      "1180/1180 [==============================] - 1s 939us/sample - loss: 0.4665 - val_loss: 0.7754\n",
      "Epoch 20/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4576 - val_loss: 0.7357\n",
      "Epoch 21/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4534 - val_loss: 0.8727\n",
      "Epoch 22/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4445 - val_loss: 0.8853\n",
      "Epoch 23/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4593 - val_loss: 0.7217\n",
      "Epoch 24/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.4167 - val_loss: 0.7673\n",
      "Epoch 25/38\n",
      "1180/1180 [==============================] - 2s 1ms/sample - loss: 0.4053 - val_loss: 0.7758\n",
      "Epoch 26/38\n",
      "1180/1180 [==============================] - 1s 807us/sample - loss: 0.4004 - val_loss: 0.6657\n",
      "Epoch 27/38\n",
      "1180/1180 [==============================] - 1s 786us/sample - loss: 0.4270 - val_loss: 0.8082\n",
      "Epoch 28/38\n",
      "1180/1180 [==============================] - 1s 879us/sample - loss: 0.3764 - val_loss: 0.8382\n",
      "Epoch 29/38\n",
      "1180/1180 [==============================] - 1s 972us/sample - loss: 0.3785 - val_loss: 0.8670\n",
      "Epoch 30/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3602 - val_loss: 0.7828\n",
      "Epoch 31/38\n",
      "1180/1180 [==============================] - 1s 943us/sample - loss: 0.3504 - val_loss: 0.9785\n",
      "Epoch 32/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3427 - val_loss: 0.7396\n",
      "Epoch 33/38\n",
      "1180/1180 [==============================] - 1s 978us/sample - loss: 0.3313 - val_loss: 0.6407\n",
      "Epoch 34/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3725 - val_loss: 0.6875\n",
      "Epoch 35/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.3480 - val_loss: 0.8413\n",
      "Epoch 36/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.2970 - val_loss: 0.7957\n",
      "Epoch 37/38\n",
      "1180/1180 [==============================] - 1s 1ms/sample - loss: 0.2861 - val_loss: 0.8485\n",
      "Epoch 38/38\n",
      "1180/1180 [==============================] - 2s 2ms/sample - loss: 0.2840 - val_loss: 0.7837\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, seed in enumerate([7, 77, 77]):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "    performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc3a61cbc18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfp = pd.DataFrame(performance.history)[[ 'auc', 'val_auc']]\n",
    "dfp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9727011494252874"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002453092472163022"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ClinTox</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.778142</td>\n",
       "      <td>0.975204</td>\n",
       "      <td>ROC</td>\n",
       "      <td>717602</td>\n",
       "      <td>37</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ClinTox</td>\n",
       "      <td>0.966651</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.972599</td>\n",
       "      <td>ROC</td>\n",
       "      <td>717602</td>\n",
       "      <td>37</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ClinTox</td>\n",
       "      <td>0.970406</td>\n",
       "      <td>0.807080</td>\n",
       "      <td>0.970301</td>\n",
       "      <td>ROC</td>\n",
       "      <td>717602</td>\n",
       "      <td>37</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0   ClinTox   0.948209   0.778142  0.975204    ROC              717602   \n",
       "1   ClinTox   0.966651   0.806900  0.972599    ROC              717602   \n",
       "2   ClinTox   0.970406   0.807080  0.970301    ROC              717602   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0          37         128  0.0001             0  \n",
       "1          37         128  0.0001             0  \n",
       "2          37         128  0.0001             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
