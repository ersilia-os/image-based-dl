{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [17:46:10] Enabling RDKit 2019.09.2 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def get_attentiveFP_idx(df, file = './split_and_data/05_BACE_attentiveFP.data'):\n",
    "    \"\"\" attentiveFP dataset\"\"\"\n",
    "    train, valid,test = load(file)\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train), len(valid), len(test)))\n",
    "    train_idx = df[df.smiles.isin(train.mol)].index\n",
    "    valid_idx = df[df.smiles.isin(valid.mol)].index\n",
    "    test_idx = df[df.smiles.isin(test.mol)].index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: BACE number of split times: 3\n",
      "training set: 1211, valid set: 151, test set 151\n",
      "training set: 1211, valid set: 151, test set 151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1211, 151, 151)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_name = 'BACE'\n",
    "from chembench import load_data\n",
    "df, _ = load_data(task_name)\n",
    "\n",
    "train_idx, valid_idx, test_idx = get_attentiveFP_idx(df) \n",
    "len(train_idx), len(valid_idx), len(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_dir = '../02_OutofTheBox_benchmark_comparison_DMPNN/tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)\n",
    "\n",
    "    \n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').values\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211 151 151\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "trainX = (X1[train_idx], X2[train_idx])\n",
    "trainY = Y[train_idx]\n",
    "\n",
    "validX = (X1[valid_idx], X2[valid_idx])\n",
    "validY = Y[valid_idx]\n",
    "\n",
    "testX = (X1[test_idx], X2[test_idx])\n",
    "testY = Y[test_idx]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256, 128, 32]\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, loss: 0.7556 - val_loss: 0.7496; auc: 0.7959 - val_auc: 0.7475                                                                                                    \n",
      "epoch: 0002, loss: 0.7368 - val_loss: 0.7499; auc: 0.8055 - val_auc: 0.7408                                                                                                    \n",
      "epoch: 0003, loss: 0.7192 - val_loss: 0.7242; auc: 0.8214 - val_auc: 0.7592                                                                                                    \n",
      "epoch: 0004, loss: 0.7014 - val_loss: 0.6970; auc: 0.8337 - val_auc: 0.7738                                                                                                    \n",
      "epoch: 0005, loss: 0.6857 - val_loss: 0.6811; auc: 0.8458 - val_auc: 0.7893                                                                                                    \n",
      "epoch: 0006, loss: 0.6584 - val_loss: 0.6616; auc: 0.8530 - val_auc: 0.7991                                                                                                    \n",
      "epoch: 0007, loss: 0.6365 - val_loss: 0.6423; auc: 0.8607 - val_auc: 0.8063                                                                                                    \n",
      "epoch: 0008, loss: 0.6123 - val_loss: 0.6317; auc: 0.8694 - val_auc: 0.8157                                                                                                    \n",
      "epoch: 0009, loss: 0.5798 - val_loss: 0.6108; auc: 0.8768 - val_auc: 0.8218                                                                                                    \n",
      "epoch: 0010, loss: 0.5635 - val_loss: 0.5922; auc: 0.8813 - val_auc: 0.8312                                                                                                    \n",
      "epoch: 0011, loss: 0.5506 - val_loss: 0.5722; auc: 0.8864 - val_auc: 0.8363                                                                                                    \n",
      "epoch: 0012, loss: 0.5301 - val_loss: 0.6409; auc: 0.8959 - val_auc: 0.8373                                                                                                    \n",
      "epoch: 0013, loss: 0.5367 - val_loss: 0.6363; auc: 0.8989 - val_auc: 0.8319                                                                                                    \n",
      "epoch: 0014, loss: 0.5116 - val_loss: 0.5857; auc: 0.9023 - val_auc: 0.8400                                                                                                    \n",
      "epoch: 0015, loss: 0.4802 - val_loss: 0.5566; auc: 0.9057 - val_auc: 0.8438                                                                                                    \n",
      "epoch: 0016, loss: 0.4695 - val_loss: 0.5394; auc: 0.9093 - val_auc: 0.8445                                                                                                    \n",
      "epoch: 0017, loss: 0.4551 - val_loss: 0.5294; auc: 0.9130 - val_auc: 0.8482                                                                                                    \n",
      "epoch: 0018, loss: 0.4627 - val_loss: 0.5363; auc: 0.9157 - val_auc: 0.8498                                                                                                    \n",
      "epoch: 0019, loss: 0.4334 - val_loss: 0.5309; auc: 0.9202 - val_auc: 0.8493                                                                                                    \n",
      "epoch: 0020, loss: 0.4244 - val_loss: 0.5737; auc: 0.9259 - val_auc: 0.8504                                                                                                    \n",
      "epoch: 0021, loss: 0.4108 - val_loss: 0.5733; auc: 0.9293 - val_auc: 0.8505                                                                                                    \n",
      "epoch: 0022, loss: 0.4085 - val_loss: 0.5338; auc: 0.9322 - val_auc: 0.8553                                                                                                    \n",
      "epoch: 0023, loss: 0.3888 - val_loss: 0.5231; auc: 0.9357 - val_auc: 0.8565                                                                                                    \n",
      "epoch: 0024, loss: 0.3784 - val_loss: 0.5097; auc: 0.9384 - val_auc: 0.8588                                                                                                    \n",
      "epoch: 0025, loss: 0.3717 - val_loss: 0.5119; auc: 0.9425 - val_auc: 0.8620                                                                                                    \n",
      "epoch: 0026, loss: 0.3773 - val_loss: 0.5021; auc: 0.9447 - val_auc: 0.8627                                                                                                    \n",
      "epoch: 0027, loss: 0.3750 - val_loss: 0.5043; auc: 0.9475 - val_auc: 0.8614                                                                                                    \n",
      "epoch: 0028, loss: 0.3658 - val_loss: 0.5739; auc: 0.9509 - val_auc: 0.8613                                                                                                    \n",
      "epoch: 0029, loss: 0.3669 - val_loss: 0.4997; auc: 0.9510 - val_auc: 0.8648                                                                                                    \n",
      "epoch: 0030, loss: 0.3386 - val_loss: 0.5131; auc: 0.9531 - val_auc: 0.8678                                                                                                    \n",
      "epoch: 0031, loss: 0.3364 - val_loss: 0.5251; auc: 0.9561 - val_auc: 0.8683                                                                                                    \n",
      "epoch: 0032, loss: 0.3423 - val_loss: 0.5175; auc: 0.9592 - val_auc: 0.8653                                                                                                    \n",
      "epoch: 0033, loss: 0.3439 - val_loss: 0.5830; auc: 0.9575 - val_auc: 0.8699                                                                                                    \n",
      "epoch: 0034, loss: 0.3490 - val_loss: 0.5120; auc: 0.9594 - val_auc: 0.8692                                                                                                    \n",
      "epoch: 0035, loss: 0.3282 - val_loss: 0.5245; auc: 0.9634 - val_auc: 0.8678                                                                                                    \n",
      "epoch: 0036, loss: 0.3071 - val_loss: 0.6389; auc: 0.9650 - val_auc: 0.8685                                                                                                    \n",
      "epoch: 0037, loss: 0.3347 - val_loss: 0.5054; auc: 0.9661 - val_auc: 0.8743                                                                                                    \n",
      "epoch: 0038, loss: 0.2905 - val_loss: 0.4805; auc: 0.9673 - val_auc: 0.8750                                                                                                    \n",
      "epoch: 0039, loss: 0.2840 - val_loss: 0.4805; auc: 0.9691 - val_auc: 0.8754                                                                                                    \n",
      "epoch: 0040, loss: 0.2752 - val_loss: 0.4931; auc: 0.9708 - val_auc: 0.8732                                                                                                    \n",
      "epoch: 0041, loss: 0.2675 - val_loss: 0.4795; auc: 0.9722 - val_auc: 0.8785                                                                                                    \n",
      "epoch: 0042, loss: 0.2608 - val_loss: 0.4820; auc: 0.9740 - val_auc: 0.8761                                                                                                    \n",
      "epoch: 0043, loss: 0.2659 - val_loss: 0.6054; auc: 0.9759 - val_auc: 0.8755                                                                                                    \n",
      "epoch: 0044, loss: 0.2884 - val_loss: 0.4822; auc: 0.9757 - val_auc: 0.8783                                                                                                    \n",
      "epoch: 0045, loss: 0.2530 - val_loss: 0.4937; auc: 0.9775 - val_auc: 0.8766                                                                                                    \n",
      "epoch: 0046, loss: 0.2448 - val_loss: 0.4825; auc: 0.9788 - val_auc: 0.8805                                                                                                    \n",
      "epoch: 0047, loss: 0.2399 - val_loss: 0.4867; auc: 0.9806 - val_auc: 0.8810                                                                                                    \n",
      "epoch: 0048, loss: 0.2293 - val_loss: 0.4818; auc: 0.9813 - val_auc: 0.8843                                                                                                    \n",
      "epoch: 0049, loss: 0.2316 - val_loss: 0.5013; auc: 0.9828 - val_auc: 0.8831                                                                                                    \n",
      "epoch: 0050, loss: 0.2160 - val_loss: 0.5270; auc: 0.9838 - val_auc: 0.8822                                                                                                    \n",
      "epoch: 0051, loss: 0.2162 - val_loss: 0.4832; auc: 0.9843 - val_auc: 0.8845                                                                                                    \n",
      "epoch: 0052, loss: 0.2099 - val_loss: 0.4908; auc: 0.9855 - val_auc: 0.8852                                                                                                    \n",
      "epoch: 0053, loss: 0.2114 - val_loss: 0.5022; auc: 0.9864 - val_auc: 0.8861                                                                                                    \n",
      "epoch: 0054, loss: 0.2118 - val_loss: 0.4911; auc: 0.9872 - val_auc: 0.8854                                                                                                    \n",
      "epoch: 0055, loss: 0.2184 - val_loss: 0.4803; auc: 0.9876 - val_auc: 0.8912                                                                                                    \n",
      "epoch: 0056, loss: 0.1912 - val_loss: 0.4964; auc: 0.9892 - val_auc: 0.8915                                                                                                    \n",
      "epoch: 0057, loss: 0.1900 - val_loss: 0.5183; auc: 0.9897 - val_auc: 0.8886                                                                                                    \n",
      "epoch: 0058, loss: 0.1922 - val_loss: 0.4865; auc: 0.9901 - val_auc: 0.8937                                                                                                    \n",
      "epoch: 0059, loss: 0.1780 - val_loss: 0.4892; auc: 0.9911 - val_auc: 0.8940                                                                                                    \n",
      "epoch: 0060, loss: 0.1947 - val_loss: 0.5534; auc: 0.9914 - val_auc: 0.8910                                                                                                    \n",
      "epoch: 0061, loss: 0.1837 - val_loss: 0.5032; auc: 0.9913 - val_auc: 0.8928                                                                                                    \n",
      "epoch: 0062, loss: 0.1754 - val_loss: 0.5425; auc: 0.9928 - val_auc: 0.8908                                                                                                    \n",
      "epoch: 0063, loss: 0.2047 - val_loss: 0.5250; auc: 0.9924 - val_auc: 0.8921                                                                                                    \n",
      "epoch: 0064, loss: 0.1989 - val_loss: 0.5517; auc: 0.9920 - val_auc: 0.8919                                                                                                    \n",
      "epoch: 0065, loss: 0.1824 - val_loss: 0.4949; auc: 0.9929 - val_auc: 0.8945                                                                                                    \n",
      "epoch: 0066, loss: 0.1542 - val_loss: 0.5050; auc: 0.9937 - val_auc: 0.8954                                                                                                    \n",
      "epoch: 0067, loss: 0.1549 - val_loss: 0.5129; auc: 0.9942 - val_auc: 0.8967                                                                                                    \n",
      "epoch: 0068, loss: 0.1548 - val_loss: 0.4985; auc: 0.9946 - val_auc: 0.8968                                                                                                    \n",
      "epoch: 0069, loss: 0.1772 - val_loss: 0.4916; auc: 0.9945 - val_auc: 0.9019                                                                                                    \n",
      "epoch: 0070, loss: 0.1731 - val_loss: 0.4925; auc: 0.9951 - val_auc: 0.9018                                                                                                    \n",
      "epoch: 0071, loss: 0.1391 - val_loss: 0.5083; auc: 0.9955 - val_auc: 0.8968                                                                                                    \n",
      "epoch: 0072, loss: 0.1362 - val_loss: 0.5147; auc: 0.9959 - val_auc: 0.8954                                                                                                    \n",
      "epoch: 0073, loss: 0.1401 - val_loss: 0.5202; auc: 0.9960 - val_auc: 0.8991                                                                                                    \n",
      "epoch: 0074, loss: 0.1370 - val_loss: 0.5304; auc: 0.9964 - val_auc: 0.8951                                                                                                    \n",
      "epoch: 0075, loss: 0.1311 - val_loss: 0.5135; auc: 0.9965 - val_auc: 0.8986                                                                                                    \n",
      "epoch: 0076, loss: 0.1239 - val_loss: 0.5179; auc: 0.9969 - val_auc: 0.8986                                                                                                    \n",
      "epoch: 0077, loss: 0.1356 - val_loss: 0.5216; auc: 0.9968 - val_auc: 0.8977                                                                                                    \n",
      "epoch: 0078, loss: 0.1193 - val_loss: 0.5183; auc: 0.9972 - val_auc: 0.8986                                                                                                    \n",
      "epoch: 0079, loss: 0.1245 - val_loss: 0.6244; auc: 0.9970 - val_auc: 0.8988                                                                                                    \n",
      "epoch: 0080, loss: 0.1457 - val_loss: 0.5414; auc: 0.9973 - val_auc: 0.8974                                                                                                    \n",
      "epoch: 0081, loss: 0.1181 - val_loss: 0.5364; auc: 0.9975 - val_auc: 0.8960                                                                                                    \n",
      "epoch: 0082, loss: 0.1335 - val_loss: 0.5545; auc: 0.9976 - val_auc: 0.8975                                                                                                    \n",
      "epoch: 0083, loss: 0.1164 - val_loss: 0.5253; auc: 0.9979 - val_auc: 0.9004                                                                                                    \n",
      "epoch: 0084, loss: 0.1117 - val_loss: 0.5961; auc: 0.9976 - val_auc: 0.8952                                                                                                    \n",
      "epoch: 0085, loss: 0.1155 - val_loss: 0.5463; auc: 0.9979 - val_auc: 0.9000                                                                                                    \n",
      "epoch: 0086, loss: 0.1055 - val_loss: 0.5414; auc: 0.9983 - val_auc: 0.8981                                                                                                    \n",
      "epoch: 0087, loss: 0.1051 - val_loss: 0.5368; auc: 0.9982 - val_auc: 0.9014                                                                                                    \n",
      "epoch: 0088, loss: 0.1143 - val_loss: 0.5563; auc: 0.9984 - val_auc: 0.9007                                                                                                    \n",
      "epoch: 0089, loss: 0.1015 - val_loss: 0.5458; auc: 0.9981 - val_auc: 0.9000                                                                                                    \n",
      "epoch: 0090, loss: 0.0958 - val_loss: 0.5434; auc: 0.9986 - val_auc: 0.8995                                                                                                    \n",
      "epoch: 0091, loss: 0.0954 - val_loss: 0.5751; auc: 0.9981 - val_auc: 0.9012                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00091: early stopping\n",
      "Train on 1211 samples, validate on 151 samples\n",
      "Epoch 1/41\n",
      "1211/1211 [==============================] - 2s 2ms/sample - loss: 0.7539 - val_loss: 0.7572\n",
      "Epoch 2/41\n",
      "1211/1211 [==============================] - 1s 543us/sample - loss: 0.7339 - val_loss: 0.7348\n",
      "Epoch 3/41\n",
      "1211/1211 [==============================] - 1s 576us/sample - loss: 0.7140 - val_loss: 0.7243\n",
      "Epoch 4/41\n",
      "1211/1211 [==============================] - 1s 589us/sample - loss: 0.6932 - val_loss: 0.7008\n",
      "Epoch 5/41\n",
      "1211/1211 [==============================] - 1s 584us/sample - loss: 0.6666 - val_loss: 0.6703\n",
      "Epoch 6/41\n",
      "1211/1211 [==============================] - 1s 555us/sample - loss: 0.6447 - val_loss: 0.6464\n",
      "Epoch 7/41\n",
      "1211/1211 [==============================] - 1s 621us/sample - loss: 0.6147 - val_loss: 0.6291\n",
      "Epoch 8/41\n",
      "1211/1211 [==============================] - 1s 583us/sample - loss: 0.5817 - val_loss: 0.6263\n",
      "Epoch 9/41\n",
      "1211/1211 [==============================] - 1s 570us/sample - loss: 0.5518 - val_loss: 0.6065\n",
      "Epoch 10/41\n",
      "1211/1211 [==============================] - 1s 594us/sample - loss: 0.5282 - val_loss: 0.5780\n",
      "Epoch 11/41\n",
      "1211/1211 [==============================] - 1s 584us/sample - loss: 0.5006 - val_loss: 0.5603\n",
      "Epoch 12/41\n",
      "1211/1211 [==============================] - 1s 623us/sample - loss: 0.4863 - val_loss: 0.5802\n",
      "Epoch 13/41\n",
      "1211/1211 [==============================] - 1s 577us/sample - loss: 0.4620 - val_loss: 0.5531\n",
      "Epoch 14/41\n",
      "1211/1211 [==============================] - 1s 604us/sample - loss: 0.4456 - val_loss: 0.5265\n",
      "Epoch 15/41\n",
      "1211/1211 [==============================] - 1s 585us/sample - loss: 0.4312 - val_loss: 0.6874\n",
      "Epoch 16/41\n",
      "1211/1211 [==============================] - 1s 579us/sample - loss: 0.4321 - val_loss: 0.5235\n",
      "Epoch 17/41\n",
      "1211/1211 [==============================] - 1s 527us/sample - loss: 0.4033 - val_loss: 0.5243\n",
      "Epoch 18/41\n",
      "1211/1211 [==============================] - 1s 569us/sample - loss: 0.3933 - val_loss: 0.5481\n",
      "Epoch 19/41\n",
      "1211/1211 [==============================] - 1s 598us/sample - loss: 0.4007 - val_loss: 0.5269\n",
      "Epoch 20/41\n",
      "1211/1211 [==============================] - 1s 608us/sample - loss: 0.3804 - val_loss: 0.5216\n",
      "Epoch 21/41\n",
      "1211/1211 [==============================] - 1s 617us/sample - loss: 0.3800 - val_loss: 0.6080\n",
      "Epoch 22/41\n",
      "1211/1211 [==============================] - 1s 573us/sample - loss: 0.3887 - val_loss: 0.5124\n",
      "Epoch 23/41\n",
      "1211/1211 [==============================] - 1s 595us/sample - loss: 0.3460 - val_loss: 0.5030\n",
      "Epoch 24/41\n",
      "1211/1211 [==============================] - 1s 587us/sample - loss: 0.3362 - val_loss: 0.5353\n",
      "Epoch 25/41\n",
      "1211/1211 [==============================] - 1s 636us/sample - loss: 0.3881 - val_loss: 0.7164\n",
      "Epoch 26/41\n",
      "1211/1211 [==============================] - 1s 587us/sample - loss: 0.3925 - val_loss: 0.5939\n",
      "Epoch 27/41\n",
      "1211/1211 [==============================] - 1s 589us/sample - loss: 0.3601 - val_loss: 0.5068\n",
      "Epoch 28/41\n",
      "1211/1211 [==============================] - 1s 606us/sample - loss: 0.3347 - val_loss: 0.5141\n",
      "Epoch 29/41\n",
      "1211/1211 [==============================] - 1s 600us/sample - loss: 0.3245 - val_loss: 0.5031\n",
      "Epoch 30/41\n",
      "1211/1211 [==============================] - 1s 603us/sample - loss: 0.3048 - val_loss: 0.5071\n",
      "Epoch 31/41\n",
      "1211/1211 [==============================] - 1s 602us/sample - loss: 0.2977 - val_loss: 0.5172\n",
      "Epoch 32/41\n",
      "1211/1211 [==============================] - 1s 578us/sample - loss: 0.2955 - val_loss: 0.4989\n",
      "Epoch 33/41\n",
      "1211/1211 [==============================] - 1s 591us/sample - loss: 0.2875 - val_loss: 0.5015\n",
      "Epoch 34/41\n",
      "1211/1211 [==============================] - 1s 622us/sample - loss: 0.2829 - val_loss: 0.4980\n",
      "Epoch 35/41\n",
      "1211/1211 [==============================] - 1s 605us/sample - loss: 0.3020 - val_loss: 0.5356\n",
      "Epoch 36/41\n",
      "1211/1211 [==============================] - 1s 642us/sample - loss: 0.2941 - val_loss: 0.5163\n",
      "Epoch 37/41\n",
      "1211/1211 [==============================] - 1s 569us/sample - loss: 0.2665 - val_loss: 0.5033\n",
      "Epoch 38/41\n",
      "1211/1211 [==============================] - 1s 561us/sample - loss: 0.2620 - val_loss: 0.5046\n",
      "Epoch 39/41\n",
      "1211/1211 [==============================] - 1s 588us/sample - loss: 0.2542 - val_loss: 0.5332\n",
      "Epoch 40/41\n",
      "1211/1211 [==============================] - 1s 605us/sample - loss: 0.2470 - val_loss: 0.5032\n",
      "Epoch 41/41\n",
      "1211/1211 [==============================] - 1s 574us/sample - loss: 0.2540 - val_loss: 0.6636\n",
      "Train on 1211 samples, validate on 151 samples\n",
      "Epoch 1/41\n",
      "1211/1211 [==============================] - 2s 2ms/sample - loss: 0.7539 - val_loss: 0.7574\n",
      "Epoch 2/41\n",
      "1211/1211 [==============================] - 1s 727us/sample - loss: 0.7339 - val_loss: 0.7348\n",
      "Epoch 3/41\n",
      "1211/1211 [==============================] - 1s 689us/sample - loss: 0.7141 - val_loss: 0.7247\n",
      "Epoch 4/41\n",
      "1211/1211 [==============================] - 1s 748us/sample - loss: 0.6929 - val_loss: 0.7006\n",
      "Epoch 5/41\n",
      "1211/1211 [==============================] - 1s 728us/sample - loss: 0.6664 - val_loss: 0.6700\n",
      "Epoch 6/41\n",
      "1211/1211 [==============================] - 1s 708us/sample - loss: 0.6444 - val_loss: 0.6461\n",
      "Epoch 7/41\n",
      "1211/1211 [==============================] - 1s 636us/sample - loss: 0.6145 - val_loss: 0.6278\n",
      "Epoch 8/41\n",
      "1211/1211 [==============================] - 1s 688us/sample - loss: 0.5804 - val_loss: 0.6248\n",
      "Epoch 9/41\n",
      "1211/1211 [==============================] - 1s 674us/sample - loss: 0.5502 - val_loss: 0.6062\n",
      "Epoch 10/41\n",
      "1211/1211 [==============================] - 1s 700us/sample - loss: 0.5268 - val_loss: 0.5763\n",
      "Epoch 11/41\n",
      "1211/1211 [==============================] - 1s 684us/sample - loss: 0.4994 - val_loss: 0.5619\n",
      "Epoch 12/41\n",
      "1211/1211 [==============================] - 1s 679us/sample - loss: 0.4876 - val_loss: 0.5849\n",
      "Epoch 13/41\n",
      "1211/1211 [==============================] - 1s 697us/sample - loss: 0.4646 - val_loss: 0.5439\n",
      "Epoch 14/41\n",
      "1211/1211 [==============================] - 1s 709us/sample - loss: 0.4476 - val_loss: 0.5269\n",
      "Epoch 15/41\n",
      "1211/1211 [==============================] - 1s 730us/sample - loss: 0.4350 - val_loss: 0.7061\n",
      "Epoch 16/41\n",
      "1211/1211 [==============================] - 1s 672us/sample - loss: 0.4351 - val_loss: 0.5250\n",
      "Epoch 17/41\n",
      "1211/1211 [==============================] - 1s 731us/sample - loss: 0.4045 - val_loss: 0.5277\n",
      "Epoch 18/41\n",
      "1211/1211 [==============================] - 1s 693us/sample - loss: 0.3881 - val_loss: 0.5373\n",
      "Epoch 19/41\n",
      "1211/1211 [==============================] - 1s 720us/sample - loss: 0.3973 - val_loss: 0.5271\n",
      "Epoch 20/41\n",
      "1211/1211 [==============================] - 1s 698us/sample - loss: 0.3838 - val_loss: 0.5196\n",
      "Epoch 21/41\n",
      "1211/1211 [==============================] - 1s 712us/sample - loss: 0.3838 - val_loss: 0.5915\n",
      "Epoch 22/41\n",
      "1211/1211 [==============================] - 1s 729us/sample - loss: 0.3831 - val_loss: 0.5120\n",
      "Epoch 23/41\n",
      "1211/1211 [==============================] - 1s 736us/sample - loss: 0.3441 - val_loss: 0.5024\n",
      "Epoch 24/41\n",
      "1211/1211 [==============================] - 1s 751us/sample - loss: 0.3364 - val_loss: 0.5324\n",
      "Epoch 25/41\n",
      "1211/1211 [==============================] - 1s 728us/sample - loss: 0.3874 - val_loss: 0.7060\n",
      "Epoch 26/41\n",
      "1211/1211 [==============================] - 1s 688us/sample - loss: 0.3910 - val_loss: 0.6102\n",
      "Epoch 27/41\n",
      "1211/1211 [==============================] - 1s 687us/sample - loss: 0.3616 - val_loss: 0.5095\n",
      "Epoch 28/41\n",
      "1211/1211 [==============================] - 1s 721us/sample - loss: 0.3378 - val_loss: 0.5115\n",
      "Epoch 29/41\n",
      "1211/1211 [==============================] - 1s 710us/sample - loss: 0.3239 - val_loss: 0.5043\n",
      "Epoch 30/41\n",
      "1211/1211 [==============================] - 1s 732us/sample - loss: 0.3049 - val_loss: 0.5040\n",
      "Epoch 31/41\n",
      "1211/1211 [==============================] - 1s 690us/sample - loss: 0.2971 - val_loss: 0.5184\n",
      "Epoch 32/41\n",
      "1211/1211 [==============================] - 1s 716us/sample - loss: 0.2944 - val_loss: 0.4985\n",
      "Epoch 33/41\n",
      "1211/1211 [==============================] - 1s 676us/sample - loss: 0.2863 - val_loss: 0.5006\n",
      "Epoch 34/41\n",
      "1211/1211 [==============================] - 1s 699us/sample - loss: 0.2817 - val_loss: 0.4972\n",
      "Epoch 35/41\n",
      "1211/1211 [==============================] - 1s 743us/sample - loss: 0.2997 - val_loss: 0.5352\n",
      "Epoch 36/41\n",
      "1211/1211 [==============================] - 1s 723us/sample - loss: 0.2916 - val_loss: 0.5173\n",
      "Epoch 37/41\n",
      "1211/1211 [==============================] - 1s 722us/sample - loss: 0.2647 - val_loss: 0.5016\n",
      "Epoch 38/41\n",
      "1211/1211 [==============================] - 1s 755us/sample - loss: 0.2606 - val_loss: 0.5047\n",
      "Epoch 39/41\n",
      "1211/1211 [==============================] - 1s 726us/sample - loss: 0.2520 - val_loss: 0.5342\n",
      "Epoch 40/41\n",
      "1211/1211 [==============================] - 1s 691us/sample - loss: 0.2448 - val_loss: 0.5021\n",
      "Epoch 41/41\n",
      "1211/1211 [==============================] - 1s 687us/sample - loss: 0.2522 - val_loss: 0.6674\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, seed in enumerate([7, 77, 77]):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "        performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe4f7873cc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.880799045915325"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014459577403675673"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BACE</td>\n",
       "      <td>0.972186</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.879249</td>\n",
       "      <td>ROC</td>\n",
       "      <td>803681</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BACE</td>\n",
       "      <td>0.978767</td>\n",
       "      <td>0.870775</td>\n",
       "      <td>0.882111</td>\n",
       "      <td>ROC</td>\n",
       "      <td>803681</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BACE</td>\n",
       "      <td>0.979110</td>\n",
       "      <td>0.870951</td>\n",
       "      <td>0.881038</td>\n",
       "      <td>ROC</td>\n",
       "      <td>803681</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0      BACE   0.972186   0.878521  0.879249    ROC              803681   \n",
       "1      BACE   0.978767   0.870775  0.882111    ROC              803681   \n",
       "2      BACE   0.979110   0.870951  0.881038    ROC              803681   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0          40         128  0.0001             0  \n",
       "1          40         128  0.0001             0  \n",
       "2          40         128  0.0001             0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
