{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [14:13:00] Enabling RDKit 2019.09.2 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attentiveFP_idx(df):\n",
    "    \"\"\" attentiveFP dataset\"\"\"\n",
    "    train, valid,test = load('./split_and_data/08_Tox21_attentiveFP.data')\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train), len(valid), len(test)))\n",
    "    train_idx = df[df.smiles.isin(train.smiles)].index\n",
    "    valid_idx = df[df.smiles.isin(valid.smiles)].index\n",
    "    test_idx = df[df.smiles.isin(test.smiles)].index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: Tox21 number of split times: 3\n",
      "training set: 6249, valid set: 781, test set 781\n",
      "training set: 6249, valid set: 781, test set 781\n"
     ]
    }
   ],
   "source": [
    "task_name = 'Tox21'\n",
    "\n",
    "from chembench import load_data\n",
    "df, _ = load_data(task_name)\n",
    "\n",
    "train_idx, valid_idx, test_idx = get_attentiveFP_idx(df) \n",
    "len(train_idx), len(valid_idx), len(test_idx)\n",
    "\n",
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_dir = '../02_OutofTheBox_benchmark_comparison_DMPNN/tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)\n",
    "\n",
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256] #12 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "metric = 'ROC'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6249 781 781\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "trainX = (X1[train_idx], X2[train_idx])\n",
    "trainY = Y[train_idx]\n",
    "\n",
    "validX = (X1[valid_idx], X2[valid_idx])\n",
    "validY = Y[valid_idx]\n",
    "\n",
    "testX = (X1[test_idx], X2[test_idx])\n",
    "testY = Y[test_idx]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, loss: 1.1609 - val_loss: 1.1380; auc: 0.7204 - val_auc: 0.7037                                                                                                    \n",
      "epoch: 0002, loss: 1.1183 - val_loss: 1.1008; auc: 0.7340 - val_auc: 0.7156                                                                                                    \n",
      "epoch: 0003, loss: 1.0707 - val_loss: 1.0575; auc: 0.7438 - val_auc: 0.7298                                                                                                    \n",
      "epoch: 0004, loss: 1.0379 - val_loss: 1.0420; auc: 0.7555 - val_auc: 0.7400                                                                                                    \n",
      "epoch: 0005, loss: 1.0137 - val_loss: 1.0162; auc: 0.7674 - val_auc: 0.7547                                                                                                    \n",
      "epoch: 0006, loss: 0.9977 - val_loss: 1.0064; auc: 0.7793 - val_auc: 0.7639                                                                                                    \n",
      "epoch: 0007, loss: 0.9887 - val_loss: 0.9958; auc: 0.7853 - val_auc: 0.7690                                                                                                    \n",
      "epoch: 0008, loss: 0.9728 - val_loss: 0.9800; auc: 0.7959 - val_auc: 0.7761                                                                                                    \n",
      "epoch: 0009, loss: 0.9610 - val_loss: 0.9699; auc: 0.8010 - val_auc: 0.7814                                                                                                    \n",
      "epoch: 0010, loss: 0.9457 - val_loss: 0.9660; auc: 0.8106 - val_auc: 0.7886                                                                                                    \n",
      "epoch: 0011, loss: 0.9363 - val_loss: 0.9488; auc: 0.8182 - val_auc: 0.7921                                                                                                    \n",
      "epoch: 0012, loss: 0.9267 - val_loss: 0.9704; auc: 0.8200 - val_auc: 0.7870                                                                                                    \n",
      "epoch: 0013, loss: 0.9225 - val_loss: 0.9472; auc: 0.8278 - val_auc: 0.7964                                                                                                    \n",
      "epoch: 0014, loss: 0.9111 - val_loss: 0.9320; auc: 0.8341 - val_auc: 0.7987                                                                                                    \n",
      "epoch: 0015, loss: 0.8997 - val_loss: 0.9329; auc: 0.8415 - val_auc: 0.8062                                                                                                    \n",
      "epoch: 0016, loss: 0.8866 - val_loss: 0.9328; auc: 0.8427 - val_auc: 0.8067                                                                                                    \n",
      "epoch: 0017, loss: 0.8809 - val_loss: 0.9155; auc: 0.8485 - val_auc: 0.8097                                                                                                    \n",
      "epoch: 0018, loss: 0.8674 - val_loss: 0.9169; auc: 0.8513 - val_auc: 0.8110                                                                                                    \n",
      "epoch: 0019, loss: 0.8577 - val_loss: 0.9067; auc: 0.8571 - val_auc: 0.8163                                                                                                    \n",
      "epoch: 0020, loss: 0.8602 - val_loss: 0.9228; auc: 0.8629 - val_auc: 0.8204                                                                                                    \n",
      "epoch: 0021, loss: 0.8436 - val_loss: 0.8965; auc: 0.8639 - val_auc: 0.8198                                                                                                    \n",
      "epoch: 0022, loss: 0.8389 - val_loss: 0.9066; auc: 0.8664 - val_auc: 0.8194                                                                                                    \n",
      "epoch: 0023, loss: 0.8324 - val_loss: 0.8877; auc: 0.8721 - val_auc: 0.8238                                                                                                    \n",
      "epoch: 0024, loss: 0.8150 - val_loss: 0.8908; auc: 0.8772 - val_auc: 0.8267                                                                                                    \n",
      "epoch: 0025, loss: 0.8115 - val_loss: 0.8810; auc: 0.8790 - val_auc: 0.8287                                                                                                    \n",
      "epoch: 0026, loss: 0.8086 - val_loss: 0.9041; auc: 0.8789 - val_auc: 0.8283                                                                                                    \n",
      "epoch: 0027, loss: 0.7979 - val_loss: 0.8752; auc: 0.8830 - val_auc: 0.8310                                                                                                    \n",
      "epoch: 0028, loss: 0.7826 - val_loss: 0.8831; auc: 0.8873 - val_auc: 0.8339                                                                                                    \n",
      "epoch: 0029, loss: 0.7802 - val_loss: 0.8752; auc: 0.8914 - val_auc: 0.8358                                                                                                    \n",
      "epoch: 0030, loss: 0.7858 - val_loss: 0.8817; auc: 0.8919 - val_auc: 0.8343                                                                                                    \n",
      "epoch: 0031, loss: 0.7680 - val_loss: 0.8986; auc: 0.8962 - val_auc: 0.8380                                                                                                    \n",
      "epoch: 0032, loss: 0.7616 - val_loss: 0.8578; auc: 0.8980 - val_auc: 0.8399                                                                                                    \n",
      "epoch: 0033, loss: 0.7493 - val_loss: 0.8538; auc: 0.9000 - val_auc: 0.8421                                                                                                    \n",
      "epoch: 0034, loss: 0.7453 - val_loss: 0.8457; auc: 0.9037 - val_auc: 0.8428                                                                                                    \n",
      "epoch: 0035, loss: 0.7391 - val_loss: 0.8762; auc: 0.9049 - val_auc: 0.8432                                                                                                    \n",
      "epoch: 0036, loss: 0.7364 - val_loss: 0.8496; auc: 0.9063 - val_auc: 0.8437                                                                                                    \n",
      "epoch: 0037, loss: 0.7204 - val_loss: 0.8775; auc: 0.9072 - val_auc: 0.8454                                                                                                    \n",
      "epoch: 0038, loss: 0.7157 - val_loss: 0.8562; auc: 0.9117 - val_auc: 0.8452                                                                                                    \n",
      "epoch: 0039, loss: 0.7107 - val_loss: 0.8885; auc: 0.9129 - val_auc: 0.8470                                                                                                    \n",
      "epoch: 0040, loss: 0.7145 - val_loss: 0.8905; auc: 0.9142 - val_auc: 0.8476                                                                                                    \n",
      "epoch: 0041, loss: 0.7040 - val_loss: 0.8335; auc: 0.9161 - val_auc: 0.8486                                                                                                    \n",
      "epoch: 0042, loss: 0.6960 - val_loss: 0.8401; auc: 0.9183 - val_auc: 0.8495                                                                                                    \n",
      "epoch: 0043, loss: 0.6923 - val_loss: 0.8298; auc: 0.9193 - val_auc: 0.8501                                                                                                    \n",
      "epoch: 0044, loss: 0.6812 - val_loss: 0.8382; auc: 0.9192 - val_auc: 0.8499                                                                                                    \n",
      "epoch: 0045, loss: 0.6866 - val_loss: 0.8351; auc: 0.9238 - val_auc: 0.8506                                                                                                    \n",
      "epoch: 0046, loss: 0.6641 - val_loss: 0.8385; auc: 0.9236 - val_auc: 0.8518                                                                                                    \n",
      "epoch: 0047, loss: 0.6708 - val_loss: 0.8514; auc: 0.9270 - val_auc: 0.8521                                                                                                    \n",
      "epoch: 0048, loss: 0.6610 - val_loss: 0.8901; auc: 0.9284 - val_auc: 0.8531                                                                                                    \n",
      "epoch: 0049, loss: 0.6504 - val_loss: 0.8623; auc: 0.9299 - val_auc: 0.8536                                                                                                    \n",
      "epoch: 0050, loss: 0.6477 - val_loss: 0.8253; auc: 0.9305 - val_auc: 0.8543                                                                                                    \n",
      "epoch: 0051, loss: 0.6391 - val_loss: 0.8272; auc: 0.9313 - val_auc: 0.8549                                                                                                    \n",
      "epoch: 0052, loss: 0.6390 - val_loss: 0.8342; auc: 0.9320 - val_auc: 0.8541                                                                                                    \n",
      "epoch: 0053, loss: 0.6243 - val_loss: 0.8264; auc: 0.9346 - val_auc: 0.8556                                                                                                    \n",
      "epoch: 0054, loss: 0.6230 - val_loss: 0.8651; auc: 0.9369 - val_auc: 0.8547                                                                                                    \n",
      "epoch: 0055, loss: 0.6193 - val_loss: 0.8340; auc: 0.9367 - val_auc: 0.8556                                                                                                    \n",
      "epoch: 0056, loss: 0.6121 - val_loss: 0.8434; auc: 0.9382 - val_auc: 0.8550                                                                                                    \n",
      "epoch: 0057, loss: 0.6122 - val_loss: 0.8692; auc: 0.9402 - val_auc: 0.8568                                                                                                    \n",
      "epoch: 0058, loss: 0.6086 - val_loss: 0.8396; auc: 0.9388 - val_auc: 0.8584                                                                                                    \n",
      "epoch: 0059, loss: 0.5936 - val_loss: 0.8279; auc: 0.9411 - val_auc: 0.8588                                                                                                    \n",
      "epoch: 0060, loss: 0.5948 - val_loss: 0.8790; auc: 0.9429 - val_auc: 0.8569                                                                                                    \n",
      "epoch: 0061, loss: 0.5851 - val_loss: 0.8224; auc: 0.9425 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0062, loss: 0.5771 - val_loss: 0.8406; auc: 0.9442 - val_auc: 0.8580                                                                                                    \n",
      "epoch: 0063, loss: 0.5710 - val_loss: 0.8613; auc: 0.9458 - val_auc: 0.8580                                                                                                    \n",
      "epoch: 0064, loss: 0.5715 - val_loss: 0.8547; auc: 0.9471 - val_auc: 0.8581                                                                                                    \n",
      "epoch: 0065, loss: 0.5618 - val_loss: 0.9090; auc: 0.9480 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0066, loss: 0.5655 - val_loss: 0.8746; auc: 0.9483 - val_auc: 0.8589                                                                                                    \n",
      "epoch: 0067, loss: 0.5553 - val_loss: 0.8621; auc: 0.9499 - val_auc: 0.8590                                                                                                    \n",
      "epoch: 0068, loss: 0.5531 - val_loss: 0.8636; auc: 0.9510 - val_auc: 0.8582                                                                                                    \n",
      "epoch: 0069, loss: 0.5471 - val_loss: 0.8464; auc: 0.9506 - val_auc: 0.8592                                                                                                    \n",
      "epoch: 0070, loss: 0.5448 - val_loss: 0.8769; auc: 0.9524 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0071, loss: 0.5383 - val_loss: 0.8425; auc: 0.9529 - val_auc: 0.8595                                                                                                    \n",
      "epoch: 0072, loss: 0.5342 - val_loss: 0.8828; auc: 0.9543 - val_auc: 0.8580                                                                                                    \n",
      "epoch: 0073, loss: 0.5289 - val_loss: 0.9367; auc: 0.9545 - val_auc: 0.8609                                                                                                    \n",
      "epoch: 0074, loss: 0.5223 - val_loss: 0.8936; auc: 0.9550 - val_auc: 0.8602                                                                                                    \n",
      "epoch: 0075, loss: 0.5225 - val_loss: 0.8366; auc: 0.9545 - val_auc: 0.8603                                                                                                    \n",
      "epoch: 0076, loss: 0.5200 - val_loss: 0.8624; auc: 0.9567 - val_auc: 0.8589                                                                                                    \n",
      "epoch: 0077, loss: 0.5107 - val_loss: 0.8671; auc: 0.9567 - val_auc: 0.8606                                                                                                    \n",
      "epoch: 0078, loss: 0.5058 - val_loss: 0.8895; auc: 0.9571 - val_auc: 0.8603                                                                                                    \n",
      "epoch: 0079, loss: 0.4978 - val_loss: 0.8930; auc: 0.9584 - val_auc: 0.8617                                                                                                    \n",
      "epoch: 0080, loss: 0.4941 - val_loss: 0.8453; auc: 0.9593 - val_auc: 0.8609                                                                                                    \n",
      "epoch: 0081, loss: 0.4940 - val_loss: 0.8927; auc: 0.9593 - val_auc: 0.8618                                                                                                    \n",
      "epoch: 0082, loss: 0.4909 - val_loss: 0.9153; auc: 0.9607 - val_auc: 0.8607                                                                                                    \n",
      "epoch: 0083, loss: 0.4835 - val_loss: 0.8975; auc: 0.9612 - val_auc: 0.8588                                                                                                    \n",
      "epoch: 0084, loss: 0.4799 - val_loss: 0.8857; auc: 0.9611 - val_auc: 0.8600                                                                                                    \n",
      "epoch: 0085, loss: 0.4742 - val_loss: 0.9672; auc: 0.9628 - val_auc: 0.8594                                                                                                    \n",
      "epoch: 0086, loss: 0.4795 - val_loss: 0.9110; auc: 0.9631 - val_auc: 0.8586                                                                                                    \n",
      "epoch: 0087, loss: 0.4709 - val_loss: 0.9239; auc: 0.9630 - val_auc: 0.8617                                                                                                    \n",
      "epoch: 0088, loss: 0.4651 - val_loss: 0.9180; auc: 0.9639 - val_auc: 0.8620                                                                                                    \n",
      "epoch: 0089, loss: 0.4688 - val_loss: 0.9128; auc: 0.9642 - val_auc: 0.8599                                                                                                    \n",
      "epoch: 0090, loss: 0.4623 - val_loss: 0.9075; auc: 0.9642 - val_auc: 0.8592                                                                                                    \n",
      "epoch: 0091, loss: 0.4556 - val_loss: 1.0299; auc: 0.9657 - val_auc: 0.8600                                                                                                    \n",
      "epoch: 0092, loss: 0.4486 - val_loss: 1.0083; auc: 0.9665 - val_auc: 0.8578                                                                                                    \n",
      "epoch: 0093, loss: 0.4497 - val_loss: 0.9457; auc: 0.9666 - val_auc: 0.8608                                                                                                    \n",
      "epoch: 0094, loss: 0.4452 - val_loss: 0.9761; auc: 0.9675 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0095, loss: 0.4398 - val_loss: 0.8997; auc: 0.9674 - val_auc: 0.8607                                                                                                    \n",
      "epoch: 0096, loss: 0.4363 - val_loss: 0.9075; auc: 0.9678 - val_auc: 0.8616                                                                                                    \n",
      "epoch: 0097, loss: 0.4352 - val_loss: 1.0056; auc: 0.9688 - val_auc: 0.8613                                                                                                    \n",
      "epoch: 0098, loss: 0.4298 - val_loss: 0.9395; auc: 0.9685 - val_auc: 0.8618                                                                                                    \n",
      "epoch: 0099, loss: 0.4330 - val_loss: 1.0363; auc: 0.9700 - val_auc: 0.8589                                                                                                    \n",
      "epoch: 0100, loss: 0.4278 - val_loss: 0.8984; auc: 0.9695 - val_auc: 0.8607                                                                                                    \n",
      "epoch: 0101, loss: 0.4248 - val_loss: 0.9354; auc: 0.9696 - val_auc: 0.8598                                                                                                    \n",
      "epoch: 0102, loss: 0.4254 - val_loss: 1.1020; auc: 0.9708 - val_auc: 0.8594                                                                                                    \n",
      "epoch: 0103, loss: 0.4191 - val_loss: 0.9619; auc: 0.9704 - val_auc: 0.8610                                                                                                    \n",
      "epoch: 0104, loss: 0.4177 - val_loss: 1.0230; auc: 0.9712 - val_auc: 0.8608                                                                                                    \n",
      "epoch: 0105, loss: 0.4113 - val_loss: 0.9659; auc: 0.9716 - val_auc: 0.8622                                                                                                    \n",
      "epoch: 0106, loss: 0.4072 - val_loss: 0.9706; auc: 0.9723 - val_auc: 0.8610                                                                                                    \n",
      "epoch: 0107, loss: 0.4062 - val_loss: 1.0241; auc: 0.9729 - val_auc: 0.8601                                                                                                    \n",
      "epoch: 0108, loss: 0.4019 - val_loss: 1.0947; auc: 0.9735 - val_auc: 0.8591                                                                                                    \n",
      "epoch: 0109, loss: 0.4006 - val_loss: 1.0437; auc: 0.9739 - val_auc: 0.8602                                                                                                    \n",
      "epoch: 0110, loss: 0.3951 - val_loss: 1.1288; auc: 0.9747 - val_auc: 0.8589                                                                                                    \n",
      "epoch: 0111, loss: 0.3932 - val_loss: 0.9926; auc: 0.9744 - val_auc: 0.8608                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00111: early stopping\n",
      "Train on 6249 samples, validate on 781 samples\n",
      "Epoch 1/61\n",
      "6249/6249 [==============================] - 5s 766us/sample - loss: 1.1703 - val_loss: 1.1608\n",
      "Epoch 2/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 1.1359 - val_loss: 1.1170\n",
      "Epoch 3/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 1.0849 - val_loss: 1.0860\n",
      "Epoch 4/61\n",
      "6249/6249 [==============================] - 3s 532us/sample - loss: 1.0497 - val_loss: 1.0462\n",
      "Epoch 5/61\n",
      "6249/6249 [==============================] - 3s 542us/sample - loss: 1.0245 - val_loss: 1.0349\n",
      "Epoch 6/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 1.0049 - val_loss: 1.0134\n",
      "Epoch 7/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.9872 - val_loss: 0.9956\n",
      "Epoch 8/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.9747 - val_loss: 0.9883\n",
      "Epoch 9/61\n",
      "6249/6249 [==============================] - 3s 530us/sample - loss: 0.9664 - val_loss: 0.9973\n",
      "Epoch 10/61\n",
      "6249/6249 [==============================] - 3s 539us/sample - loss: 0.9522 - val_loss: 0.9771\n",
      "Epoch 11/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.9455 - val_loss: 0.9621\n",
      "Epoch 12/61\n",
      "6249/6249 [==============================] - 3s 540us/sample - loss: 0.9277 - val_loss: 0.9630\n",
      "Epoch 13/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.9296 - val_loss: 0.9525\n",
      "Epoch 14/61\n",
      "6249/6249 [==============================] - 3s 530us/sample - loss: 0.9093 - val_loss: 0.9453\n",
      "Epoch 15/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.9016 - val_loss: 0.9540\n",
      "Epoch 16/61\n",
      "6249/6249 [==============================] - 3s 528us/sample - loss: 0.8931 - val_loss: 0.9676\n",
      "Epoch 17/61\n",
      "6249/6249 [==============================] - 3s 534us/sample - loss: 0.8836 - val_loss: 0.9404\n",
      "Epoch 18/61\n",
      "6249/6249 [==============================] - 3s 521us/sample - loss: 0.8708 - val_loss: 0.9236\n",
      "Epoch 19/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.8692 - val_loss: 0.9232\n",
      "Epoch 20/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.8568 - val_loss: 0.9169\n",
      "Epoch 21/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 0.8568 - val_loss: 0.9275\n",
      "Epoch 22/61\n",
      "6249/6249 [==============================] - 3s 522us/sample - loss: 0.8409 - val_loss: 0.9049\n",
      "Epoch 23/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.8278 - val_loss: 0.9025\n",
      "Epoch 24/61\n",
      "6249/6249 [==============================] - 3s 527us/sample - loss: 0.8194 - val_loss: 0.9010\n",
      "Epoch 25/61\n",
      "6249/6249 [==============================] - 3s 527us/sample - loss: 0.8120 - val_loss: 0.9090\n",
      "Epoch 26/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.8084 - val_loss: 0.8942\n",
      "Epoch 27/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.7929 - val_loss: 0.8928\n",
      "Epoch 28/61\n",
      "6249/6249 [==============================] - 3s 542us/sample - loss: 0.7963 - val_loss: 0.8880\n",
      "Epoch 29/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.7831 - val_loss: 0.9182\n",
      "Epoch 30/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.7741 - val_loss: 0.8819\n",
      "Epoch 31/61\n",
      "6249/6249 [==============================] - 3s 525us/sample - loss: 0.7693 - val_loss: 0.9042\n",
      "Epoch 32/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.7619 - val_loss: 0.8899\n",
      "Epoch 33/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.7500 - val_loss: 0.8832\n",
      "Epoch 34/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 0.7392 - val_loss: 0.8792\n",
      "Epoch 35/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.7445 - val_loss: 0.8745\n",
      "Epoch 36/61\n",
      "6249/6249 [==============================] - 3s 528us/sample - loss: 0.7316 - val_loss: 0.8616\n",
      "Epoch 37/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 0.7237 - val_loss: 0.8593\n",
      "Epoch 38/61\n",
      "6249/6249 [==============================] - 3s 528us/sample - loss: 0.7170 - val_loss: 0.8721\n",
      "Epoch 39/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 0.7131 - val_loss: 0.8716\n",
      "Epoch 40/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.7044 - val_loss: 0.8985\n",
      "Epoch 41/61\n",
      "6249/6249 [==============================] - 3s 546us/sample - loss: 0.6949 - val_loss: 0.8769\n",
      "Epoch 42/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 0.6889 - val_loss: 0.8655\n",
      "Epoch 43/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 0.6820 - val_loss: 0.8863\n",
      "Epoch 44/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.6795 - val_loss: 0.8746\n",
      "Epoch 45/61\n",
      "6249/6249 [==============================] - 3s 531us/sample - loss: 0.6712 - val_loss: 0.8674\n",
      "Epoch 46/61\n",
      "6249/6249 [==============================] - 3s 542us/sample - loss: 0.6745 - val_loss: 0.8665\n",
      "Epoch 47/61\n",
      "6249/6249 [==============================] - 3s 531us/sample - loss: 0.6573 - val_loss: 0.8500\n",
      "Epoch 48/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.6704 - val_loss: 0.8836\n",
      "Epoch 49/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 0.6460 - val_loss: 0.8543\n",
      "Epoch 50/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 0.6485 - val_loss: 0.8927\n",
      "Epoch 51/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.6408 - val_loss: 0.8566\n",
      "Epoch 52/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 0.6349 - val_loss: 0.8602\n",
      "Epoch 53/61\n",
      "6249/6249 [==============================] - 3s 541us/sample - loss: 0.6218 - val_loss: 0.8627\n",
      "Epoch 54/61\n",
      "6249/6249 [==============================] - 3s 542us/sample - loss: 0.6179 - val_loss: 0.9174\n",
      "Epoch 55/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.6250 - val_loss: 0.8624\n",
      "Epoch 56/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.6113 - val_loss: 0.9264\n",
      "Epoch 57/61\n",
      "6249/6249 [==============================] - 3s 542us/sample - loss: 0.6049 - val_loss: 0.8783\n",
      "Epoch 58/61\n",
      "6249/6249 [==============================] - 3s 542us/sample - loss: 0.5938 - val_loss: 0.8800\n",
      "Epoch 59/61\n",
      "6249/6249 [==============================] - 3s 549us/sample - loss: 0.5915 - val_loss: 0.8806\n",
      "Epoch 60/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.5877 - val_loss: 0.8965\n",
      "Epoch 61/61\n",
      "6249/6249 [==============================] - 3s 522us/sample - loss: 0.5951 - val_loss: 0.8769\n",
      "Train on 6249 samples, validate on 781 samples\n",
      "Epoch 1/61\n",
      "6249/6249 [==============================] - 5s 811us/sample - loss: 1.1703 - val_loss: 1.1602\n",
      "Epoch 2/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 1.1347 - val_loss: 1.1157\n",
      "Epoch 3/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 1.0830 - val_loss: 1.0843\n",
      "Epoch 4/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 1.0487 - val_loss: 1.0470\n",
      "Epoch 5/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 1.0236 - val_loss: 1.0360\n",
      "Epoch 6/61\n",
      "6249/6249 [==============================] - 3s 532us/sample - loss: 1.0049 - val_loss: 1.0118\n",
      "Epoch 7/61\n",
      "6249/6249 [==============================] - 3s 521us/sample - loss: 0.9864 - val_loss: 0.9938\n",
      "Epoch 8/61\n",
      "6249/6249 [==============================] - 3s 534us/sample - loss: 0.9738 - val_loss: 0.9885\n",
      "Epoch 9/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 0.9658 - val_loss: 0.9955\n",
      "Epoch 10/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.9507 - val_loss: 0.9773\n",
      "Epoch 11/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.9439 - val_loss: 0.9597\n",
      "Epoch 12/61\n",
      "6249/6249 [==============================] - 3s 541us/sample - loss: 0.9261 - val_loss: 0.9600\n",
      "Epoch 13/61\n",
      "6249/6249 [==============================] - 3s 539us/sample - loss: 0.9260 - val_loss: 0.9488\n",
      "Epoch 14/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.9077 - val_loss: 0.9435\n",
      "Epoch 15/61\n",
      "6249/6249 [==============================] - 3s 532us/sample - loss: 0.8995 - val_loss: 0.9541\n",
      "Epoch 16/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.8905 - val_loss: 0.9590\n",
      "Epoch 17/61\n",
      "6249/6249 [==============================] - 3s 520us/sample - loss: 0.8805 - val_loss: 0.9371\n",
      "Epoch 18/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.8676 - val_loss: 0.9219\n",
      "Epoch 19/61\n",
      "6249/6249 [==============================] - 3s 543us/sample - loss: 0.8667 - val_loss: 0.9215\n",
      "Epoch 20/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.8546 - val_loss: 0.9155\n",
      "Epoch 21/61\n",
      "6249/6249 [==============================] - 3s 525us/sample - loss: 0.8540 - val_loss: 0.9230\n",
      "Epoch 22/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 0.8382 - val_loss: 0.9027\n",
      "Epoch 23/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 0.8256 - val_loss: 0.9011\n",
      "Epoch 24/61\n",
      "6249/6249 [==============================] - 3s 534us/sample - loss: 0.8177 - val_loss: 0.8995\n",
      "Epoch 25/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.8095 - val_loss: 0.9118\n",
      "Epoch 26/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.8068 - val_loss: 0.8942\n",
      "Epoch 27/61\n",
      "6249/6249 [==============================] - 3s 540us/sample - loss: 0.7899 - val_loss: 0.8936\n",
      "Epoch 28/61\n",
      "6249/6249 [==============================] - 3s 524us/sample - loss: 0.7935 - val_loss: 0.8863\n",
      "Epoch 29/61\n",
      "6249/6249 [==============================] - 3s 530us/sample - loss: 0.7809 - val_loss: 0.9227\n",
      "Epoch 30/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 0.7710 - val_loss: 0.8837\n",
      "Epoch 31/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.7664 - val_loss: 0.9064\n",
      "Epoch 32/61\n",
      "6249/6249 [==============================] - 3s 525us/sample - loss: 0.7581 - val_loss: 0.8895\n",
      "Epoch 33/61\n",
      "6249/6249 [==============================] - 3s 543us/sample - loss: 0.7455 - val_loss: 0.8829\n",
      "Epoch 34/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.7355 - val_loss: 0.8808\n",
      "Epoch 35/61\n",
      "6249/6249 [==============================] - 3s 527us/sample - loss: 0.7427 - val_loss: 0.8749\n",
      "Epoch 36/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.7297 - val_loss: 0.8610\n",
      "Epoch 37/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 0.7202 - val_loss: 0.8575\n",
      "Epoch 38/61\n",
      "6249/6249 [==============================] - 3s 530us/sample - loss: 0.7132 - val_loss: 0.8690\n",
      "Epoch 39/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 0.7089 - val_loss: 0.8723\n",
      "Epoch 40/61\n",
      "6249/6249 [==============================] - 3s 521us/sample - loss: 0.7012 - val_loss: 0.8946\n",
      "Epoch 41/61\n",
      "6249/6249 [==============================] - 3s 527us/sample - loss: 0.6910 - val_loss: 0.8783\n",
      "Epoch 42/61\n",
      "6249/6249 [==============================] - 3s 526us/sample - loss: 0.6855 - val_loss: 0.8615\n",
      "Epoch 43/61\n",
      "6249/6249 [==============================] - 3s 530us/sample - loss: 0.6784 - val_loss: 0.8894\n",
      "Epoch 44/61\n",
      "6249/6249 [==============================] - 3s 521us/sample - loss: 0.6763 - val_loss: 0.8734\n",
      "Epoch 45/61\n",
      "6249/6249 [==============================] - 3s 540us/sample - loss: 0.6682 - val_loss: 0.8633\n",
      "Epoch 46/61\n",
      "6249/6249 [==============================] - 3s 519us/sample - loss: 0.6690 - val_loss: 0.8663\n",
      "Epoch 47/61\n",
      "6249/6249 [==============================] - 3s 531us/sample - loss: 0.6534 - val_loss: 0.8476\n",
      "Epoch 48/61\n",
      "6249/6249 [==============================] - 3s 532us/sample - loss: 0.6665 - val_loss: 0.8771\n",
      "Epoch 49/61\n",
      "6249/6249 [==============================] - 3s 535us/sample - loss: 0.6422 - val_loss: 0.8524\n",
      "Epoch 50/61\n",
      "6249/6249 [==============================] - 3s 520us/sample - loss: 0.6434 - val_loss: 0.8802\n",
      "Epoch 51/61\n",
      "6249/6249 [==============================] - 3s 528us/sample - loss: 0.6365 - val_loss: 0.8567\n",
      "Epoch 52/61\n",
      "6249/6249 [==============================] - 3s 537us/sample - loss: 0.6311 - val_loss: 0.8557\n",
      "Epoch 53/61\n",
      "6249/6249 [==============================] - 3s 534us/sample - loss: 0.6169 - val_loss: 0.8591\n",
      "Epoch 54/61\n",
      "6249/6249 [==============================] - 3s 545us/sample - loss: 0.6139 - val_loss: 0.9121\n",
      "Epoch 55/61\n",
      "6249/6249 [==============================] - 3s 541us/sample - loss: 0.6220 - val_loss: 0.8608\n",
      "Epoch 56/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.6084 - val_loss: 0.9268\n",
      "Epoch 57/61\n",
      "6249/6249 [==============================] - 3s 538us/sample - loss: 0.6017 - val_loss: 0.8767\n",
      "Epoch 58/61\n",
      "6249/6249 [==============================] - 3s 536us/sample - loss: 0.5903 - val_loss: 0.8743\n",
      "Epoch 59/61\n",
      "6249/6249 [==============================] - 3s 533us/sample - loss: 0.5878 - val_loss: 0.8819\n",
      "Epoch 60/61\n",
      "6249/6249 [==============================] - 3s 529us/sample - loss: 0.5844 - val_loss: 0.8920\n",
      "Epoch 61/61\n",
      "6249/6249 [==============================] - 3s 523us/sample - loss: 0.5902 - val_loss: 0.8745\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, seed in enumerate([7, 77, 77]):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "    performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9abd328b00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.841642517088629"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00034273805115751885"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Tox21</td>\n",
       "      <td>0.942460</td>\n",
       "      <td>0.858518</td>\n",
       "      <td>0.841801</td>\n",
       "      <td>ROC</td>\n",
       "      <td>769708</td>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Tox21</td>\n",
       "      <td>0.939267</td>\n",
       "      <td>0.844432</td>\n",
       "      <td>0.841249</td>\n",
       "      <td>ROC</td>\n",
       "      <td>769708</td>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Tox21</td>\n",
       "      <td>0.940094</td>\n",
       "      <td>0.845946</td>\n",
       "      <td>0.841877</td>\n",
       "      <td>ROC</td>\n",
       "      <td>769708</td>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0     Tox21   0.942460   0.858518  0.841801    ROC              769708   \n",
       "1     Tox21   0.939267   0.844432  0.841249    ROC              769708   \n",
       "2     Tox21   0.940094   0.845946  0.841877    ROC              769708   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0          60         128  0.0001             0  \n",
       "1          60         128  0.0001             0  \n",
       "2          60         128  0.0001             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
