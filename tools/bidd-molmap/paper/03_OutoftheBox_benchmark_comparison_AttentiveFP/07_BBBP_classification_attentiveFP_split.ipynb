{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [20:34:18] Enabling RDKit 2019.09.2 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attentiveFP_idx(df):\n",
    "    \"\"\" attentiveFP dataset\"\"\"\n",
    "    train, valid,test = load('./split_and_data/07_BBBP_attentiveFP.data')\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train), len(valid), len(test)))\n",
    "    train_idx = df[df.smiles.isin(train.smiles)].index\n",
    "    valid_idx = df[df.smiles.isin(valid.smiles)].index\n",
    "    test_idx = df[df.smiles.isin(test.smiles)].index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: BBBP number of split times: 3\n",
      "training set: 1628, valid set: 204, test set 204\n",
      "training set: 1628, valid set: 204, test set 204\n"
     ]
    }
   ],
   "source": [
    "task_name = 'BBBP'\n",
    "\n",
    "from chembench import load_data\n",
    "df, _ = load_data(task_name)\n",
    "\n",
    "train_idx, valid_idx, test_idx = get_attentiveFP_idx(df) \n",
    "len(train_idx), len(valid_idx), len(test_idx)\n",
    "\n",
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_dir = '../02_OutofTheBox_benchmark_comparison_DMPNN/tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)\n",
    "\n",
    "    \n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').values\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1628 204 204\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "trainX = (X1[train_idx], X2[train_idx])\n",
    "trainY = Y[train_idx]\n",
    "\n",
    "validX = (X1[valid_idx], X2[valid_idx])\n",
    "validY = Y[valid_idx]\n",
    "\n",
    "testX = (X1[test_idx], X2[test_idx])\n",
    "testY = Y[test_idx]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256, 128, 32]\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, loss: 0.3060 - val_loss: 0.3057; auc: 0.8488 - val_auc: 0.8665                                                                                                    \n",
      "epoch: 0002, loss: 0.2888 - val_loss: 0.2801; auc: 0.8528 - val_auc: 0.8652                                                                                                    \n",
      "epoch: 0003, loss: 0.2643 - val_loss: 0.2489; auc: 0.8491 - val_auc: 0.8584                                                                                                    \n",
      "epoch: 0004, loss: 0.2397 - val_loss: 0.2218; auc: 0.8555 - val_auc: 0.8608                                                                                                    \n",
      "epoch: 0005, loss: 0.2233 - val_loss: 0.2204; auc: 0.8677 - val_auc: 0.8688                                                                                                    \n",
      "epoch: 0006, loss: 0.2073 - val_loss: 0.1994; auc: 0.8805 - val_auc: 0.8822                                                                                                    \n",
      "epoch: 0007, loss: 0.2163 - val_loss: 0.2129; auc: 0.8907 - val_auc: 0.8891                                                                                                    \n",
      "epoch: 0008, loss: 0.1993 - val_loss: 0.1883; auc: 0.8986 - val_auc: 0.8956                                                                                                    \n",
      "epoch: 0009, loss: 0.1902 - val_loss: 0.1839; auc: 0.9046 - val_auc: 0.9009                                                                                                    \n",
      "epoch: 0010, loss: 0.1845 - val_loss: 0.1748; auc: 0.9104 - val_auc: 0.9066                                                                                                    \n",
      "epoch: 0011, loss: 0.1798 - val_loss: 0.1710; auc: 0.9177 - val_auc: 0.9104                                                                                                    \n",
      "epoch: 0012, loss: 0.1748 - val_loss: 0.1705; auc: 0.9243 - val_auc: 0.9177                                                                                                    \n",
      "epoch: 0013, loss: 0.1709 - val_loss: 0.1810; auc: 0.9260 - val_auc: 0.9226                                                                                                    \n",
      "epoch: 0014, loss: 0.1685 - val_loss: 0.1624; auc: 0.9330 - val_auc: 0.9252                                                                                                    \n",
      "epoch: 0015, loss: 0.1566 - val_loss: 0.1620; auc: 0.9391 - val_auc: 0.9305                                                                                                    \n",
      "epoch: 0016, loss: 0.1517 - val_loss: 0.1563; auc: 0.9431 - val_auc: 0.9335                                                                                                    \n",
      "epoch: 0017, loss: 0.1505 - val_loss: 0.1825; auc: 0.9462 - val_auc: 0.9326                                                                                                    \n",
      "epoch: 0018, loss: 0.1514 - val_loss: 0.1545; auc: 0.9493 - val_auc: 0.9348                                                                                                    \n",
      "epoch: 0019, loss: 0.1387 - val_loss: 0.1499; auc: 0.9535 - val_auc: 0.9386                                                                                                    \n",
      "epoch: 0020, loss: 0.1342 - val_loss: 0.1584; auc: 0.9576 - val_auc: 0.9394                                                                                                    \n",
      "epoch: 0021, loss: 0.1308 - val_loss: 0.1500; auc: 0.9603 - val_auc: 0.9403                                                                                                    \n",
      "epoch: 0022, loss: 0.1231 - val_loss: 0.1525; auc: 0.9640 - val_auc: 0.9441                                                                                                    \n",
      "epoch: 0023, loss: 0.1284 - val_loss: 0.1497; auc: 0.9660 - val_auc: 0.9430                                                                                                    \n",
      "epoch: 0024, loss: 0.1360 - val_loss: 0.1791; auc: 0.9668 - val_auc: 0.9412                                                                                                    \n",
      "epoch: 0025, loss: 0.1170 - val_loss: 0.1445; auc: 0.9698 - val_auc: 0.9436                                                                                                    \n",
      "epoch: 0026, loss: 0.1128 - val_loss: 0.1429; auc: 0.9719 - val_auc: 0.9440                                                                                                    \n",
      "epoch: 0027, loss: 0.1060 - val_loss: 0.1580; auc: 0.9741 - val_auc: 0.9454                                                                                                    \n",
      "epoch: 0028, loss: 0.1005 - val_loss: 0.1493; auc: 0.9772 - val_auc: 0.9448                                                                                                    \n",
      "epoch: 0029, loss: 0.0977 - val_loss: 0.1580; auc: 0.9793 - val_auc: 0.9448                                                                                                    \n",
      "epoch: 0030, loss: 0.0968 - val_loss: 0.1423; auc: 0.9795 - val_auc: 0.9437                                                                                                    \n",
      "epoch: 0031, loss: 0.0970 - val_loss: 0.1880; auc: 0.9825 - val_auc: 0.9425                                                                                                    \n",
      "epoch: 0032, loss: 0.0838 - val_loss: 0.1453; auc: 0.9831 - val_auc: 0.9443                                                                                                    \n",
      "epoch: 0033, loss: 0.0805 - val_loss: 0.1812; auc: 0.9855 - val_auc: 0.9445                                                                                                    \n",
      "epoch: 0034, loss: 0.0805 - val_loss: 0.1559; auc: 0.9861 - val_auc: 0.9439                                                                                                    \n",
      "epoch: 0035, loss: 0.0746 - val_loss: 0.1482; auc: 0.9876 - val_auc: 0.9428                                                                                                    \n",
      "epoch: 0036, loss: 0.0697 - val_loss: 0.1735; auc: 0.9885 - val_auc: 0.9443                                                                                                    \n",
      "epoch: 0037, loss: 0.0679 - val_loss: 0.1903; auc: 0.9891 - val_auc: 0.9432                                                                                                    \n",
      "epoch: 0038, loss: 0.0631 - val_loss: 0.1532; auc: 0.9900 - val_auc: 0.9427                                                                                                    \n",
      "epoch: 0039, loss: 0.0612 - val_loss: 0.2298; auc: 0.9904 - val_auc: 0.9376                                                                                                    \n",
      "epoch: 0040, loss: 0.0566 - val_loss: 0.1578; auc: 0.9908 - val_auc: 0.9436                                                                                                    \n",
      "epoch: 0041, loss: 0.0532 - val_loss: 0.1905; auc: 0.9918 - val_auc: 0.9396                                                                                                    \n",
      "epoch: 0042, loss: 0.0494 - val_loss: 0.2155; auc: 0.9923 - val_auc: 0.9410                                                                                                    \n",
      "epoch: 0043, loss: 0.0468 - val_loss: 0.2466; auc: 0.9930 - val_auc: 0.9377                                                                                                    \n",
      "epoch: 0044, loss: 0.0462 - val_loss: 0.2177; auc: 0.9935 - val_auc: 0.9376                                                                                                    \n",
      "epoch: 0045, loss: 0.0427 - val_loss: 0.1878; auc: 0.9942 - val_auc: 0.9390                                                                                                    \n",
      "epoch: 0046, loss: 0.0468 - val_loss: 0.2289; auc: 0.9944 - val_auc: 0.9349                                                                                                    \n",
      "epoch: 0047, loss: 0.0391 - val_loss: 0.2586; auc: 0.9951 - val_auc: 0.9341                                                                                                    \n",
      "epoch: 0048, loss: 0.0384 - val_loss: 0.1863; auc: 0.9957 - val_auc: 0.9381                                                                                                    \n",
      "epoch: 0049, loss: 0.0405 - val_loss: 0.2035; auc: 0.9960 - val_auc: 0.9400                                                                                                    \n",
      "epoch: 0050, loss: 0.0347 - val_loss: 0.2995; auc: 0.9966 - val_auc: 0.9334                                                                                                    \n",
      "epoch: 0051, loss: 0.0342 - val_loss: 0.1820; auc: 0.9969 - val_auc: 0.9387                                                                                                    \n",
      "epoch: 0052, loss: 0.0284 - val_loss: 0.2451; auc: 0.9977 - val_auc: 0.9355                                                                                                    \n",
      "epoch: 0053, loss: 0.0261 - val_loss: 0.2427; auc: 0.9982 - val_auc: 0.9343                                                                                                    \n",
      "epoch: 0054, loss: 0.0260 - val_loss: 0.2676; auc: 0.9983 - val_auc: 0.9334                                                                                                    \n",
      "epoch: 0055, loss: 0.0243 - val_loss: 0.2602; auc: 0.9987 - val_auc: 0.9330                                                                                                    \n",
      "epoch: 0056, loss: 0.0229 - val_loss: 0.2737; auc: 0.9987 - val_auc: 0.9334                                                                                                    \n",
      "epoch: 0057, loss: 0.0211 - val_loss: 0.3067; auc: 0.9989 - val_auc: 0.9307                                                                                                    \n",
      "epoch: 0058, loss: 0.0220 - val_loss: 0.3349; auc: 0.9990 - val_auc: 0.9302                                                                                                    \n",
      "epoch: 0059, loss: 0.0208 - val_loss: 0.2369; auc: 0.9991 - val_auc: 0.9335                                                                                                    \n",
      "epoch: 0060, loss: 0.0190 - val_loss: 0.2921; auc: 0.9991 - val_auc: 0.9296                                                                                                    \n",
      "epoch: 0061, loss: 0.0206 - val_loss: 0.2541; auc: 0.9991 - val_auc: 0.9334                                                                                                    \n",
      "epoch: 0062, loss: 0.0174 - val_loss: 0.3034; auc: 0.9991 - val_auc: 0.9300                                                                                                    \n",
      "epoch: 0063, loss: 0.0214 - val_loss: 0.1918; auc: 0.9991 - val_auc: 0.9344                                                                                                    \n",
      "epoch: 0064, loss: 0.0218 - val_loss: 0.2016; auc: 0.9992 - val_auc: 0.9368                                                                                                    \n",
      "epoch: 0065, loss: 0.0218 - val_loss: 0.3486; auc: 0.9992 - val_auc: 0.9295                                                                                                    \n",
      "epoch: 0066, loss: 0.0169 - val_loss: 0.3765; auc: 0.9992 - val_auc: 0.9245                                                                                                    \n",
      "epoch: 0067, loss: 0.0177 - val_loss: 0.4002; auc: 0.9992 - val_auc: 0.9267                                                                                                    \n",
      "epoch: 0068, loss: 0.0177 - val_loss: 0.2772; auc: 0.9992 - val_auc: 0.9302                                                                                                    \n",
      "epoch: 0069, loss: 0.0143 - val_loss: 0.3354; auc: 0.9993 - val_auc: 0.9280                                                                                                    \n",
      "epoch: 0070, loss: 0.0135 - val_loss: 0.2867; auc: 0.9993 - val_auc: 0.9305                                                                                                    \n",
      "epoch: 0071, loss: 0.0131 - val_loss: 0.2847; auc: 0.9993 - val_auc: 0.9303                                                                                                    \n",
      "epoch: 0072, loss: 0.0144 - val_loss: 0.2904; auc: 0.9993 - val_auc: 0.9285                                                                                                    \n",
      "epoch: 0073, loss: 0.0167 - val_loss: 0.2467; auc: 0.9994 - val_auc: 0.9296                                                                                                    \n",
      "epoch: 0074, loss: 0.0170 - val_loss: 0.3256; auc: 0.9993 - val_auc: 0.9295                                                                                                    \n",
      "epoch: 0075, loss: 0.0181 - val_loss: 0.4798; auc: 0.9992 - val_auc: 0.9234                                                                                                    \n",
      "epoch: 0076, loss: 0.0183 - val_loss: 0.2225; auc: 0.9994 - val_auc: 0.9312                                                                                                    \n",
      "epoch: 0077, loss: 0.0198 - val_loss: 0.3755; auc: 0.9992 - val_auc: 0.9296                                                                                                    \n",
      "epoch: 0078, loss: 0.0133 - val_loss: 0.3319; auc: 0.9994 - val_auc: 0.9282                                                                                                    \n",
      "epoch: 0079, loss: 0.0161 - val_loss: 0.3206; auc: 0.9993 - val_auc: 0.9286                                                                                                    \n",
      "epoch: 0080, loss: 0.0156 - val_loss: 0.4503; auc: 0.9994 - val_auc: 0.9252                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00080: early stopping\n",
      "Train on 1628 samples, validate on 204 samples\n",
      "Epoch 1/30\n",
      "1628/1628 [==============================] - 2s 1ms/sample - loss: 0.3064 - val_loss: 0.3049\n",
      "Epoch 2/30\n",
      "1628/1628 [==============================] - 1s 621us/sample - loss: 0.2854 - val_loss: 0.2743\n",
      "Epoch 3/30\n",
      "1628/1628 [==============================] - 1s 571us/sample - loss: 0.2576 - val_loss: 0.2385\n",
      "Epoch 4/30\n",
      "1628/1628 [==============================] - 1s 619us/sample - loss: 0.2317 - val_loss: 0.2135\n",
      "Epoch 5/30\n",
      "1628/1628 [==============================] - 1s 610us/sample - loss: 0.2166 - val_loss: 0.2046\n",
      "Epoch 6/30\n",
      "1628/1628 [==============================] - 1s 632us/sample - loss: 0.2039 - val_loss: 0.1901\n",
      "Epoch 7/30\n",
      "1628/1628 [==============================] - 1s 628us/sample - loss: 0.1950 - val_loss: 0.1955\n",
      "Epoch 8/30\n",
      "1628/1628 [==============================] - 1s 627us/sample - loss: 0.1838 - val_loss: 0.1769\n",
      "Epoch 9/30\n",
      "1628/1628 [==============================] - 1s 624us/sample - loss: 0.1813 - val_loss: 0.1712\n",
      "Epoch 10/30\n",
      "1628/1628 [==============================] - 1s 633us/sample - loss: 0.1691 - val_loss: 0.1675\n",
      "Epoch 11/30\n",
      "1628/1628 [==============================] - 1s 632us/sample - loss: 0.1637 - val_loss: 0.1632\n",
      "Epoch 12/30\n",
      "1628/1628 [==============================] - 1s 638us/sample - loss: 0.1610 - val_loss: 0.1607\n",
      "Epoch 13/30\n",
      "1628/1628 [==============================] - 1s 593us/sample - loss: 0.1565 - val_loss: 0.1648\n",
      "Epoch 14/30\n",
      "1628/1628 [==============================] - 1s 604us/sample - loss: 0.1462 - val_loss: 0.1561\n",
      "Epoch 15/30\n",
      "1628/1628 [==============================] - 1s 639us/sample - loss: 0.1402 - val_loss: 0.1522\n",
      "Epoch 16/30\n",
      "1628/1628 [==============================] - 1s 627us/sample - loss: 0.1335 - val_loss: 0.1495\n",
      "Epoch 17/30\n",
      "1628/1628 [==============================] - 1s 601us/sample - loss: 0.1299 - val_loss: 0.1591\n",
      "Epoch 18/30\n",
      "1628/1628 [==============================] - 1s 629us/sample - loss: 0.1251 - val_loss: 0.1483\n",
      "Epoch 19/30\n",
      "1628/1628 [==============================] - 1s 613us/sample - loss: 0.1172 - val_loss: 0.1449\n",
      "Epoch 20/30\n",
      "1628/1628 [==============================] - 1s 597us/sample - loss: 0.1147 - val_loss: 0.1464\n",
      "Epoch 21/30\n",
      "1628/1628 [==============================] - 1s 603us/sample - loss: 0.1132 - val_loss: 0.1460\n",
      "Epoch 22/30\n",
      "1628/1628 [==============================] - 1s 604us/sample - loss: 0.1035 - val_loss: 0.1645\n",
      "Epoch 23/30\n",
      "1628/1628 [==============================] - 1s 614us/sample - loss: 0.1000 - val_loss: 0.1444\n",
      "Epoch 24/30\n",
      "1628/1628 [==============================] - 1s 601us/sample - loss: 0.0943 - val_loss: 0.1454\n",
      "Epoch 25/30\n",
      "1628/1628 [==============================] - 1s 610us/sample - loss: 0.0927 - val_loss: 0.1466\n",
      "Epoch 26/30\n",
      "1628/1628 [==============================] - 1s 594us/sample - loss: 0.0950 - val_loss: 0.1460\n",
      "Epoch 27/30\n",
      "1628/1628 [==============================] - 1s 611us/sample - loss: 0.0888 - val_loss: 0.1430\n",
      "Epoch 28/30\n",
      "1628/1628 [==============================] - 1s 620us/sample - loss: 0.0767 - val_loss: 0.1594\n",
      "Epoch 29/30\n",
      "1628/1628 [==============================] - 1s 630us/sample - loss: 0.0701 - val_loss: 0.1660\n",
      "Epoch 30/30\n",
      "1628/1628 [==============================] - 1s 603us/sample - loss: 0.0681 - val_loss: 0.1982\n",
      "Train on 1628 samples, validate on 204 samples\n",
      "Epoch 1/30\n",
      "1628/1628 [==============================] - 3s 2ms/sample - loss: 0.3064 - val_loss: 0.3041\n",
      "Epoch 2/30\n",
      "1628/1628 [==============================] - 1s 599us/sample - loss: 0.2851 - val_loss: 0.2735\n",
      "Epoch 3/30\n",
      "1628/1628 [==============================] - 1s 620us/sample - loss: 0.2573 - val_loss: 0.2381\n",
      "Epoch 4/30\n",
      "1628/1628 [==============================] - 1s 620us/sample - loss: 0.2313 - val_loss: 0.2132\n",
      "Epoch 5/30\n",
      "1628/1628 [==============================] - 1s 618us/sample - loss: 0.2165 - val_loss: 0.2054\n",
      "Epoch 6/30\n",
      "1628/1628 [==============================] - 1s 653us/sample - loss: 0.2041 - val_loss: 0.1910\n",
      "Epoch 7/30\n",
      "1628/1628 [==============================] - 1s 630us/sample - loss: 0.1947 - val_loss: 0.1955\n",
      "Epoch 8/30\n",
      "1628/1628 [==============================] - 1s 647us/sample - loss: 0.1834 - val_loss: 0.1772\n",
      "Epoch 9/30\n",
      "1628/1628 [==============================] - 1s 671us/sample - loss: 0.1809 - val_loss: 0.1716\n",
      "Epoch 10/30\n",
      "1628/1628 [==============================] - 1s 635us/sample - loss: 0.1684 - val_loss: 0.1674\n",
      "Epoch 11/30\n",
      "1628/1628 [==============================] - 1s 657us/sample - loss: 0.1629 - val_loss: 0.1633\n",
      "Epoch 12/30\n",
      "1628/1628 [==============================] - 1s 667us/sample - loss: 0.1601 - val_loss: 0.1608\n",
      "Epoch 13/30\n",
      "1628/1628 [==============================] - 1s 658us/sample - loss: 0.1561 - val_loss: 0.1650\n",
      "Epoch 14/30\n",
      "1628/1628 [==============================] - 1s 654us/sample - loss: 0.1453 - val_loss: 0.1554\n",
      "Epoch 15/30\n",
      "1628/1628 [==============================] - 1s 656us/sample - loss: 0.1394 - val_loss: 0.1515\n",
      "Epoch 16/30\n",
      "1628/1628 [==============================] - 1s 662us/sample - loss: 0.1327 - val_loss: 0.1486\n",
      "Epoch 17/30\n",
      "1628/1628 [==============================] - 1s 669us/sample - loss: 0.1293 - val_loss: 0.1585\n",
      "Epoch 18/30\n",
      "1628/1628 [==============================] - 1s 620us/sample - loss: 0.1245 - val_loss: 0.1476\n",
      "Epoch 19/30\n",
      "1628/1628 [==============================] - 1s 667us/sample - loss: 0.1166 - val_loss: 0.1437\n",
      "Epoch 20/30\n",
      "1628/1628 [==============================] - 1s 702us/sample - loss: 0.1143 - val_loss: 0.1451\n",
      "Epoch 21/30\n",
      "1628/1628 [==============================] - 1s 660us/sample - loss: 0.1127 - val_loss: 0.1447\n",
      "Epoch 22/30\n",
      "1628/1628 [==============================] - 1s 658us/sample - loss: 0.1029 - val_loss: 0.1622\n",
      "Epoch 23/30\n",
      "1628/1628 [==============================] - 1s 629us/sample - loss: 0.0994 - val_loss: 0.1425\n",
      "Epoch 24/30\n",
      "1628/1628 [==============================] - 1s 645us/sample - loss: 0.0943 - val_loss: 0.1432\n",
      "Epoch 25/30\n",
      "1628/1628 [==============================] - 1s 646us/sample - loss: 0.0923 - val_loss: 0.1454\n",
      "Epoch 26/30\n",
      "1628/1628 [==============================] - 1s 636us/sample - loss: 0.0936 - val_loss: 0.1440\n",
      "Epoch 27/30\n",
      "1628/1628 [==============================] - 1s 643us/sample - loss: 0.0878 - val_loss: 0.1415\n",
      "Epoch 28/30\n",
      "1628/1628 [==============================] - 1s 624us/sample - loss: 0.0760 - val_loss: 0.1563\n",
      "Epoch 29/30\n",
      "1628/1628 [==============================] - 1s 647us/sample - loss: 0.0695 - val_loss: 0.1659\n",
      "Epoch 30/30\n",
      "1628/1628 [==============================] - 1s 666us/sample - loss: 0.0678 - val_loss: 0.2006\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, seed in enumerate([7, 77, 77]):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "    performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607148480554244"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005156067230935283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
