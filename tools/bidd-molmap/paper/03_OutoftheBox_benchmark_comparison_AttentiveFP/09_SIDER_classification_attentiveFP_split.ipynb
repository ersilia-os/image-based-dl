{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [22:59:47] Enabling RDKit 2019.09.2 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attentiveFP_idx(df):\n",
    "    \"\"\" attentiveFP dataset\"\"\"\n",
    "    train, valid,test = load('./split_and_data/09_SIDER_attentiveFP.data')\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train), len(valid), len(test)))\n",
    "    train_idx = df[df.smiles.isin(train.smiles)].index\n",
    "    valid_idx = df[df.smiles.isin(valid.smiles)].index\n",
    "    test_idx = df[df.smiles.isin(test.smiles)].index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: SIDER number of split times: 3\n",
      "training set: 1092, valid set: 137, test set 137\n",
      "training set: 1092, valid set: 137, test set 137\n"
     ]
    }
   ],
   "source": [
    "task_name = 'SIDER'\n",
    "\n",
    "from chembench import load_data\n",
    "df, _ = load_data(task_name)\n",
    "\n",
    "train_idx, valid_idx, test_idx = get_attentiveFP_idx(df) \n",
    "len(train_idx), len(valid_idx), len(test_idx)\n",
    "\n",
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_dir = '../02_OutofTheBox_benchmark_comparison_DMPNN/tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)\n",
    "\n",
    "\n",
    "\n",
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256] #27 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_auc'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092 137 137\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "trainX = (X1[train_idx], X2[train_idx])\n",
    "trainY = Y[train_idx]\n",
    "\n",
    "validX = (X1[valid_idx], X2[valid_idx])\n",
    "validY = Y[valid_idx]\n",
    "\n",
    "testX = (X1[test_idx], X2[test_idx])\n",
    "testY = Y[test_idx]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, loss: 0.6001 - val_loss: 0.6185; auc: 0.5830 - val_auc: 0.5353                                                                                                    \n",
      "epoch: 0002, loss: 0.5954 - val_loss: 0.6251; auc: 0.6194 - val_auc: 0.5396                                                                                                    \n",
      "epoch: 0003, loss: 0.5936 - val_loss: 0.6212; auc: 0.6391 - val_auc: 0.5528                                                                                                    \n",
      "epoch: 0004, loss: 0.5919 - val_loss: 0.6171; auc: 0.6519 - val_auc: 0.5612                                                                                                    \n",
      "epoch: 0005, loss: 0.5905 - val_loss: 0.6185; auc: 0.6635 - val_auc: 0.5661                                                                                                    \n",
      "epoch: 0006, loss: 0.5888 - val_loss: 0.6177; auc: 0.6696 - val_auc: 0.5726                                                                                                    \n",
      "epoch: 0007, loss: 0.5870 - val_loss: 0.6189; auc: 0.6753 - val_auc: 0.5740                                                                                                    \n",
      "epoch: 0008, loss: 0.5855 - val_loss: 0.6219; auc: 0.6781 - val_auc: 0.5726                                                                                                    \n",
      "epoch: 0009, loss: 0.5849 - val_loss: 0.6225; auc: 0.6790 - val_auc: 0.5734                                                                                                    \n",
      "epoch: 0010, loss: 0.5825 - val_loss: 0.6130; auc: 0.6796 - val_auc: 0.5753                                                                                                    \n",
      "epoch: 0011, loss: 0.5807 - val_loss: 0.6224; auc: 0.6798 - val_auc: 0.5760                                                                                                    \n",
      "epoch: 0012, loss: 0.5794 - val_loss: 0.6176; auc: 0.6796 - val_auc: 0.5769                                                                                                    \n",
      "epoch: 0013, loss: 0.5775 - val_loss: 0.6174; auc: 0.6799 - val_auc: 0.5768                                                                                                    \n",
      "epoch: 0014, loss: 0.5748 - val_loss: 0.6200; auc: 0.6794 - val_auc: 0.5759                                                                                                    \n",
      "epoch: 0015, loss: 0.5728 - val_loss: 0.6182; auc: 0.6792 - val_auc: 0.5769                                                                                                    \n",
      "epoch: 0016, loss: 0.5703 - val_loss: 0.6179; auc: 0.6807 - val_auc: 0.5789                                                                                                    \n",
      "epoch: 0017, loss: 0.5685 - val_loss: 0.6196; auc: 0.6805 - val_auc: 0.5794                                                                                                    \n",
      "epoch: 0018, loss: 0.5652 - val_loss: 0.6284; auc: 0.6801 - val_auc: 0.5799                                                                                                    \n",
      "epoch: 0019, loss: 0.5650 - val_loss: 0.6202; auc: 0.6813 - val_auc: 0.5810                                                                                                    \n",
      "epoch: 0020, loss: 0.5627 - val_loss: 0.6295; auc: 0.6830 - val_auc: 0.5817                                                                                                    \n",
      "epoch: 0021, loss: 0.5592 - val_loss: 0.6212; auc: 0.6833 - val_auc: 0.5833                                                                                                    \n",
      "epoch: 0022, loss: 0.5568 - val_loss: 0.6204; auc: 0.6855 - val_auc: 0.5826                                                                                                    \n",
      "epoch: 0023, loss: 0.5539 - val_loss: 0.6262; auc: 0.6841 - val_auc: 0.5832                                                                                                    \n",
      "epoch: 0024, loss: 0.5520 - val_loss: 0.6587; auc: 0.6872 - val_auc: 0.5848                                                                                                    \n",
      "epoch: 0025, loss: 0.5565 - val_loss: 0.6088; auc: 0.6852 - val_auc: 0.5848                                                                                                    \n",
      "epoch: 0026, loss: 0.5520 - val_loss: 0.6504; auc: 0.6922 - val_auc: 0.5866                                                                                                    \n",
      "epoch: 0027, loss: 0.5454 - val_loss: 0.6264; auc: 0.6921 - val_auc: 0.5855                                                                                                    \n",
      "epoch: 0028, loss: 0.5454 - val_loss: 0.6314; auc: 0.6921 - val_auc: 0.5857                                                                                                    \n",
      "epoch: 0029, loss: 0.5411 - val_loss: 0.6524; auc: 0.6955 - val_auc: 0.5871                                                                                                    \n",
      "epoch: 0030, loss: 0.5391 - val_loss: 0.6462; auc: 0.6955 - val_auc: 0.5870                                                                                                    \n",
      "epoch: 0031, loss: 0.5383 - val_loss: 0.6543; auc: 0.6984 - val_auc: 0.5886                                                                                                    \n",
      "epoch: 0032, loss: 0.5378 - val_loss: 0.6873; auc: 0.7022 - val_auc: 0.5921                                                                                                    \n",
      "epoch: 0033, loss: 0.5431 - val_loss: 0.6281; auc: 0.6984 - val_auc: 0.5880                                                                                                    \n",
      "epoch: 0034, loss: 0.5361 - val_loss: 0.6704; auc: 0.7075 - val_auc: 0.5922                                                                                                    \n",
      "epoch: 0035, loss: 0.5326 - val_loss: 0.6551; auc: 0.7054 - val_auc: 0.5927                                                                                                    \n",
      "epoch: 0036, loss: 0.5289 - val_loss: 0.6571; auc: 0.7081 - val_auc: 0.5928                                                                                                    \n",
      "epoch: 0037, loss: 0.5249 - val_loss: 0.6622; auc: 0.7100 - val_auc: 0.5940                                                                                                    \n",
      "epoch: 0038, loss: 0.5229 - val_loss: 0.6694; auc: 0.7121 - val_auc: 0.5948                                                                                                    \n",
      "epoch: 0039, loss: 0.5209 - val_loss: 0.6609; auc: 0.7131 - val_auc: 0.5950                                                                                                    \n",
      "epoch: 0040, loss: 0.5212 - val_loss: 0.7021; auc: 0.7177 - val_auc: 0.5968                                                                                                    \n",
      "epoch: 0041, loss: 0.5216 - val_loss: 0.7088; auc: 0.7194 - val_auc: 0.5966                                                                                                    \n",
      "epoch: 0042, loss: 0.5153 - val_loss: 0.6673; auc: 0.7179 - val_auc: 0.5953                                                                                                    \n",
      "epoch: 0043, loss: 0.5171 - val_loss: 0.6501; auc: 0.7212 - val_auc: 0.5977                                                                                                    \n",
      "epoch: 0044, loss: 0.5157 - val_loss: 0.7271; auc: 0.7234 - val_auc: 0.5986                                                                                                    \n",
      "epoch: 0045, loss: 0.5101 - val_loss: 0.6721; auc: 0.7249 - val_auc: 0.5993                                                                                                    \n",
      "epoch: 0046, loss: 0.5100 - val_loss: 0.7250; auc: 0.7272 - val_auc: 0.5994                                                                                                    \n",
      "epoch: 0047, loss: 0.5073 - val_loss: 0.7020; auc: 0.7307 - val_auc: 0.6001                                                                                                    \n",
      "epoch: 0048, loss: 0.5062 - val_loss: 0.7278; auc: 0.7325 - val_auc: 0.6014                                                                                                    \n",
      "epoch: 0049, loss: 0.5041 - val_loss: 0.7261; auc: 0.7314 - val_auc: 0.6012                                                                                                    \n",
      "epoch: 0050, loss: 0.5029 - val_loss: 0.6939; auc: 0.7347 - val_auc: 0.6019                                                                                                    \n",
      "epoch: 0051, loss: 0.5014 - val_loss: 0.7118; auc: 0.7359 - val_auc: 0.6029                                                                                                    \n",
      "epoch: 0052, loss: 0.4991 - val_loss: 0.7509; auc: 0.7425 - val_auc: 0.6046                                                                                                    \n",
      "epoch: 0053, loss: 0.4965 - val_loss: 0.7355; auc: 0.7447 - val_auc: 0.6056                                                                                                    \n",
      "epoch: 0054, loss: 0.4946 - val_loss: 0.7663; auc: 0.7458 - val_auc: 0.6056                                                                                                    \n",
      "epoch: 0055, loss: 0.4911 - val_loss: 0.7440; auc: 0.7465 - val_auc: 0.6055                                                                                                    \n",
      "epoch: 0056, loss: 0.4903 - val_loss: 0.7617; auc: 0.7492 - val_auc: 0.6063                                                                                                    \n",
      "epoch: 0057, loss: 0.4914 - val_loss: 0.7452; auc: 0.7497 - val_auc: 0.6075                                                                                                    \n",
      "epoch: 0058, loss: 0.4907 - val_loss: 0.7254; auc: 0.7502 - val_auc: 0.6060                                                                                                    \n",
      "epoch: 0059, loss: 0.4850 - val_loss: 0.7800; auc: 0.7559 - val_auc: 0.6082                                                                                                    \n",
      "epoch: 0060, loss: 0.4819 - val_loss: 0.8053; auc: 0.7592 - val_auc: 0.6097                                                                                                    \n",
      "epoch: 0061, loss: 0.4835 - val_loss: 0.7509; auc: 0.7581 - val_auc: 0.6096                                                                                                    \n",
      "epoch: 0062, loss: 0.4793 - val_loss: 0.7710; auc: 0.7608 - val_auc: 0.6099                                                                                                    \n",
      "epoch: 0063, loss: 0.4754 - val_loss: 0.8048; auc: 0.7638 - val_auc: 0.6108                                                                                                    \n",
      "epoch: 0064, loss: 0.4734 - val_loss: 0.7987; auc: 0.7656 - val_auc: 0.6102                                                                                                    \n",
      "epoch: 0065, loss: 0.4732 - val_loss: 0.8334; auc: 0.7685 - val_auc: 0.6115                                                                                                    \n",
      "epoch: 0066, loss: 0.4714 - val_loss: 0.7932; auc: 0.7701 - val_auc: 0.6114                                                                                                    \n",
      "epoch: 0067, loss: 0.4710 - val_loss: 0.8769; auc: 0.7744 - val_auc: 0.6119                                                                                                    \n",
      "epoch: 0068, loss: 0.4710 - val_loss: 0.8438; auc: 0.7776 - val_auc: 0.6138                                                                                                    \n",
      "epoch: 0069, loss: 0.4692 - val_loss: 0.8761; auc: 0.7798 - val_auc: 0.6135                                                                                                    \n",
      "epoch: 0070, loss: 0.4657 - val_loss: 0.8210; auc: 0.7784 - val_auc: 0.6139                                                                                                    \n",
      "epoch: 0071, loss: 0.4617 - val_loss: 0.8020; auc: 0.7791 - val_auc: 0.6143                                                                                                    \n",
      "epoch: 0072, loss: 0.4645 - val_loss: 0.7985; auc: 0.7818 - val_auc: 0.6148                                                                                                    \n",
      "epoch: 0073, loss: 0.4608 - val_loss: 0.8434; auc: 0.7861 - val_auc: 0.6151                                                                                                    \n",
      "epoch: 0074, loss: 0.4558 - val_loss: 0.8581; auc: 0.7861 - val_auc: 0.6148                                                                                                    \n",
      "epoch: 0075, loss: 0.4540 - val_loss: 0.8261; auc: 0.7872 - val_auc: 0.6163                                                                                                    \n",
      "epoch: 0076, loss: 0.4527 - val_loss: 0.8726; auc: 0.7930 - val_auc: 0.6162                                                                                                    \n",
      "epoch: 0077, loss: 0.4518 - val_loss: 0.8881; auc: 0.7937 - val_auc: 0.6184                                                                                                    \n",
      "epoch: 0078, loss: 0.4502 - val_loss: 0.8545; auc: 0.7948 - val_auc: 0.6182                                                                                                    \n",
      "epoch: 0079, loss: 0.4528 - val_loss: 0.8605; auc: 0.7978 - val_auc: 0.6183                                                                                                    \n",
      "epoch: 0080, loss: 0.4455 - val_loss: 0.8482; auc: 0.7970 - val_auc: 0.6195                                                                                                    \n",
      "epoch: 0081, loss: 0.4430 - val_loss: 0.8522; auc: 0.7995 - val_auc: 0.6207                                                                                                    \n",
      "epoch: 0082, loss: 0.4409 - val_loss: 0.9021; auc: 0.8035 - val_auc: 0.6200                                                                                                    \n",
      "epoch: 0083, loss: 0.4391 - val_loss: 0.8723; auc: 0.8036 - val_auc: 0.6193                                                                                                    \n",
      "epoch: 0084, loss: 0.4360 - val_loss: 0.8643; auc: 0.8049 - val_auc: 0.6215                                                                                                    \n",
      "epoch: 0085, loss: 0.4354 - val_loss: 0.8633; auc: 0.8053 - val_auc: 0.6227                                                                                                    \n",
      "epoch: 0086, loss: 0.4340 - val_loss: 0.9110; auc: 0.8101 - val_auc: 0.6231                                                                                                    \n",
      "epoch: 0087, loss: 0.4335 - val_loss: 0.9367; auc: 0.8138 - val_auc: 0.6230                                                                                                    \n",
      "epoch: 0088, loss: 0.4301 - val_loss: 0.9148; auc: 0.8153 - val_auc: 0.6236                                                                                                    \n",
      "epoch: 0089, loss: 0.4282 - val_loss: 0.8892; auc: 0.8144 - val_auc: 0.6244                                                                                                    \n",
      "epoch: 0090, loss: 0.4268 - val_loss: 0.9013; auc: 0.8174 - val_auc: 0.6242                                                                                                    \n",
      "epoch: 0091, loss: 0.4257 - val_loss: 0.9104; auc: 0.8205 - val_auc: 0.6264                                                                                                    \n",
      "epoch: 0092, loss: 0.4231 - val_loss: 0.9007; auc: 0.8212 - val_auc: 0.6260                                                                                                    \n",
      "epoch: 0093, loss: 0.4177 - val_loss: 0.9377; auc: 0.8240 - val_auc: 0.6261                                                                                                    \n",
      "epoch: 0094, loss: 0.4177 - val_loss: 0.9247; auc: 0.8251 - val_auc: 0.6278                                                                                                    \n",
      "epoch: 0095, loss: 0.4197 - val_loss: 0.9085; auc: 0.8239 - val_auc: 0.6283                                                                                                    \n",
      "epoch: 0096, loss: 0.4173 - val_loss: 0.9251; auc: 0.8298 - val_auc: 0.6289                                                                                                    \n",
      "epoch: 0097, loss: 0.4124 - val_loss: 0.9137; auc: 0.8286 - val_auc: 0.6310                                                                                                    \n",
      "epoch: 0098, loss: 0.4094 - val_loss: 0.8947; auc: 0.8295 - val_auc: 0.6300                                                                                                    \n",
      "epoch: 0099, loss: 0.4096 - val_loss: 0.9582; auc: 0.8339 - val_auc: 0.6302                                                                                                    \n",
      "epoch: 0100, loss: 0.4070 - val_loss: 0.9591; auc: 0.8358 - val_auc: 0.6314                                                                                                    \n",
      "epoch: 0101, loss: 0.4042 - val_loss: 0.9438; auc: 0.8361 - val_auc: 0.6327                                                                                                    \n",
      "epoch: 0102, loss: 0.4032 - val_loss: 0.9183; auc: 0.8370 - val_auc: 0.6323                                                                                                    \n",
      "epoch: 0103, loss: 0.3992 - val_loss: 0.9519; auc: 0.8385 - val_auc: 0.6334                                                                                                    \n",
      "epoch: 0104, loss: 0.3974 - val_loss: 0.9626; auc: 0.8421 - val_auc: 0.6343                                                                                                    \n",
      "epoch: 0105, loss: 0.3951 - val_loss: 0.9424; auc: 0.8426 - val_auc: 0.6349                                                                                                    \n",
      "epoch: 0106, loss: 0.3929 - val_loss: 0.9807; auc: 0.8446 - val_auc: 0.6340                                                                                                    \n",
      "epoch: 0107, loss: 0.3936 - val_loss: 0.9163; auc: 0.8423 - val_auc: 0.6346                                                                                                    \n",
      "epoch: 0108, loss: 0.3928 - val_loss: 0.9944; auc: 0.8478 - val_auc: 0.6356                                                                                                    \n",
      "epoch: 0109, loss: 0.3882 - val_loss: 0.9304; auc: 0.8477 - val_auc: 0.6372                                                                                                    \n",
      "epoch: 0110, loss: 0.3901 - val_loss: 1.0008; auc: 0.8517 - val_auc: 0.6370                                                                                                    \n",
      "epoch: 0111, loss: 0.3868 - val_loss: 0.8963; auc: 0.8471 - val_auc: 0.6378                                                                                                    \n",
      "epoch: 0112, loss: 0.3876 - val_loss: 1.0410; auc: 0.8554 - val_auc: 0.6360                                                                                                    \n",
      "epoch: 0113, loss: 0.3840 - val_loss: 0.9697; auc: 0.8547 - val_auc: 0.6355                                                                                                    \n",
      "epoch: 0114, loss: 0.3801 - val_loss: 0.9689; auc: 0.8553 - val_auc: 0.6398                                                                                                    \n",
      "epoch: 0115, loss: 0.3776 - val_loss: 0.9726; auc: 0.8563 - val_auc: 0.6401                                                                                                    \n",
      "epoch: 0116, loss: 0.3776 - val_loss: 1.0037; auc: 0.8592 - val_auc: 0.6390                                                                                                    \n",
      "epoch: 0117, loss: 0.3773 - val_loss: 0.9623; auc: 0.8584 - val_auc: 0.6409                                                                                                    \n",
      "epoch: 0118, loss: 0.3768 - val_loss: 1.0345; auc: 0.8632 - val_auc: 0.6405                                                                                                    \n",
      "epoch: 0119, loss: 0.3750 - val_loss: 0.9684; auc: 0.8597 - val_auc: 0.6415                                                                                                    \n",
      "epoch: 0120, loss: 0.3712 - val_loss: 1.0179; auc: 0.8641 - val_auc: 0.6421                                                                                                    \n",
      "epoch: 0121, loss: 0.3666 - val_loss: 0.9927; auc: 0.8650 - val_auc: 0.6406                                                                                                    \n",
      "epoch: 0122, loss: 0.3672 - val_loss: 1.0137; auc: 0.8659 - val_auc: 0.6438                                                                                                    \n",
      "epoch: 0123, loss: 0.3668 - val_loss: 1.0251; auc: 0.8656 - val_auc: 0.6423                                                                                                    \n",
      "epoch: 0124, loss: 0.3615 - val_loss: 1.0191; auc: 0.8684 - val_auc: 0.6415                                                                                                    \n",
      "epoch: 0125, loss: 0.3603 - val_loss: 0.9909; auc: 0.8677 - val_auc: 0.6442                                                                                                    \n",
      "epoch: 0126, loss: 0.3578 - val_loss: 0.9825; auc: 0.8684 - val_auc: 0.6454                                                                                                    \n",
      "epoch: 0127, loss: 0.3592 - val_loss: 0.9954; auc: 0.8679 - val_auc: 0.6435                                                                                                    \n",
      "epoch: 0128, loss: 0.3608 - val_loss: 1.0146; auc: 0.8724 - val_auc: 0.6445                                                                                                    \n",
      "epoch: 0129, loss: 0.3546 - val_loss: 0.9817; auc: 0.8704 - val_auc: 0.6465                                                                                                    \n",
      "epoch: 0130, loss: 0.3565 - val_loss: 1.0402; auc: 0.8738 - val_auc: 0.6441                                                                                                    \n",
      "epoch: 0131, loss: 0.3524 - val_loss: 1.0561; auc: 0.8749 - val_auc: 0.6463                                                                                                    \n",
      "epoch: 0132, loss: 0.3510 - val_loss: 1.0358; auc: 0.8764 - val_auc: 0.6449                                                                                                    \n",
      "epoch: 0133, loss: 0.3487 - val_loss: 1.0096; auc: 0.8761 - val_auc: 0.6468                                                                                                    \n",
      "epoch: 0134, loss: 0.3480 - val_loss: 0.9716; auc: 0.8753 - val_auc: 0.6476                                                                                                    \n",
      "epoch: 0135, loss: 0.3494 - val_loss: 1.0022; auc: 0.8759 - val_auc: 0.6476                                                                                                    \n",
      "epoch: 0136, loss: 0.3447 - val_loss: 1.0845; auc: 0.8802 - val_auc: 0.6470                                                                                                    \n",
      "epoch: 0137, loss: 0.3428 - val_loss: 1.0166; auc: 0.8796 - val_auc: 0.6492                                                                                                    \n",
      "epoch: 0138, loss: 0.3411 - val_loss: 1.0782; auc: 0.8815 - val_auc: 0.6471                                                                                                    \n",
      "epoch: 0139, loss: 0.3408 - val_loss: 1.0256; auc: 0.8815 - val_auc: 0.6484                                                                                                    \n",
      "epoch: 0140, loss: 0.3377 - val_loss: 1.0352; auc: 0.8812 - val_auc: 0.6479                                                                                                    \n",
      "epoch: 0141, loss: 0.3375 - val_loss: 1.0529; auc: 0.8843 - val_auc: 0.6491                                                                                                    \n",
      "epoch: 0142, loss: 0.3350 - val_loss: 1.0440; auc: 0.8839 - val_auc: 0.6490                                                                                                    \n",
      "epoch: 0143, loss: 0.3345 - val_loss: 1.0260; auc: 0.8827 - val_auc: 0.6486                                                                                                    \n",
      "epoch: 0144, loss: 0.3387 - val_loss: 1.0708; auc: 0.8857 - val_auc: 0.6490                                                                                                    \n",
      "epoch: 0145, loss: 0.3354 - val_loss: 1.0781; auc: 0.8875 - val_auc: 0.6511                                                                                                    \n",
      "epoch: 0146, loss: 0.3302 - val_loss: 1.0451; auc: 0.8864 - val_auc: 0.6497                                                                                                    \n",
      "epoch: 0147, loss: 0.3287 - val_loss: 1.0561; auc: 0.8879 - val_auc: 0.6506                                                                                                    \n",
      "epoch: 0148, loss: 0.3291 - val_loss: 1.0606; auc: 0.8887 - val_auc: 0.6498                                                                                                    \n",
      "epoch: 0149, loss: 0.3266 - val_loss: 1.0737; auc: 0.8891 - val_auc: 0.6505                                                                                                    \n",
      "epoch: 0150, loss: 0.3253 - val_loss: 1.0534; auc: 0.8903 - val_auc: 0.6501                                                                                                    \n",
      "epoch: 0151, loss: 0.3259 - val_loss: 1.0871; auc: 0.8904 - val_auc: 0.6492                                                                                                    \n",
      "epoch: 0152, loss: 0.3250 - val_loss: 1.0414; auc: 0.8913 - val_auc: 0.6490                                                                                                    \n",
      "epoch: 0153, loss: 0.3210 - val_loss: 1.0988; auc: 0.8917 - val_auc: 0.6510                                                                                                    \n",
      "epoch: 0154, loss: 0.3193 - val_loss: 1.0811; auc: 0.8923 - val_auc: 0.6507                                                                                                    \n",
      "epoch: 0155, loss: 0.3183 - val_loss: 1.1171; auc: 0.8941 - val_auc: 0.6511                                                                                                    \n",
      "epoch: 0156, loss: 0.3183 - val_loss: 1.0682; auc: 0.8936 - val_auc: 0.6517                                                                                                    \n",
      "epoch: 0157, loss: 0.3158 - val_loss: 1.1385; auc: 0.8960 - val_auc: 0.6492                                                                                                    \n",
      "epoch: 0158, loss: 0.3171 - val_loss: 1.1412; auc: 0.8958 - val_auc: 0.6511                                                                                                    \n",
      "epoch: 0159, loss: 0.3157 - val_loss: 1.1384; auc: 0.8971 - val_auc: 0.6490                                                                                                    \n",
      "epoch: 0160, loss: 0.3159 - val_loss: 1.0714; auc: 0.8959 - val_auc: 0.6522                                                                                                    \n",
      "epoch: 0161, loss: 0.3148 - val_loss: 1.1210; auc: 0.8960 - val_auc: 0.6506                                                                                                    \n",
      "epoch: 0162, loss: 0.3124 - val_loss: 1.1105; auc: 0.8985 - val_auc: 0.6507                                                                                                    \n",
      "epoch: 0163, loss: 0.3124 - val_loss: 1.1420; auc: 0.8990 - val_auc: 0.6509                                                                                                    \n",
      "epoch: 0164, loss: 0.3090 - val_loss: 1.0835; auc: 0.8975 - val_auc: 0.6504                                                                                                    \n",
      "epoch: 0165, loss: 0.3091 - val_loss: 1.0865; auc: 0.8992 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0166, loss: 0.3092 - val_loss: 1.1510; auc: 0.8999 - val_auc: 0.6528                                                                                                    \n",
      "epoch: 0167, loss: 0.3061 - val_loss: 1.1273; auc: 0.9013 - val_auc: 0.6511                                                                                                    \n",
      "epoch: 0168, loss: 0.3043 - val_loss: 1.1324; auc: 0.9010 - val_auc: 0.6509                                                                                                    \n",
      "epoch: 0169, loss: 0.3028 - val_loss: 1.1369; auc: 0.9015 - val_auc: 0.6500                                                                                                    \n",
      "epoch: 0170, loss: 0.3037 - val_loss: 1.1579; auc: 0.9027 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0171, loss: 0.3033 - val_loss: 1.2046; auc: 0.9035 - val_auc: 0.6504                                                                                                    \n",
      "epoch: 0172, loss: 0.3027 - val_loss: 1.1853; auc: 0.9040 - val_auc: 0.6494                                                                                                    \n",
      "epoch: 0173, loss: 0.3007 - val_loss: 1.1759; auc: 0.9042 - val_auc: 0.6506                                                                                                    \n",
      "epoch: 0174, loss: 0.2989 - val_loss: 1.0963; auc: 0.9027 - val_auc: 0.6517                                                                                                    \n",
      "epoch: 0175, loss: 0.3024 - val_loss: 1.1325; auc: 0.9029 - val_auc: 0.6516                                                                                                    \n",
      "epoch: 0176, loss: 0.3018 - val_loss: 1.1335; auc: 0.9040 - val_auc: 0.6521                                                                                                    \n",
      "epoch: 0177, loss: 0.2980 - val_loss: 1.1445; auc: 0.9052 - val_auc: 0.6509                                                                                                    \n",
      "epoch: 0178, loss: 0.2958 - val_loss: 1.1202; auc: 0.9052 - val_auc: 0.6526                                                                                                    \n",
      "epoch: 0179, loss: 0.2944 - val_loss: 1.2179; auc: 0.9071 - val_auc: 0.6510                                                                                                    \n",
      "epoch: 0180, loss: 0.2951 - val_loss: 1.1411; auc: 0.9064 - val_auc: 0.6526                                                                                                    \n",
      "epoch: 0181, loss: 0.2927 - val_loss: 1.1563; auc: 0.9067 - val_auc: 0.6509                                                                                                    \n",
      "epoch: 0182, loss: 0.2913 - val_loss: 1.1673; auc: 0.9074 - val_auc: 0.6529                                                                                                    \n",
      "epoch: 0183, loss: 0.2898 - val_loss: 1.1624; auc: 0.9081 - val_auc: 0.6521                                                                                                    \n",
      "epoch: 0184, loss: 0.2888 - val_loss: 1.1734; auc: 0.9081 - val_auc: 0.6529                                                                                                    \n",
      "epoch: 0185, loss: 0.2896 - val_loss: 1.1699; auc: 0.9083 - val_auc: 0.6532                                                                                                    \n",
      "epoch: 0186, loss: 0.2886 - val_loss: 1.1699; auc: 0.9082 - val_auc: 0.6535                                                                                                    \n",
      "epoch: 0187, loss: 0.2884 - val_loss: 1.1344; auc: 0.9085 - val_auc: 0.6529                                                                                                    \n",
      "epoch: 0188, loss: 0.2883 - val_loss: 1.2138; auc: 0.9098 - val_auc: 0.6526                                                                                                    \n",
      "epoch: 0189, loss: 0.2933 - val_loss: 1.2604; auc: 0.9110 - val_auc: 0.6519                                                                                                    \n",
      "epoch: 0190, loss: 0.2929 - val_loss: 1.2321; auc: 0.9112 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0191, loss: 0.2859 - val_loss: 1.2041; auc: 0.9114 - val_auc: 0.6519                                                                                                    \n",
      "epoch: 0192, loss: 0.2844 - val_loss: 1.2218; auc: 0.9115 - val_auc: 0.6524                                                                                                    \n",
      "epoch: 0193, loss: 0.2844 - val_loss: 1.2078; auc: 0.9119 - val_auc: 0.6530                                                                                                    \n",
      "epoch: 0194, loss: 0.2832 - val_loss: 1.2044; auc: 0.9114 - val_auc: 0.6548                                                                                                    \n",
      "epoch: 0195, loss: 0.2827 - val_loss: 1.1345; auc: 0.9113 - val_auc: 0.6526                                                                                                    \n",
      "epoch: 0196, loss: 0.2809 - val_loss: 1.2155; auc: 0.9116 - val_auc: 0.6525                                                                                                    \n",
      "epoch: 0197, loss: 0.2833 - val_loss: 1.1500; auc: 0.9117 - val_auc: 0.6519                                                                                                    \n",
      "epoch: 0198, loss: 0.2817 - val_loss: 1.1979; auc: 0.9121 - val_auc: 0.6528                                                                                                    \n",
      "epoch: 0199, loss: 0.2793 - val_loss: 1.2106; auc: 0.9142 - val_auc: 0.6517                                                                                                    \n",
      "epoch: 0200, loss: 0.2762 - val_loss: 1.2246; auc: 0.9139 - val_auc: 0.6537                                                                                                    \n",
      "epoch: 0201, loss: 0.2752 - val_loss: 1.1953; auc: 0.9135 - val_auc: 0.6524                                                                                                    \n",
      "epoch: 0202, loss: 0.2764 - val_loss: 1.2243; auc: 0.9145 - val_auc: 0.6536                                                                                                    \n",
      "epoch: 0203, loss: 0.2757 - val_loss: 1.2188; auc: 0.9151 - val_auc: 0.6528                                                                                                    \n",
      "epoch: 0204, loss: 0.2746 - val_loss: 1.2612; auc: 0.9158 - val_auc: 0.6527                                                                                                    \n",
      "epoch: 0205, loss: 0.2767 - val_loss: 1.1952; auc: 0.9154 - val_auc: 0.6516                                                                                                    \n",
      "epoch: 0206, loss: 0.2732 - val_loss: 1.2588; auc: 0.9152 - val_auc: 0.6526                                                                                                    \n",
      "epoch: 0207, loss: 0.2733 - val_loss: 1.1960; auc: 0.9153 - val_auc: 0.6543                                                                                                    \n",
      "epoch: 0208, loss: 0.2717 - val_loss: 1.2165; auc: 0.9159 - val_auc: 0.6545                                                                                                    \n",
      "epoch: 0209, loss: 0.2752 - val_loss: 1.2860; auc: 0.9172 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0210, loss: 0.2718 - val_loss: 1.2691; auc: 0.9174 - val_auc: 0.6536                                                                                                    \n",
      "epoch: 0211, loss: 0.2703 - val_loss: 1.2337; auc: 0.9177 - val_auc: 0.6543                                                                                                    \n",
      "epoch: 0212, loss: 0.2678 - val_loss: 1.2701; auc: 0.9178 - val_auc: 0.6535                                                                                                    \n",
      "epoch: 0213, loss: 0.2679 - val_loss: 1.2459; auc: 0.9181 - val_auc: 0.6536                                                                                                    \n",
      "epoch: 0214, loss: 0.2689 - val_loss: 1.2236; auc: 0.9171 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0215, loss: 0.2703 - val_loss: 1.2536; auc: 0.9185 - val_auc: 0.6571                                                                                                    \n",
      "epoch: 0216, loss: 0.2665 - val_loss: 1.2519; auc: 0.9186 - val_auc: 0.6527                                                                                                    \n",
      "epoch: 0217, loss: 0.2654 - val_loss: 1.2908; auc: 0.9193 - val_auc: 0.6546                                                                                                    \n",
      "epoch: 0218, loss: 0.2651 - val_loss: 1.2745; auc: 0.9196 - val_auc: 0.6544                                                                                                    \n",
      "epoch: 0219, loss: 0.2641 - val_loss: 1.2472; auc: 0.9188 - val_auc: 0.6533                                                                                                    \n",
      "epoch: 0220, loss: 0.2642 - val_loss: 1.2760; auc: 0.9203 - val_auc: 0.6531                                                                                                    \n",
      "epoch: 0221, loss: 0.2626 - val_loss: 1.3182; auc: 0.9208 - val_auc: 0.6533                                                                                                    \n",
      "epoch: 0222, loss: 0.2633 - val_loss: 1.2977; auc: 0.9210 - val_auc: 0.6520                                                                                                    \n",
      "epoch: 0223, loss: 0.2614 - val_loss: 1.2991; auc: 0.9206 - val_auc: 0.6547                                                                                                    \n",
      "epoch: 0224, loss: 0.2616 - val_loss: 1.2397; auc: 0.9211 - val_auc: 0.6534                                                                                                    \n",
      "epoch: 0225, loss: 0.2649 - val_loss: 1.2603; auc: 0.9199 - val_auc: 0.6540                                                                                                    \n",
      "epoch: 0226, loss: 0.2648 - val_loss: 1.2498; auc: 0.9201 - val_auc: 0.6554                                                                                                    \n",
      "epoch: 0227, loss: 0.2690 - val_loss: 1.3085; auc: 0.9221 - val_auc: 0.6531                                                                                                    \n",
      "epoch: 0228, loss: 0.2633 - val_loss: 1.3575; auc: 0.9222 - val_auc: 0.6536                                                                                                    \n",
      "epoch: 0229, loss: 0.2613 - val_loss: 1.3501; auc: 0.9226 - val_auc: 0.6534                                                                                                    \n",
      "epoch: 0230, loss: 0.2596 - val_loss: 1.2866; auc: 0.9224 - val_auc: 0.6533                                                                                                    \n",
      "epoch: 0231, loss: 0.2562 - val_loss: 1.3222; auc: 0.9226 - val_auc: 0.6529                                                                                                    \n",
      "epoch: 0232, loss: 0.2557 - val_loss: 1.2972; auc: 0.9229 - val_auc: 0.6538                                                                                                    \n",
      "epoch: 0233, loss: 0.2545 - val_loss: 1.2960; auc: 0.9232 - val_auc: 0.6532                                                                                                    \n",
      "epoch: 0234, loss: 0.2547 - val_loss: 1.2716; auc: 0.9228 - val_auc: 0.6550                                                                                                    \n",
      "epoch: 0235, loss: 0.2547 - val_loss: 1.3029; auc: 0.9231 - val_auc: 0.6540                                                                                                    \n",
      "epoch: 0236, loss: 0.2525 - val_loss: 1.3567; auc: 0.9244 - val_auc: 0.6531                                                                                                    \n",
      "epoch: 0237, loss: 0.2530 - val_loss: 1.2894; auc: 0.9233 - val_auc: 0.6542                                                                                                    \n",
      "epoch: 0238, loss: 0.2531 - val_loss: 1.3486; auc: 0.9247 - val_auc: 0.6530                                                                                                    \n",
      "epoch: 0239, loss: 0.2521 - val_loss: 1.3079; auc: 0.9244 - val_auc: 0.6538                                                                                                    \n",
      "epoch: 0240, loss: 0.2522 - val_loss: 1.3321; auc: 0.9248 - val_auc: 0.6541                                                                                                    \n",
      "epoch: 0241, loss: 0.2508 - val_loss: 1.3860; auc: 0.9255 - val_auc: 0.6530                                                                                                    \n",
      "epoch: 0242, loss: 0.2518 - val_loss: 1.3143; auc: 0.9254 - val_auc: 0.6535                                                                                                    \n",
      "epoch: 0243, loss: 0.2500 - val_loss: 1.3142; auc: 0.9248 - val_auc: 0.6542                                                                                                    \n",
      "epoch: 0244, loss: 0.2500 - val_loss: 1.3236; auc: 0.9256 - val_auc: 0.6528                                                                                                    \n",
      "epoch: 0245, loss: 0.2475 - val_loss: 1.3665; auc: 0.9262 - val_auc: 0.6540                                                                                                    \n",
      "epoch: 0246, loss: 0.2469 - val_loss: 1.3256; auc: 0.9262 - val_auc: 0.6544                                                                                                    \n",
      "epoch: 0247, loss: 0.2471 - val_loss: 1.2998; auc: 0.9252 - val_auc: 0.6548                                                                                                    \n",
      "epoch: 0248, loss: 0.2531 - val_loss: 1.3440; auc: 0.9264 - val_auc: 0.6558                                                                                                    \n",
      "epoch: 0249, loss: 0.2494 - val_loss: 1.3563; auc: 0.9271 - val_auc: 0.6532                                                                                                    \n",
      "epoch: 0250, loss: 0.2452 - val_loss: 1.3358; auc: 0.9262 - val_auc: 0.6540                                                                                                    \n",
      "epoch: 0251, loss: 0.2474 - val_loss: 1.3552; auc: 0.9273 - val_auc: 0.6542                                                                                                    \n",
      "epoch: 0252, loss: 0.2454 - val_loss: 1.3821; auc: 0.9278 - val_auc: 0.6549                                                                                                    \n",
      "epoch: 0253, loss: 0.2436 - val_loss: 1.3633; auc: 0.9278 - val_auc: 0.6547                                                                                                    \n",
      "epoch: 0254, loss: 0.2431 - val_loss: 1.3634; auc: 0.9278 - val_auc: 0.6541                                                                                                    \n",
      "epoch: 0255, loss: 0.2427 - val_loss: 1.4114; auc: 0.9284 - val_auc: 0.6550                                                                                                    \n",
      "epoch: 0256, loss: 0.2424 - val_loss: 1.4107; auc: 0.9286 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0257, loss: 0.2434 - val_loss: 1.3533; auc: 0.9283 - val_auc: 0.6535                                                                                                    \n",
      "epoch: 0258, loss: 0.2415 - val_loss: 1.3771; auc: 0.9287 - val_auc: 0.6548                                                                                                    \n",
      "epoch: 0259, loss: 0.2412 - val_loss: 1.4035; auc: 0.9291 - val_auc: 0.6528                                                                                                    \n",
      "epoch: 0260, loss: 0.2412 - val_loss: 1.4107; auc: 0.9292 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0261, loss: 0.2392 - val_loss: 1.3878; auc: 0.9293 - val_auc: 0.6541                                                                                                    \n",
      "epoch: 0262, loss: 0.2381 - val_loss: 1.3760; auc: 0.9291 - val_auc: 0.6543                                                                                                    \n",
      "epoch: 0263, loss: 0.2388 - val_loss: 1.3875; auc: 0.9295 - val_auc: 0.6541                                                                                                    \n",
      "epoch: 0264, loss: 0.2373 - val_loss: 1.3894; auc: 0.9299 - val_auc: 0.6543                                                                                                    \n",
      "epoch: 0265, loss: 0.2368 - val_loss: 1.3967; auc: 0.9300 - val_auc: 0.6540                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00265: early stopping\n",
      "Train on 1092 samples, validate on 137 samples\n",
      "Epoch 1/215\n",
      "1092/1092 [==============================] - 5s 4ms/sample - loss: 0.5995 - val_loss: 0.6207\n",
      "Epoch 2/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5948 - val_loss: 0.6213\n",
      "Epoch 3/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5932 - val_loss: 0.6211\n",
      "Epoch 4/215\n",
      "1092/1092 [==============================] - 1s 987us/sample - loss: 0.5926 - val_loss: 0.6236\n",
      "Epoch 5/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.5900 - val_loss: 0.6164\n",
      "Epoch 6/215\n",
      "1092/1092 [==============================] - 1s 997us/sample - loss: 0.5888 - val_loss: 0.6177\n",
      "Epoch 7/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5878 - val_loss: 0.6242\n",
      "Epoch 8/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5873 - val_loss: 0.6147\n",
      "Epoch 9/215\n",
      "1092/1092 [==============================] - 1s 941us/sample - loss: 0.5840 - val_loss: 0.6237\n",
      "Epoch 10/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.5832 - val_loss: 0.6229\n",
      "Epoch 11/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5819 - val_loss: 0.6134\n",
      "Epoch 12/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5811 - val_loss: 0.6241\n",
      "Epoch 13/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5810 - val_loss: 0.6122\n",
      "Epoch 14/215\n",
      "1092/1092 [==============================] - 1s 979us/sample - loss: 0.5762 - val_loss: 0.6290\n",
      "Epoch 15/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5739 - val_loss: 0.6160\n",
      "Epoch 16/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5718 - val_loss: 0.6195\n",
      "Epoch 17/215\n",
      "1092/1092 [==============================] - 1s 984us/sample - loss: 0.5700 - val_loss: 0.6261\n",
      "Epoch 18/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5683 - val_loss: 0.6186\n",
      "Epoch 19/215\n",
      "1092/1092 [==============================] - 1s 946us/sample - loss: 0.5655 - val_loss: 0.6219\n",
      "Epoch 20/215\n",
      "1092/1092 [==============================] - 1s 994us/sample - loss: 0.5625 - val_loss: 0.6216\n",
      "Epoch 21/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5599 - val_loss: 0.6216\n",
      "Epoch 22/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5579 - val_loss: 0.6179\n",
      "Epoch 23/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5560 - val_loss: 0.6278\n",
      "Epoch 24/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5536 - val_loss: 0.6249\n",
      "Epoch 25/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5522 - val_loss: 0.6293\n",
      "Epoch 26/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5483 - val_loss: 0.6286\n",
      "Epoch 27/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5458 - val_loss: 0.6287\n",
      "Epoch 28/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5437 - val_loss: 0.6394\n",
      "Epoch 29/215\n",
      "1092/1092 [==============================] - 1s 947us/sample - loss: 0.5421 - val_loss: 0.6472\n",
      "Epoch 30/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5395 - val_loss: 0.6345\n",
      "Epoch 31/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5373 - val_loss: 0.6466\n",
      "Epoch 32/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5400 - val_loss: 0.6480\n",
      "Epoch 33/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5367 - val_loss: 0.6664\n",
      "Epoch 34/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5334 - val_loss: 0.6514\n",
      "Epoch 35/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5320 - val_loss: 0.6500\n",
      "Epoch 36/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5278 - val_loss: 0.6474\n",
      "Epoch 37/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.5248 - val_loss: 0.6540\n",
      "Epoch 38/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5230 - val_loss: 0.6775\n",
      "Epoch 39/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5200 - val_loss: 0.6926\n",
      "Epoch 40/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5190 - val_loss: 0.6683\n",
      "Epoch 41/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5153 - val_loss: 0.6898\n",
      "Epoch 42/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5135 - val_loss: 0.6733\n",
      "Epoch 43/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5115 - val_loss: 0.6953\n",
      "Epoch 44/215\n",
      "1092/1092 [==============================] - 1s 977us/sample - loss: 0.5090 - val_loss: 0.7264\n",
      "Epoch 45/215\n",
      "1092/1092 [==============================] - 1s 927us/sample - loss: 0.5099 - val_loss: 0.7245\n",
      "Epoch 46/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5074 - val_loss: 0.6978\n",
      "Epoch 47/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5037 - val_loss: 0.7042\n",
      "Epoch 48/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5007 - val_loss: 0.6954\n",
      "Epoch 49/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4985 - val_loss: 0.7460\n",
      "Epoch 50/215\n",
      "1092/1092 [==============================] - 1s 956us/sample - loss: 0.5006 - val_loss: 0.7453\n",
      "Epoch 51/215\n",
      "1092/1092 [==============================] - 1s 967us/sample - loss: 0.4961 - val_loss: 0.7200\n",
      "Epoch 52/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4931 - val_loss: 0.7192\n",
      "Epoch 53/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4923 - val_loss: 0.7076\n",
      "Epoch 54/215\n",
      "1092/1092 [==============================] - 1s 987us/sample - loss: 0.4920 - val_loss: 0.7599\n",
      "Epoch 55/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4905 - val_loss: 0.7168\n",
      "Epoch 56/215\n",
      "1092/1092 [==============================] - 1s 939us/sample - loss: 0.4877 - val_loss: 0.8060\n",
      "Epoch 57/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4846 - val_loss: 0.7740\n",
      "Epoch 58/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4840 - val_loss: 0.8079\n",
      "Epoch 59/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4829 - val_loss: 0.7867\n",
      "Epoch 60/215\n",
      "1092/1092 [==============================] - 1s 979us/sample - loss: 0.4824 - val_loss: 0.8111\n",
      "Epoch 61/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4774 - val_loss: 0.7544\n",
      "Epoch 62/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4790 - val_loss: 0.7594\n",
      "Epoch 63/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4740 - val_loss: 0.7310\n",
      "Epoch 64/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4751 - val_loss: 0.7788\n",
      "Epoch 65/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4737 - val_loss: 0.7407\n",
      "Epoch 66/215\n",
      "1092/1092 [==============================] - 1s 957us/sample - loss: 0.4715 - val_loss: 0.7517\n",
      "Epoch 67/215\n",
      "1092/1092 [==============================] - 1s 978us/sample - loss: 0.4663 - val_loss: 0.8085\n",
      "Epoch 68/215\n",
      "1092/1092 [==============================] - 1s 984us/sample - loss: 0.4650 - val_loss: 0.7815\n",
      "Epoch 69/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4634 - val_loss: 0.8466\n",
      "Epoch 70/215\n",
      "1092/1092 [==============================] - 1s 966us/sample - loss: 0.4605 - val_loss: 0.8506\n",
      "Epoch 71/215\n",
      "1092/1092 [==============================] - 1s 989us/sample - loss: 0.4591 - val_loss: 0.8217\n",
      "Epoch 72/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4554 - val_loss: 0.8544\n",
      "Epoch 73/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4543 - val_loss: 0.8497\n",
      "Epoch 74/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4519 - val_loss: 0.8130\n",
      "Epoch 75/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4515 - val_loss: 0.8474\n",
      "Epoch 76/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4493 - val_loss: 0.8922\n",
      "Epoch 77/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4479 - val_loss: 0.8025\n",
      "Epoch 78/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4462 - val_loss: 0.8497\n",
      "Epoch 79/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4432 - val_loss: 0.8708\n",
      "Epoch 80/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4414 - val_loss: 0.8195\n",
      "Epoch 81/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4392 - val_loss: 0.9093\n",
      "Epoch 82/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4370 - val_loss: 0.8391\n",
      "Epoch 83/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4334 - val_loss: 0.8764\n",
      "Epoch 84/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4324 - val_loss: 0.8637\n",
      "Epoch 85/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4320 - val_loss: 0.8367\n",
      "Epoch 86/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4280 - val_loss: 0.9304\n",
      "Epoch 87/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4266 - val_loss: 0.8465\n",
      "Epoch 88/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4254 - val_loss: 0.8952\n",
      "Epoch 89/215\n",
      "1092/1092 [==============================] - 1s 992us/sample - loss: 0.4241 - val_loss: 0.8343\n",
      "Epoch 90/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4222 - val_loss: 0.9297\n",
      "Epoch 91/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4199 - val_loss: 0.8821\n",
      "Epoch 92/215\n",
      "1092/1092 [==============================] - 1s 957us/sample - loss: 0.4202 - val_loss: 0.8857\n",
      "Epoch 93/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4148 - val_loss: 0.9204\n",
      "Epoch 94/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4131 - val_loss: 0.8546\n",
      "Epoch 95/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4104 - val_loss: 0.9549\n",
      "Epoch 96/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4088 - val_loss: 0.9286\n",
      "Epoch 97/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4068 - val_loss: 0.8869\n",
      "Epoch 98/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4039 - val_loss: 0.8500\n",
      "Epoch 99/215\n",
      "1092/1092 [==============================] - 1s 967us/sample - loss: 0.4050 - val_loss: 0.9912\n",
      "Epoch 100/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4080 - val_loss: 0.8874\n",
      "Epoch 101/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4031 - val_loss: 0.9434\n",
      "Epoch 102/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4004 - val_loss: 0.8422\n",
      "Epoch 103/215\n",
      "1092/1092 [==============================] - 1s 996us/sample - loss: 0.4058 - val_loss: 0.9989\n",
      "Epoch 104/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4014 - val_loss: 0.8729\n",
      "Epoch 105/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3959 - val_loss: 0.9207\n",
      "Epoch 106/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3892 - val_loss: 0.9132\n",
      "Epoch 107/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3878 - val_loss: 0.8972\n",
      "Epoch 108/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3859 - val_loss: 0.9283\n",
      "Epoch 109/215\n",
      "1092/1092 [==============================] - 1s 995us/sample - loss: 0.3882 - val_loss: 0.9259\n",
      "Epoch 110/215\n",
      "1092/1092 [==============================] - 1s 997us/sample - loss: 0.3890 - val_loss: 0.9502\n",
      "Epoch 111/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3825 - val_loss: 0.9419\n",
      "Epoch 112/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3822 - val_loss: 0.9760\n",
      "Epoch 113/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3840 - val_loss: 0.8970\n",
      "Epoch 114/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3785 - val_loss: 0.9785\n",
      "Epoch 115/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3739 - val_loss: 0.9448\n",
      "Epoch 116/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3736 - val_loss: 0.9483\n",
      "Epoch 117/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3713 - val_loss: 0.9267\n",
      "Epoch 118/215\n",
      "1092/1092 [==============================] - 1s 973us/sample - loss: 0.3727 - val_loss: 0.9629\n",
      "Epoch 119/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3807 - val_loss: 1.0041\n",
      "Epoch 120/215\n",
      "1092/1092 [==============================] - 1s 994us/sample - loss: 0.3724 - val_loss: 0.9533\n",
      "Epoch 121/215\n",
      "1092/1092 [==============================] - 1s 968us/sample - loss: 0.3663 - val_loss: 0.9791\n",
      "Epoch 122/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3663 - val_loss: 0.9205\n",
      "Epoch 123/215\n",
      "1092/1092 [==============================] - 1s 999us/sample - loss: 0.3647 - val_loss: 0.9998\n",
      "Epoch 124/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3637 - val_loss: 0.9626\n",
      "Epoch 125/215\n",
      "1092/1092 [==============================] - 1s 1000us/sample - loss: 0.3645 - val_loss: 0.9123\n",
      "Epoch 126/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3626 - val_loss: 1.0220\n",
      "Epoch 127/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3588 - val_loss: 0.9569\n",
      "Epoch 128/215\n",
      "1092/1092 [==============================] - 1s 946us/sample - loss: 0.3550 - val_loss: 0.9663\n",
      "Epoch 129/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3541 - val_loss: 0.9663\n",
      "Epoch 130/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3525 - val_loss: 0.9865\n",
      "Epoch 131/215\n",
      "1092/1092 [==============================] - 1s 980us/sample - loss: 0.3524 - val_loss: 0.9691\n",
      "Epoch 132/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3497 - val_loss: 0.9782\n",
      "Epoch 133/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3499 - val_loss: 1.0133\n",
      "Epoch 134/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3499 - val_loss: 0.9352\n",
      "Epoch 135/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3485 - val_loss: 1.0051\n",
      "Epoch 136/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3449 - val_loss: 1.0202\n",
      "Epoch 137/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3453 - val_loss: 1.0151\n",
      "Epoch 138/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3424 - val_loss: 0.9559\n",
      "Epoch 139/215\n",
      "1092/1092 [==============================] - 1s 960us/sample - loss: 0.3439 - val_loss: 0.9716\n",
      "Epoch 140/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3418 - val_loss: 0.9703\n",
      "Epoch 141/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3410 - val_loss: 0.9848\n",
      "Epoch 142/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3391 - val_loss: 1.0528\n",
      "Epoch 143/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3418 - val_loss: 0.9763\n",
      "Epoch 144/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3384 - val_loss: 0.9745\n",
      "Epoch 145/215\n",
      "1092/1092 [==============================] - 1s 995us/sample - loss: 0.3355 - val_loss: 0.9980\n",
      "Epoch 146/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3364 - val_loss: 1.0571\n",
      "Epoch 147/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3388 - val_loss: 1.0622\n",
      "Epoch 148/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3380 - val_loss: 1.0025\n",
      "Epoch 149/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3325 - val_loss: 0.9702\n",
      "Epoch 150/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3300 - val_loss: 1.0001\n",
      "Epoch 151/215\n",
      "1092/1092 [==============================] - 1s 973us/sample - loss: 0.3277 - val_loss: 1.0169\n",
      "Epoch 152/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3259 - val_loss: 1.0210\n",
      "Epoch 153/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3249 - val_loss: 1.0112\n",
      "Epoch 154/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3235 - val_loss: 1.0041\n",
      "Epoch 155/215\n",
      "1092/1092 [==============================] - 1s 971us/sample - loss: 0.3227 - val_loss: 0.9580\n",
      "Epoch 156/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3272 - val_loss: 0.9832\n",
      "Epoch 157/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3251 - val_loss: 1.0045\n",
      "Epoch 158/215\n",
      "1092/1092 [==============================] - 1s 972us/sample - loss: 0.3201 - val_loss: 1.0467\n",
      "Epoch 159/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3191 - val_loss: 1.0040\n",
      "Epoch 160/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3175 - val_loss: 1.0590\n",
      "Epoch 161/215\n",
      "1092/1092 [==============================] - 1s 980us/sample - loss: 0.3158 - val_loss: 1.0555\n",
      "Epoch 162/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3164 - val_loss: 1.0432\n",
      "Epoch 163/215\n",
      "1092/1092 [==============================] - 1s 979us/sample - loss: 0.3144 - val_loss: 1.0481\n",
      "Epoch 164/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3132 - val_loss: 0.9999\n",
      "Epoch 165/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3125 - val_loss: 1.0495\n",
      "Epoch 166/215\n",
      "1092/1092 [==============================] - 1s 998us/sample - loss: 0.3117 - val_loss: 0.9821\n",
      "Epoch 167/215\n",
      "1092/1092 [==============================] - 1s 970us/sample - loss: 0.3110 - val_loss: 1.0775\n",
      "Epoch 168/215\n",
      "1092/1092 [==============================] - 1s 987us/sample - loss: 0.3127 - val_loss: 1.0275\n",
      "Epoch 169/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3094 - val_loss: 1.0066\n",
      "Epoch 170/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3104 - val_loss: 1.0324\n",
      "Epoch 171/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3099 - val_loss: 0.9974\n",
      "Epoch 172/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3081 - val_loss: 1.0456\n",
      "Epoch 173/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3131 - val_loss: 0.9872\n",
      "Epoch 174/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3070 - val_loss: 1.0134\n",
      "Epoch 175/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3052 - val_loss: 1.0051\n",
      "Epoch 176/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3065 - val_loss: 1.0258\n",
      "Epoch 177/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3029 - val_loss: 1.0829\n",
      "Epoch 178/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3025 - val_loss: 1.0171\n",
      "Epoch 179/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3007 - val_loss: 1.0440\n",
      "Epoch 180/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3021 - val_loss: 1.0453\n",
      "Epoch 181/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2986 - val_loss: 1.0464\n",
      "Epoch 182/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2978 - val_loss: 1.0562\n",
      "Epoch 183/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2985 - val_loss: 1.0341\n",
      "Epoch 184/215\n",
      "1092/1092 [==============================] - 1s 978us/sample - loss: 0.2986 - val_loss: 1.0937\n",
      "Epoch 185/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2956 - val_loss: 1.1064\n",
      "Epoch 186/215\n",
      "1092/1092 [==============================] - 1s 977us/sample - loss: 0.2963 - val_loss: 1.1134\n",
      "Epoch 187/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2952 - val_loss: 1.1013\n",
      "Epoch 188/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2935 - val_loss: 1.1928\n",
      "Epoch 189/215\n",
      "1092/1092 [==============================] - 1s 999us/sample - loss: 0.2948 - val_loss: 1.1010\n",
      "Epoch 190/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.2948 - val_loss: 1.1049\n",
      "Epoch 191/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2912 - val_loss: 1.1262\n",
      "Epoch 192/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2896 - val_loss: 1.1227\n",
      "Epoch 193/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2899 - val_loss: 1.0637\n",
      "Epoch 194/215\n",
      "1092/1092 [==============================] - 1s 968us/sample - loss: 0.2893 - val_loss: 1.0957\n",
      "Epoch 195/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2891 - val_loss: 1.1746\n",
      "Epoch 196/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2904 - val_loss: 1.1368\n",
      "Epoch 197/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2864 - val_loss: 1.1244\n",
      "Epoch 198/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2873 - val_loss: 1.1620\n",
      "Epoch 199/215\n",
      "1092/1092 [==============================] - 1s 987us/sample - loss: 0.2886 - val_loss: 1.0985\n",
      "Epoch 200/215\n",
      "1092/1092 [==============================] - 1s 993us/sample - loss: 0.2847 - val_loss: 1.1576\n",
      "Epoch 201/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2853 - val_loss: 1.1174\n",
      "Epoch 202/215\n",
      "1092/1092 [==============================] - 1s 984us/sample - loss: 0.2828 - val_loss: 1.0793\n",
      "Epoch 203/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2840 - val_loss: 1.0994\n",
      "Epoch 204/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2844 - val_loss: 1.1379\n",
      "Epoch 205/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2802 - val_loss: 1.1728\n",
      "Epoch 206/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.2797 - val_loss: 1.1169\n",
      "Epoch 207/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2795 - val_loss: 1.0665\n",
      "Epoch 208/215\n",
      "1092/1092 [==============================] - 1s 959us/sample - loss: 0.2832 - val_loss: 1.0787\n",
      "Epoch 209/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2882 - val_loss: 1.1152\n",
      "Epoch 210/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2843 - val_loss: 1.1097\n",
      "Epoch 211/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2774 - val_loss: 1.1431\n",
      "Epoch 212/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2771 - val_loss: 1.2159\n",
      "Epoch 213/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2815 - val_loss: 1.2206\n",
      "Epoch 214/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2774 - val_loss: 1.1186\n",
      "Epoch 215/215\n",
      "1092/1092 [==============================] - 1s 997us/sample - loss: 0.2737 - val_loss: 1.1415\n",
      "Train on 1092 samples, validate on 137 samples\n",
      "Epoch 1/215\n",
      "1092/1092 [==============================] - 4s 4ms/sample - loss: 0.5995 - val_loss: 0.6207\n",
      "Epoch 2/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5948 - val_loss: 0.6212\n",
      "Epoch 3/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5932 - val_loss: 0.6212\n",
      "Epoch 4/215\n",
      "1092/1092 [==============================] - 1s 988us/sample - loss: 0.5927 - val_loss: 0.6236\n",
      "Epoch 5/215\n",
      "1092/1092 [==============================] - 1s 954us/sample - loss: 0.5901 - val_loss: 0.6164\n",
      "Epoch 6/215\n",
      "1092/1092 [==============================] - 1s 975us/sample - loss: 0.5888 - val_loss: 0.6180\n",
      "Epoch 7/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5878 - val_loss: 0.6245\n",
      "Epoch 8/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5873 - val_loss: 0.6149\n",
      "Epoch 9/215\n",
      "1092/1092 [==============================] - 1s 947us/sample - loss: 0.5840 - val_loss: 0.6231\n",
      "Epoch 10/215\n",
      "1092/1092 [==============================] - 1s 970us/sample - loss: 0.5832 - val_loss: 0.6224\n",
      "Epoch 11/215\n",
      "1092/1092 [==============================] - 1s 931us/sample - loss: 0.5819 - val_loss: 0.6134\n",
      "Epoch 12/215\n",
      "1092/1092 [==============================] - 1s 977us/sample - loss: 0.5812 - val_loss: 0.6243\n",
      "Epoch 13/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5811 - val_loss: 0.6118\n",
      "Epoch 14/215\n",
      "1092/1092 [==============================] - 1s 966us/sample - loss: 0.5762 - val_loss: 0.6292\n",
      "Epoch 15/215\n",
      "1092/1092 [==============================] - 1s 938us/sample - loss: 0.5738 - val_loss: 0.6157\n",
      "Epoch 16/215\n",
      "1092/1092 [==============================] - 1s 913us/sample - loss: 0.5718 - val_loss: 0.6201\n",
      "Epoch 17/215\n",
      "1092/1092 [==============================] - 1s 966us/sample - loss: 0.5700 - val_loss: 0.6262\n",
      "Epoch 18/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5682 - val_loss: 0.6187\n",
      "Epoch 19/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5655 - val_loss: 0.6220\n",
      "Epoch 20/215\n",
      "1092/1092 [==============================] - 1s 942us/sample - loss: 0.5624 - val_loss: 0.6222\n",
      "Epoch 21/215\n",
      "1092/1092 [==============================] - 1s 981us/sample - loss: 0.5598 - val_loss: 0.6217\n",
      "Epoch 22/215\n",
      "1092/1092 [==============================] - 1s 986us/sample - loss: 0.5577 - val_loss: 0.6185\n",
      "Epoch 23/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.5560 - val_loss: 0.6280\n",
      "Epoch 24/215\n",
      "1092/1092 [==============================] - 1s 985us/sample - loss: 0.5535 - val_loss: 0.6252\n",
      "Epoch 25/215\n",
      "1092/1092 [==============================] - 1s 945us/sample - loss: 0.5523 - val_loss: 0.6301\n",
      "Epoch 26/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5481 - val_loss: 0.6288\n",
      "Epoch 27/215\n",
      "1092/1092 [==============================] - 1s 977us/sample - loss: 0.5457 - val_loss: 0.6284\n",
      "Epoch 28/215\n",
      "1092/1092 [==============================] - 1s 980us/sample - loss: 0.5439 - val_loss: 0.6387\n",
      "Epoch 29/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5420 - val_loss: 0.6481\n",
      "Epoch 30/215\n",
      "1092/1092 [==============================] - 1s 701us/sample - loss: 0.5393 - val_loss: 0.6350\n",
      "Epoch 31/215\n",
      "1092/1092 [==============================] - 1s 640us/sample - loss: 0.5370 - val_loss: 0.6487\n",
      "Epoch 32/215\n",
      "1092/1092 [==============================] - 1s 910us/sample - loss: 0.5396 - val_loss: 0.6475\n",
      "Epoch 33/215\n",
      "1092/1092 [==============================] - 1s 954us/sample - loss: 0.5360 - val_loss: 0.6681\n",
      "Epoch 34/215\n",
      "1092/1092 [==============================] - 1s 944us/sample - loss: 0.5331 - val_loss: 0.6500\n",
      "Epoch 35/215\n",
      "1092/1092 [==============================] - 1s 905us/sample - loss: 0.5314 - val_loss: 0.6523\n",
      "Epoch 36/215\n",
      "1092/1092 [==============================] - 1s 945us/sample - loss: 0.5274 - val_loss: 0.6473\n",
      "Epoch 37/215\n",
      "1092/1092 [==============================] - 1s 969us/sample - loss: 0.5243 - val_loss: 0.6547\n",
      "Epoch 38/215\n",
      "1092/1092 [==============================] - 1s 717us/sample - loss: 0.5226 - val_loss: 0.6755\n",
      "Epoch 39/215\n",
      "1092/1092 [==============================] - 1s 943us/sample - loss: 0.5193 - val_loss: 0.6953\n",
      "Epoch 40/215\n",
      "1092/1092 [==============================] - 1s 690us/sample - loss: 0.5185 - val_loss: 0.6711\n",
      "Epoch 41/215\n",
      "1092/1092 [==============================] - 1s 758us/sample - loss: 0.5144 - val_loss: 0.6922\n",
      "Epoch 42/215\n",
      "1092/1092 [==============================] - 1s 671us/sample - loss: 0.5125 - val_loss: 0.6770\n",
      "Epoch 43/215\n",
      "1092/1092 [==============================] - 1s 690us/sample - loss: 0.5106 - val_loss: 0.6971\n",
      "Epoch 44/215\n",
      "1092/1092 [==============================] - 1s 673us/sample - loss: 0.5079 - val_loss: 0.7266\n",
      "Epoch 45/215\n",
      "1092/1092 [==============================] - 1s 685us/sample - loss: 0.5088 - val_loss: 0.7291\n",
      "Epoch 46/215\n",
      "1092/1092 [==============================] - 1s 651us/sample - loss: 0.5064 - val_loss: 0.7023\n",
      "Epoch 47/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.5023 - val_loss: 0.7108\n",
      "Epoch 48/215\n",
      "1092/1092 [==============================] - 1s 1000us/sample - loss: 0.4995 - val_loss: 0.6992\n",
      "Epoch 49/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4971 - val_loss: 0.7521\n",
      "Epoch 50/215\n",
      "1092/1092 [==============================] - 1s 955us/sample - loss: 0.4994 - val_loss: 0.7546\n",
      "Epoch 51/215\n",
      "1092/1092 [==============================] - 1s 928us/sample - loss: 0.4945 - val_loss: 0.7259\n",
      "Epoch 52/215\n",
      "1092/1092 [==============================] - 1s 843us/sample - loss: 0.4914 - val_loss: 0.7259\n",
      "Epoch 53/215\n",
      "1092/1092 [==============================] - 1s 768us/sample - loss: 0.4907 - val_loss: 0.7127\n",
      "Epoch 54/215\n",
      "1092/1092 [==============================] - 1s 703us/sample - loss: 0.4906 - val_loss: 0.7643\n",
      "Epoch 55/215\n",
      "1092/1092 [==============================] - 1s 671us/sample - loss: 0.4894 - val_loss: 0.7240\n",
      "Epoch 56/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4870 - val_loss: 0.8164\n",
      "Epoch 57/215\n",
      "1092/1092 [==============================] - 1s 965us/sample - loss: 0.4837 - val_loss: 0.7827\n",
      "Epoch 58/215\n",
      "1092/1092 [==============================] - 1s 981us/sample - loss: 0.4829 - val_loss: 0.8120\n",
      "Epoch 59/215\n",
      "1092/1092 [==============================] - 1s 988us/sample - loss: 0.4816 - val_loss: 0.7869\n",
      "Epoch 60/215\n",
      "1092/1092 [==============================] - 1s 989us/sample - loss: 0.4807 - val_loss: 0.8150\n",
      "Epoch 61/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4757 - val_loss: 0.7540\n",
      "Epoch 62/215\n",
      "1092/1092 [==============================] - 1s 995us/sample - loss: 0.4774 - val_loss: 0.7603\n",
      "Epoch 63/215\n",
      "1092/1092 [==============================] - 1s 983us/sample - loss: 0.4723 - val_loss: 0.7340\n",
      "Epoch 64/215\n",
      "1092/1092 [==============================] - 1s 942us/sample - loss: 0.4733 - val_loss: 0.7749\n",
      "Epoch 65/215\n",
      "1092/1092 [==============================] - 1s 973us/sample - loss: 0.4717 - val_loss: 0.7410\n",
      "Epoch 66/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4697 - val_loss: 0.7554\n",
      "Epoch 67/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4642 - val_loss: 0.8016\n",
      "Epoch 68/215\n",
      "1092/1092 [==============================] - 1s 978us/sample - loss: 0.4627 - val_loss: 0.7871\n",
      "Epoch 69/215\n",
      "1092/1092 [==============================] - 1s 959us/sample - loss: 0.4614 - val_loss: 0.8481\n",
      "Epoch 70/215\n",
      "1092/1092 [==============================] - 1s 978us/sample - loss: 0.4585 - val_loss: 0.8444\n",
      "Epoch 71/215\n",
      "1092/1092 [==============================] - 1s 981us/sample - loss: 0.4573 - val_loss: 0.8286\n",
      "Epoch 72/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4533 - val_loss: 0.8504\n",
      "Epoch 73/215\n",
      "1092/1092 [==============================] - 1s 990us/sample - loss: 0.4522 - val_loss: 0.8479\n",
      "Epoch 74/215\n",
      "1092/1092 [==============================] - 1s 989us/sample - loss: 0.4496 - val_loss: 0.8145\n",
      "Epoch 75/215\n",
      "1092/1092 [==============================] - 1s 946us/sample - loss: 0.4490 - val_loss: 0.8430\n",
      "Epoch 76/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4470 - val_loss: 0.8873\n",
      "Epoch 77/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4454 - val_loss: 0.7969\n",
      "Epoch 78/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4446 - val_loss: 0.8547\n",
      "Epoch 79/215\n",
      "1092/1092 [==============================] - 1s 968us/sample - loss: 0.4413 - val_loss: 0.8668\n",
      "Epoch 80/215\n",
      "1092/1092 [==============================] - 1s 987us/sample - loss: 0.4396 - val_loss: 0.8226\n",
      "Epoch 81/215\n",
      "1092/1092 [==============================] - 1s 942us/sample - loss: 0.4375 - val_loss: 0.9100\n",
      "Epoch 82/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.4352 - val_loss: 0.8379\n",
      "Epoch 83/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4313 - val_loss: 0.8801\n",
      "Epoch 84/215\n",
      "1092/1092 [==============================] - 1s 951us/sample - loss: 0.4303 - val_loss: 0.8643\n",
      "Epoch 85/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4301 - val_loss: 0.8381\n",
      "Epoch 86/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4258 - val_loss: 0.9347\n",
      "Epoch 87/215\n",
      "1092/1092 [==============================] - 1s 979us/sample - loss: 0.4248 - val_loss: 0.8473\n",
      "Epoch 88/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4232 - val_loss: 0.9055\n",
      "Epoch 89/215\n",
      "1092/1092 [==============================] - 1s 938us/sample - loss: 0.4220 - val_loss: 0.8355\n",
      "Epoch 90/215\n",
      "1092/1092 [==============================] - 1s 964us/sample - loss: 0.4201 - val_loss: 0.9310\n",
      "Epoch 91/215\n",
      "1092/1092 [==============================] - 1s 948us/sample - loss: 0.4176 - val_loss: 0.8835\n",
      "Epoch 92/215\n",
      "1092/1092 [==============================] - 1s 964us/sample - loss: 0.4174 - val_loss: 0.8920\n",
      "Epoch 93/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4126 - val_loss: 0.9204\n",
      "Epoch 94/215\n",
      "1092/1092 [==============================] - 1s 979us/sample - loss: 0.4110 - val_loss: 0.8618\n",
      "Epoch 95/215\n",
      "1092/1092 [==============================] - 1s 997us/sample - loss: 0.4080 - val_loss: 0.9504\n",
      "Epoch 96/215\n",
      "1092/1092 [==============================] - 1s 976us/sample - loss: 0.4066 - val_loss: 0.9327\n",
      "Epoch 97/215\n",
      "1092/1092 [==============================] - 1s 928us/sample - loss: 0.4048 - val_loss: 0.8904\n",
      "Epoch 98/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.4018 - val_loss: 0.8585\n",
      "Epoch 99/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.4028 - val_loss: 0.9893\n",
      "Epoch 100/215\n",
      "1092/1092 [==============================] - 1s 922us/sample - loss: 0.4059 - val_loss: 0.8933\n",
      "Epoch 101/215\n",
      "1092/1092 [==============================] - 1s 971us/sample - loss: 0.4019 - val_loss: 0.9445\n",
      "Epoch 102/215\n",
      "1092/1092 [==============================] - 1s 919us/sample - loss: 0.3989 - val_loss: 0.8453\n",
      "Epoch 103/215\n",
      "1092/1092 [==============================] - 1s 947us/sample - loss: 0.4044 - val_loss: 1.0034\n",
      "Epoch 104/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3992 - val_loss: 0.8781\n",
      "Epoch 105/215\n",
      "1092/1092 [==============================] - 1s 959us/sample - loss: 0.3940 - val_loss: 0.9280\n",
      "Epoch 106/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3872 - val_loss: 0.9164\n",
      "Epoch 107/215\n",
      "1092/1092 [==============================] - 1s 993us/sample - loss: 0.3857 - val_loss: 0.9013\n",
      "Epoch 108/215\n",
      "1092/1092 [==============================] - 1s 972us/sample - loss: 0.3839 - val_loss: 0.9299\n",
      "Epoch 109/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3858 - val_loss: 0.9330\n",
      "Epoch 110/215\n",
      "1092/1092 [==============================] - 1s 999us/sample - loss: 0.3864 - val_loss: 0.9469\n",
      "Epoch 111/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3805 - val_loss: 0.9503\n",
      "Epoch 112/215\n",
      "1092/1092 [==============================] - 1s 981us/sample - loss: 0.3805 - val_loss: 0.9720\n",
      "Epoch 113/215\n",
      "1092/1092 [==============================] - 1s 924us/sample - loss: 0.3813 - val_loss: 0.9014\n",
      "Epoch 114/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3758 - val_loss: 0.9845\n",
      "Epoch 115/215\n",
      "1092/1092 [==============================] - 1s 958us/sample - loss: 0.3718 - val_loss: 0.9438\n",
      "Epoch 116/215\n",
      "1092/1092 [==============================] - 1s 941us/sample - loss: 0.3710 - val_loss: 0.9574\n",
      "Epoch 117/215\n",
      "1092/1092 [==============================] - 1s 982us/sample - loss: 0.3691 - val_loss: 0.9273\n",
      "Epoch 118/215\n",
      "1092/1092 [==============================] - 1s 938us/sample - loss: 0.3711 - val_loss: 0.9652\n",
      "Epoch 119/215\n",
      "1092/1092 [==============================] - 1s 973us/sample - loss: 0.3794 - val_loss: 1.0113\n",
      "Epoch 120/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3703 - val_loss: 0.9563\n",
      "Epoch 121/215\n",
      "1092/1092 [==============================] - 1s 931us/sample - loss: 0.3640 - val_loss: 0.9788\n",
      "Epoch 122/215\n",
      "1092/1092 [==============================] - 1s 943us/sample - loss: 0.3634 - val_loss: 0.9242\n",
      "Epoch 123/215\n",
      "1092/1092 [==============================] - 1s 932us/sample - loss: 0.3621 - val_loss: 0.9979\n",
      "Epoch 124/215\n",
      "1092/1092 [==============================] - 1s 931us/sample - loss: 0.3615 - val_loss: 0.9661\n",
      "Epoch 125/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3620 - val_loss: 0.9119\n",
      "Epoch 126/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3600 - val_loss: 1.0383\n",
      "Epoch 127/215\n",
      "1092/1092 [==============================] - 1s 935us/sample - loss: 0.3566 - val_loss: 0.9581\n",
      "Epoch 128/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3533 - val_loss: 0.9745\n",
      "Epoch 129/215\n",
      "1092/1092 [==============================] - 1s 922us/sample - loss: 0.3518 - val_loss: 0.9716\n",
      "Epoch 130/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3503 - val_loss: 0.9915\n",
      "Epoch 131/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3494 - val_loss: 0.9779\n",
      "Epoch 132/215\n",
      "1092/1092 [==============================] - 1s 953us/sample - loss: 0.3468 - val_loss: 0.9701\n",
      "Epoch 133/215\n",
      "1092/1092 [==============================] - 1s 964us/sample - loss: 0.3469 - val_loss: 1.0317\n",
      "Epoch 134/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3471 - val_loss: 0.9424\n",
      "Epoch 135/215\n",
      "1092/1092 [==============================] - 1s 920us/sample - loss: 0.3464 - val_loss: 1.0020\n",
      "Epoch 136/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3422 - val_loss: 1.0274\n",
      "Epoch 137/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3426 - val_loss: 1.0335\n",
      "Epoch 138/215\n",
      "1092/1092 [==============================] - 1s 970us/sample - loss: 0.3400 - val_loss: 0.9636\n",
      "Epoch 139/215\n",
      "1092/1092 [==============================] - 1s 986us/sample - loss: 0.3417 - val_loss: 0.9744\n",
      "Epoch 140/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3391 - val_loss: 0.9763\n",
      "Epoch 141/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3391 - val_loss: 0.9856\n",
      "Epoch 142/215\n",
      "1092/1092 [==============================] - 1s 974us/sample - loss: 0.3360 - val_loss: 1.0689\n",
      "Epoch 143/215\n",
      "1092/1092 [==============================] - 1s 955us/sample - loss: 0.3394 - val_loss: 0.9883\n",
      "Epoch 144/215\n",
      "1092/1092 [==============================] - 1s 961us/sample - loss: 0.3364 - val_loss: 0.9875\n",
      "Epoch 145/215\n",
      "1092/1092 [==============================] - 1s 972us/sample - loss: 0.3316 - val_loss: 1.0060\n",
      "Epoch 146/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.3322 - val_loss: 1.0535\n",
      "Epoch 147/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3340 - val_loss: 1.0674\n",
      "Epoch 148/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3341 - val_loss: 1.0470\n",
      "Epoch 149/215\n",
      "1092/1092 [==============================] - 1s 961us/sample - loss: 0.3299 - val_loss: 0.9689\n",
      "Epoch 150/215\n",
      "1092/1092 [==============================] - 1s 949us/sample - loss: 0.3295 - val_loss: 0.9999\n",
      "Epoch 151/215\n",
      "1092/1092 [==============================] - 1s 927us/sample - loss: 0.3251 - val_loss: 1.0277\n",
      "Epoch 152/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3240 - val_loss: 1.0372\n",
      "Epoch 153/215\n",
      "1092/1092 [==============================] - 1s 971us/sample - loss: 0.3224 - val_loss: 1.0308\n",
      "Epoch 154/215\n",
      "1092/1092 [==============================] - 1s 922us/sample - loss: 0.3205 - val_loss: 1.0166\n",
      "Epoch 155/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3191 - val_loss: 0.9831\n",
      "Epoch 156/215\n",
      "1092/1092 [==============================] - 1s 982us/sample - loss: 0.3238 - val_loss: 0.9898\n",
      "Epoch 157/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3215 - val_loss: 0.9973\n",
      "Epoch 158/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3181 - val_loss: 1.0432\n",
      "Epoch 159/215\n",
      "1092/1092 [==============================] - 1s 960us/sample - loss: 0.3176 - val_loss: 1.0081\n",
      "Epoch 160/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3150 - val_loss: 1.0652\n",
      "Epoch 161/215\n",
      "1092/1092 [==============================] - 1s 970us/sample - loss: 0.3127 - val_loss: 1.0702\n",
      "Epoch 162/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3135 - val_loss: 1.0662\n",
      "Epoch 163/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.3109 - val_loss: 1.0664\n",
      "Epoch 164/215\n",
      "1092/1092 [==============================] - 1s 972us/sample - loss: 0.3104 - val_loss: 1.0133\n",
      "Epoch 165/215\n",
      "1092/1092 [==============================] - 1s 939us/sample - loss: 0.3095 - val_loss: 1.0676\n",
      "Epoch 166/215\n",
      "1092/1092 [==============================] - 1s 926us/sample - loss: 0.3086 - val_loss: 0.9954\n",
      "Epoch 167/215\n",
      "1092/1092 [==============================] - 1s 993us/sample - loss: 0.3082 - val_loss: 1.0865\n",
      "Epoch 168/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3107 - val_loss: 1.0437\n",
      "Epoch 169/215\n",
      "1092/1092 [==============================] - 1s 973us/sample - loss: 0.3077 - val_loss: 1.0201\n",
      "Epoch 170/215\n",
      "1092/1092 [==============================] - 1s 983us/sample - loss: 0.3077 - val_loss: 1.0448\n",
      "Epoch 171/215\n",
      "1092/1092 [==============================] - 1s 967us/sample - loss: 0.3071 - val_loss: 1.0105\n",
      "Epoch 172/215\n",
      "1092/1092 [==============================] - 1s 939us/sample - loss: 0.3046 - val_loss: 1.0695\n",
      "Epoch 173/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.3095 - val_loss: 1.0032\n",
      "Epoch 174/215\n",
      "1092/1092 [==============================] - 1s 954us/sample - loss: 0.3036 - val_loss: 1.0424\n",
      "Epoch 175/215\n",
      "1092/1092 [==============================] - 1s 977us/sample - loss: 0.3009 - val_loss: 1.0306\n",
      "Epoch 176/215\n",
      "1092/1092 [==============================] - 1s 965us/sample - loss: 0.3022 - val_loss: 1.0528\n",
      "Epoch 177/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2994 - val_loss: 1.1113\n",
      "Epoch 178/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.2991 - val_loss: 1.0353\n",
      "Epoch 179/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2981 - val_loss: 1.0716\n",
      "Epoch 180/215\n",
      "1092/1092 [==============================] - 1s 966us/sample - loss: 0.2983 - val_loss: 1.0660\n",
      "Epoch 181/215\n",
      "1092/1092 [==============================] - 1s 927us/sample - loss: 0.2959 - val_loss: 1.0592\n",
      "Epoch 182/215\n",
      "1092/1092 [==============================] - 1s 970us/sample - loss: 0.2953 - val_loss: 1.0768\n",
      "Epoch 183/215\n",
      "1092/1092 [==============================] - 1s 961us/sample - loss: 0.2952 - val_loss: 1.0492\n",
      "Epoch 184/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2961 - val_loss: 1.1072\n",
      "Epoch 185/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2929 - val_loss: 1.1218\n",
      "Epoch 186/215\n",
      "1092/1092 [==============================] - 1s 981us/sample - loss: 0.2934 - val_loss: 1.1301\n",
      "Epoch 187/215\n",
      "1092/1092 [==============================] - 1s 997us/sample - loss: 0.2926 - val_loss: 1.1170\n",
      "Epoch 188/215\n",
      "1092/1092 [==============================] - 1s 977us/sample - loss: 0.2908 - val_loss: 1.2087\n",
      "Epoch 189/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2921 - val_loss: 1.1118\n",
      "Epoch 190/215\n",
      "1092/1092 [==============================] - 1s 914us/sample - loss: 0.2915 - val_loss: 1.1205\n",
      "Epoch 191/215\n",
      "1092/1092 [==============================] - 1s 989us/sample - loss: 0.2882 - val_loss: 1.1499\n",
      "Epoch 192/215\n",
      "1092/1092 [==============================] - 1s 966us/sample - loss: 0.2868 - val_loss: 1.1433\n",
      "Epoch 193/215\n",
      "1092/1092 [==============================] - 1s 987us/sample - loss: 0.2865 - val_loss: 1.0887\n",
      "Epoch 194/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2853 - val_loss: 1.1055\n",
      "Epoch 195/215\n",
      "1092/1092 [==============================] - 1s 976us/sample - loss: 0.2853 - val_loss: 1.1836\n",
      "Epoch 196/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2853 - val_loss: 1.1346\n",
      "Epoch 197/215\n",
      "1092/1092 [==============================] - 1s 891us/sample - loss: 0.2827 - val_loss: 1.1227\n",
      "Epoch 198/215\n",
      "1092/1092 [==============================] - 1s 997us/sample - loss: 0.2826 - val_loss: 1.1645\n",
      "Epoch 199/215\n",
      "1092/1092 [==============================] - 1s 999us/sample - loss: 0.2835 - val_loss: 1.1019\n",
      "Epoch 200/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2829 - val_loss: 1.1922\n",
      "Epoch 201/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2842 - val_loss: 1.1293\n",
      "Epoch 202/215\n",
      "1092/1092 [==============================] - 1s 962us/sample - loss: 0.2802 - val_loss: 1.0963\n",
      "Epoch 203/215\n",
      "1092/1092 [==============================] - 1s 932us/sample - loss: 0.2824 - val_loss: 1.1168\n",
      "Epoch 204/215\n",
      "1092/1092 [==============================] - 1s 944us/sample - loss: 0.2824 - val_loss: 1.1811\n",
      "Epoch 205/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.2779 - val_loss: 1.2012\n",
      "Epoch 206/215\n",
      "1092/1092 [==============================] - 1s 993us/sample - loss: 0.2774 - val_loss: 1.1236\n",
      "Epoch 207/215\n",
      "1092/1092 [==============================] - 1s 928us/sample - loss: 0.2784 - val_loss: 1.0796\n",
      "Epoch 208/215\n",
      "1092/1092 [==============================] - 1s 934us/sample - loss: 0.2830 - val_loss: 1.0877\n",
      "Epoch 209/215\n",
      "1092/1092 [==============================] - 1s 1ms/sample - loss: 0.2874 - val_loss: 1.1182\n",
      "Epoch 210/215\n",
      "1092/1092 [==============================] - 2s 2ms/sample - loss: 0.2822 - val_loss: 1.1027\n",
      "Epoch 211/215\n",
      "1092/1092 [==============================] - 1s 982us/sample - loss: 0.2746 - val_loss: 1.1395\n",
      "Epoch 212/215\n",
      "1092/1092 [==============================] - 1s 963us/sample - loss: 0.2741 - val_loss: 1.2061\n",
      "Epoch 213/215\n",
      "1092/1092 [==============================] - 1s 994us/sample - loss: 0.2758 - val_loss: 1.2244\n",
      "Epoch 214/215\n",
      "1092/1092 [==============================] - 1s 982us/sample - loss: 0.2731 - val_loss: 1.1392\n",
      "Epoch 215/215\n",
      "1092/1092 [==============================] - 2s 1ms/sample - loss: 0.2708 - val_loss: 1.1657\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, seed in enumerate([7, 77, 77]):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "    performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa45d771c18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['auc', 'val_auc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999684360802951"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004853895020854922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>SIDER</td>\n",
       "      <td>0.918516</td>\n",
       "      <td>0.657146</td>\n",
       "      <td>0.697717</td>\n",
       "      <td>ROC</td>\n",
       "      <td>773563</td>\n",
       "      <td>214</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>SIDER</td>\n",
       "      <td>0.916467</td>\n",
       "      <td>0.657526</td>\n",
       "      <td>0.696649</td>\n",
       "      <td>ROC</td>\n",
       "      <td>773563</td>\n",
       "      <td>214</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SIDER</td>\n",
       "      <td>0.918030</td>\n",
       "      <td>0.650607</td>\n",
       "      <td>0.705539</td>\n",
       "      <td>ROC</td>\n",
       "      <td>773563</td>\n",
       "      <td>214</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0     SIDER   0.918516   0.657146  0.697717    ROC              773563   \n",
       "1     SIDER   0.916467   0.657526  0.696649    ROC              773563   \n",
       "2     SIDER   0.918030   0.650607  0.705539    ROC              773563   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0         214         128  0.0001             0  \n",
       "1         214         128  0.0001             0  \n",
       "2         214         128  0.0001             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
