{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: ClinTox number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'ClinTox'\n",
    "from chembench import load_data\n",
    "df, induces = load_data(task_name)\n",
    "\n",
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X1.astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [128] #2 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182 148 148\n",
      "epoch: 0001, loss: 0.6880 - val_loss: 0.6100; auc: 0.7220 - val_auc: 0.7036                                                                                                    \n",
      "epoch: 0002, loss: 0.6668 - val_loss: 0.5755; auc: 0.7925 - val_auc: 0.7844                                                                                                    \n",
      "epoch: 0003, loss: 0.6462 - val_loss: 0.5620; auc: 0.7955 - val_auc: 0.8000                                                                                                    \n",
      "epoch: 0004, loss: 0.6306 - val_loss: 0.5511; auc: 0.7941 - val_auc: 0.8165                                                                                                    \n",
      "epoch: 0005, loss: 0.6157 - val_loss: 0.5473; auc: 0.7979 - val_auc: 0.8143                                                                                                    \n",
      "epoch: 0006, loss: 0.5998 - val_loss: 0.5181; auc: 0.8075 - val_auc: 0.8179                                                                                                    \n",
      "epoch: 0007, loss: 0.5956 - val_loss: 0.5378; auc: 0.8057 - val_auc: 0.8219                                                                                                    \n",
      "epoch: 0008, loss: 0.5961 - val_loss: 0.5234; auc: 0.8131 - val_auc: 0.8214                                                                                                    \n",
      "epoch: 0009, loss: 0.5756 - val_loss: 0.4956; auc: 0.8275 - val_auc: 0.8259                                                                                                    \n",
      "epoch: 0010, loss: 0.5628 - val_loss: 0.5104; auc: 0.8285 - val_auc: 0.8254                                                                                                    \n",
      "epoch: 0011, loss: 0.5505 - val_loss: 0.5039; auc: 0.8330 - val_auc: 0.8272                                                                                                    \n",
      "epoch: 0012, loss: 0.5465 - val_loss: 0.4794; auc: 0.8441 - val_auc: 0.8277                                                                                                    \n",
      "epoch: 0013, loss: 0.5439 - val_loss: 0.5268; auc: 0.8454 - val_auc: 0.8326                                                                                                    \n",
      "epoch: 0014, loss: 0.5198 - val_loss: 0.4731; auc: 0.8589 - val_auc: 0.8330                                                                                                    \n",
      "epoch: 0015, loss: 0.5071 - val_loss: 0.4920; auc: 0.8586 - val_auc: 0.8335                                                                                                    \n",
      "epoch: 0016, loss: 0.5078 - val_loss: 0.4836; auc: 0.8637 - val_auc: 0.8353                                                                                                    \n",
      "epoch: 0017, loss: 0.4863 - val_loss: 0.4584; auc: 0.8791 - val_auc: 0.8393                                                                                                    \n",
      "epoch: 0018, loss: 0.5115 - val_loss: 0.5464; auc: 0.8656 - val_auc: 0.8344                                                                                                    \n",
      "epoch: 0019, loss: 0.4926 - val_loss: 0.4558; auc: 0.8869 - val_auc: 0.8406                                                                                                    \n",
      "epoch: 0020, loss: 0.4656 - val_loss: 0.4527; auc: 0.8931 - val_auc: 0.8429                                                                                                    \n",
      "epoch: 0021, loss: 0.4516 - val_loss: 0.4709; auc: 0.8949 - val_auc: 0.8424                                                                                                    \n",
      "epoch: 0022, loss: 0.4482 - val_loss: 0.4591; auc: 0.9013 - val_auc: 0.8438                                                                                                    \n",
      "epoch: 0023, loss: 0.4294 - val_loss: 0.4571; auc: 0.9181 - val_auc: 0.8424                                                                                                    \n",
      "epoch: 0024, loss: 0.4631 - val_loss: 0.4490; auc: 0.9119 - val_auc: 0.8455                                                                                                    \n",
      "epoch: 0025, loss: 0.4223 - val_loss: 0.4364; auc: 0.9219 - val_auc: 0.8473                                                                                                    \n",
      "epoch: 0026, loss: 0.4051 - val_loss: 0.4497; auc: 0.9219 - val_auc: 0.8491                                                                                                    \n",
      "epoch: 0027, loss: 0.3925 - val_loss: 0.4343; auc: 0.9305 - val_auc: 0.8527                                                                                                    \n",
      "epoch: 0028, loss: 0.3781 - val_loss: 0.4323; auc: 0.9420 - val_auc: 0.8536                                                                                                    \n",
      "epoch: 0029, loss: 0.3693 - val_loss: 0.4299; auc: 0.9484 - val_auc: 0.8594                                                                                                    \n",
      "epoch: 0030, loss: 0.3586 - val_loss: 0.4284; auc: 0.9519 - val_auc: 0.8594                                                                                                    \n",
      "epoch: 0031, loss: 0.3548 - val_loss: 0.4576; auc: 0.9602 - val_auc: 0.8576                                                                                                    \n",
      "epoch: 0032, loss: 0.3626 - val_loss: 0.4234; auc: 0.9578 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0033, loss: 0.3307 - val_loss: 0.4187; auc: 0.9579 - val_auc: 0.8634                                                                                                    \n",
      "epoch: 0034, loss: 0.3131 - val_loss: 0.4242; auc: 0.9658 - val_auc: 0.8634                                                                                                    \n",
      "epoch: 0035, loss: 0.3047 - val_loss: 0.4332; auc: 0.9643 - val_auc: 0.8603                                                                                                    \n",
      "epoch: 0036, loss: 0.2982 - val_loss: 0.4232; auc: 0.9714 - val_auc: 0.8625                                                                                                    \n",
      "epoch: 0037, loss: 0.2799 - val_loss: 0.4222; auc: 0.9719 - val_auc: 0.8643                                                                                                    \n",
      "epoch: 0038, loss: 0.2726 - val_loss: 0.4095; auc: 0.9716 - val_auc: 0.8683                                                                                                    \n",
      "epoch: 0039, loss: 0.2593 - val_loss: 0.4138; auc: 0.9790 - val_auc: 0.8710                                                                                                    \n",
      "epoch: 0040, loss: 0.2479 - val_loss: 0.4100; auc: 0.9789 - val_auc: 0.8696                                                                                                    \n",
      "epoch: 0041, loss: 0.2414 - val_loss: 0.4104; auc: 0.9783 - val_auc: 0.8732                                                                                                    \n",
      "epoch: 0042, loss: 0.2300 - val_loss: 0.4621; auc: 0.9850 - val_auc: 0.8598                                                                                                    \n",
      "epoch: 0043, loss: 0.2198 - val_loss: 0.4498; auc: 0.9875 - val_auc: 0.8652                                                                                                    \n",
      "epoch: 0044, loss: 0.2214 - val_loss: 0.4137; auc: 0.9844 - val_auc: 0.8746                                                                                                    \n",
      "epoch: 0045, loss: 0.2138 - val_loss: 0.4354; auc: 0.9855 - val_auc: 0.8795                                                                                                    \n",
      "epoch: 0046, loss: 0.1953 - val_loss: 0.4126; auc: 0.9886 - val_auc: 0.8777                                                                                                    \n",
      "epoch: 0047, loss: 0.1844 - val_loss: 0.4162; auc: 0.9901 - val_auc: 0.8737                                                                                                    \n",
      "epoch: 0048, loss: 0.1725 - val_loss: 0.4079; auc: 0.9905 - val_auc: 0.8723                                                                                                    \n",
      "epoch: 0049, loss: 0.1665 - val_loss: 0.3976; auc: 0.9894 - val_auc: 0.8786                                                                                                    \n",
      "epoch: 0050, loss: 0.1623 - val_loss: 0.3998; auc: 0.9898 - val_auc: 0.8772                                                                                                    \n",
      "epoch: 0051, loss: 0.1572 - val_loss: 0.4589; auc: 0.9922 - val_auc: 0.8661                                                                                                    \n",
      "epoch: 0052, loss: 0.1536 - val_loss: 0.5676; auc: 0.9936 - val_auc: 0.8603                                                                                                    \n",
      "epoch: 0053, loss: 0.1530 - val_loss: 0.4856; auc: 0.9937 - val_auc: 0.8665                                                                                                    \n",
      "epoch: 0054, loss: 0.1443 - val_loss: 0.4519; auc: 0.9944 - val_auc: 0.8580                                                                                                    \n",
      "epoch: 0055, loss: 0.1330 - val_loss: 0.4284; auc: 0.9932 - val_auc: 0.8719                                                                                                    \n",
      "epoch: 0056, loss: 0.1286 - val_loss: 0.4610; auc: 0.9936 - val_auc: 0.8670                                                                                                    \n",
      "epoch: 0057, loss: 0.1201 - val_loss: 0.4532; auc: 0.9949 - val_auc: 0.8661                                                                                                    \n",
      "epoch: 0058, loss: 0.1150 - val_loss: 0.4912; auc: 0.9951 - val_auc: 0.8616                                                                                                    \n",
      "epoch: 0059, loss: 0.1155 - val_loss: 0.5167; auc: 0.9953 - val_auc: 0.8674                                                                                                    \n",
      "epoch: 0060, loss: 0.1084 - val_loss: 0.4803; auc: 0.9953 - val_auc: 0.8634                                                                                                    \n",
      "epoch: 0061, loss: 0.1025 - val_loss: 0.4470; auc: 0.9949 - val_auc: 0.8692                                                                                                    \n",
      "epoch: 0062, loss: 0.1067 - val_loss: 0.4271; auc: 0.9946 - val_auc: 0.8737                                                                                                    \n",
      "epoch: 0063, loss: 0.1055 - val_loss: 0.4617; auc: 0.9949 - val_auc: 0.8763                                                                                                    \n",
      "epoch: 0064, loss: 0.0995 - val_loss: 0.7494; auc: 0.9960 - val_auc: 0.8442                                                                                                    \n",
      "epoch: 0065, loss: 0.1111 - val_loss: 0.7925; auc: 0.9964 - val_auc: 0.8375                                                                                                    \n",
      "epoch: 0066, loss: 0.0951 - val_loss: 0.5221; auc: 0.9947 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0067, loss: 0.1029 - val_loss: 0.4251; auc: 0.9948 - val_auc: 0.8781                                                                                                    \n",
      "epoch: 0068, loss: 0.1026 - val_loss: 0.4264; auc: 0.9949 - val_auc: 0.8848                                                                                                    \n",
      "epoch: 0069, loss: 0.0855 - val_loss: 0.5870; auc: 0.9955 - val_auc: 0.8598                                                                                                    \n",
      "epoch: 0070, loss: 0.0871 - val_loss: 0.4849; auc: 0.9957 - val_auc: 0.8701                                                                                                    \n",
      "epoch: 0071, loss: 0.0823 - val_loss: 0.6424; auc: 0.9960 - val_auc: 0.8580                                                                                                    \n",
      "epoch: 0072, loss: 0.0763 - val_loss: 0.5998; auc: 0.9962 - val_auc: 0.8589                                                                                                    \n",
      "epoch: 0073, loss: 0.0738 - val_loss: 0.7018; auc: 0.9969 - val_auc: 0.8290                                                                                                    \n",
      "epoch: 0074, loss: 0.0705 - val_loss: 0.4785; auc: 0.9960 - val_auc: 0.8714                                                                                                    \n",
      "epoch: 0075, loss: 0.0832 - val_loss: 0.4846; auc: 0.9958 - val_auc: 0.8705                                                                                                    \n",
      "epoch: 0076, loss: 0.0788 - val_loss: 0.6021; auc: 0.9963 - val_auc: 0.8607                                                                                                    \n",
      "epoch: 0077, loss: 0.0654 - val_loss: 0.6351; auc: 0.9964 - val_auc: 0.8603                                                                                                    \n",
      "epoch: 0078, loss: 0.0626 - val_loss: 0.8225; auc: 0.9968 - val_auc: 0.8366                                                                                                    \n",
      "epoch: 0079, loss: 0.0656 - val_loss: 0.5506; auc: 0.9966 - val_auc: 0.8710                                                                                                    \n",
      "epoch: 0080, loss: 0.0662 - val_loss: 0.5785; auc: 0.9962 - val_auc: 0.8531                                                                                                    \n",
      "epoch: 0081, loss: 0.0636 - val_loss: 0.6734; auc: 0.9966 - val_auc: 0.8411                                                                                                    \n",
      "epoch: 0082, loss: 0.0623 - val_loss: 0.6000; auc: 0.9967 - val_auc: 0.8585                                                                                                    \n",
      "epoch: 0083, loss: 0.0871 - val_loss: 0.6910; auc: 0.9970 - val_auc: 0.8460                                                                                                    \n",
      "epoch: 0084, loss: 0.0583 - val_loss: 0.7150; auc: 0.9967 - val_auc: 0.8621                                                                                                    \n",
      "epoch: 0085, loss: 0.0580 - val_loss: 0.6510; auc: 0.9967 - val_auc: 0.8634                                                                                                    \n",
      "epoch: 0086, loss: 0.0552 - val_loss: 0.6792; auc: 0.9968 - val_auc: 0.8496                                                                                                    \n",
      "epoch: 0087, loss: 0.0581 - val_loss: 0.7209; auc: 0.9967 - val_auc: 0.8504                                                                                                    \n",
      "epoch: 0088, loss: 0.0537 - val_loss: 0.7331; auc: 0.9969 - val_auc: 0.8558                                                                                                    \n",
      "epoch: 0089, loss: 0.0523 - val_loss: 0.7566; auc: 0.9972 - val_auc: 0.8478                                                                                                    \n",
      "epoch: 0090, loss: 0.0498 - val_loss: 1.0259; auc: 0.9974 - val_auc: 0.8317                                                                                                    \n",
      "epoch: 0091, loss: 0.0587 - val_loss: 0.8616; auc: 0.9971 - val_auc: 0.8549                                                                                                    \n",
      "epoch: 0092, loss: 0.0611 - val_loss: 0.9916; auc: 0.9974 - val_auc: 0.8277                                                                                                    \n",
      "epoch: 0093, loss: 0.0536 - val_loss: 0.8221; auc: 0.9974 - val_auc: 0.8438                                                                                                    \n",
      "epoch: 0094, loss: 0.0504 - val_loss: 0.6005; auc: 0.9967 - val_auc: 0.8674                                                                                                    \n",
      "epoch: 0095, loss: 0.0725 - val_loss: 0.7247; auc: 0.9961 - val_auc: 0.8393                                                                                                    \n",
      "epoch: 0096, loss: 0.0606 - val_loss: 0.7282; auc: 0.9968 - val_auc: 0.8500                                                                                                    \n",
      "epoch: 0097, loss: 0.0512 - val_loss: 0.6045; auc: 0.9966 - val_auc: 0.8732                                                                                                    \n",
      "epoch: 0098, loss: 0.0598 - val_loss: 0.7760; auc: 0.9966 - val_auc: 0.8518                                                                                                    \n",
      "epoch: 0099, loss: 0.0495 - val_loss: 0.7042; auc: 0.9966 - val_auc: 0.8598                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00099: early stopping\n",
      "1182 148 148\n",
      "Train on 1182 samples, validate on 148 samples\n",
      "Epoch 1/49\n",
      "1182/1182 [==============================] - 2s 2ms/sample - loss: 0.6919 - val_loss: 0.6466\n",
      "Epoch 2/49\n",
      "1182/1182 [==============================] - 0s 225us/sample - loss: 0.6704 - val_loss: 0.6147\n",
      "Epoch 3/49\n",
      "1182/1182 [==============================] - 0s 217us/sample - loss: 0.6504 - val_loss: 0.6082\n",
      "Epoch 4/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.6373 - val_loss: 0.5962\n",
      "Epoch 5/49\n",
      "1182/1182 [==============================] - 0s 228us/sample - loss: 0.6214 - val_loss: 0.5857\n",
      "Epoch 6/49\n",
      "1182/1182 [==============================] - 0s 228us/sample - loss: 0.6068 - val_loss: 0.5804\n",
      "Epoch 7/49\n",
      "1182/1182 [==============================] - 0s 222us/sample - loss: 0.5945 - val_loss: 0.5699\n",
      "Epoch 8/49\n",
      "1182/1182 [==============================] - 0s 225us/sample - loss: 0.5856 - val_loss: 0.5592\n",
      "Epoch 9/49\n",
      "1182/1182 [==============================] - 0s 218us/sample - loss: 0.5715 - val_loss: 0.5543\n",
      "Epoch 10/49\n",
      "1182/1182 [==============================] - 0s 219us/sample - loss: 0.5585 - val_loss: 0.5426\n",
      "Epoch 11/49\n",
      "1182/1182 [==============================] - 0s 239us/sample - loss: 0.5531 - val_loss: 0.5489\n",
      "Epoch 12/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.5361 - val_loss: 0.5326\n",
      "Epoch 13/49\n",
      "1182/1182 [==============================] - 0s 224us/sample - loss: 0.5304 - val_loss: 0.5473\n",
      "Epoch 14/49\n",
      "1182/1182 [==============================] - 0s 222us/sample - loss: 0.5219 - val_loss: 0.5214\n",
      "Epoch 15/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.5083 - val_loss: 0.5427\n",
      "Epoch 16/49\n",
      "1182/1182 [==============================] - 0s 229us/sample - loss: 0.5230 - val_loss: 0.5641\n",
      "Epoch 17/49\n",
      "1182/1182 [==============================] - 0s 225us/sample - loss: 0.5118 - val_loss: 0.5053\n",
      "Epoch 18/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.4855 - val_loss: 0.5012\n",
      "Epoch 19/49\n",
      "1182/1182 [==============================] - 0s 227us/sample - loss: 0.4752 - val_loss: 0.4948\n",
      "Epoch 20/49\n",
      "1182/1182 [==============================] - 0s 221us/sample - loss: 0.4661 - val_loss: 0.4940\n",
      "Epoch 21/49\n",
      "1182/1182 [==============================] - 0s 218us/sample - loss: 0.4537 - val_loss: 0.4881\n",
      "Epoch 22/49\n",
      "1182/1182 [==============================] - 0s 225us/sample - loss: 0.4529 - val_loss: 0.4966\n",
      "Epoch 23/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.4271 - val_loss: 0.4742\n",
      "Epoch 24/49\n",
      "1182/1182 [==============================] - 0s 229us/sample - loss: 0.4331 - val_loss: 0.4846\n",
      "Epoch 25/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.4115 - val_loss: 0.4699\n",
      "Epoch 26/49\n",
      "1182/1182 [==============================] - 0s 229us/sample - loss: 0.4008 - val_loss: 0.4575\n",
      "Epoch 27/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.3870 - val_loss: 0.4534\n",
      "Epoch 28/49\n",
      "1182/1182 [==============================] - 0s 227us/sample - loss: 0.3901 - val_loss: 0.4552\n",
      "Epoch 29/49\n",
      "1182/1182 [==============================] - 0s 224us/sample - loss: 0.3816 - val_loss: 0.4593\n",
      "Epoch 30/49\n",
      "1182/1182 [==============================] - 0s 220us/sample - loss: 0.3732 - val_loss: 0.4570\n",
      "Epoch 31/49\n",
      "1182/1182 [==============================] - 0s 229us/sample - loss: 0.3603 - val_loss: 0.4507\n",
      "Epoch 32/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.3405 - val_loss: 0.4354\n",
      "Epoch 33/49\n",
      "1182/1182 [==============================] - 0s 229us/sample - loss: 0.3248 - val_loss: 0.4390\n",
      "Epoch 34/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.3096 - val_loss: 0.4534\n",
      "Epoch 35/49\n",
      "1182/1182 [==============================] - 0s 233us/sample - loss: 0.3031 - val_loss: 0.4540\n",
      "Epoch 36/49\n",
      "1182/1182 [==============================] - 0s 211us/sample - loss: 0.2936 - val_loss: 0.4456\n",
      "Epoch 37/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.3052 - val_loss: 0.5192\n",
      "Epoch 38/49\n",
      "1182/1182 [==============================] - 0s 219us/sample - loss: 0.2960 - val_loss: 0.4752\n",
      "Epoch 39/49\n",
      "1182/1182 [==============================] - 0s 235us/sample - loss: 0.2625 - val_loss: 0.4928\n",
      "Epoch 40/49\n",
      "1182/1182 [==============================] - 0s 232us/sample - loss: 0.2635 - val_loss: 0.4939\n",
      "Epoch 41/49\n",
      "1182/1182 [==============================] - 0s 218us/sample - loss: 0.2686 - val_loss: 0.5155\n",
      "Epoch 42/49\n",
      "1182/1182 [==============================] - 0s 219us/sample - loss: 0.2427 - val_loss: 0.4463\n",
      "Epoch 43/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.2292 - val_loss: 0.4820\n",
      "Epoch 44/49\n",
      "1182/1182 [==============================] - 0s 219us/sample - loss: 0.2182 - val_loss: 0.4176\n",
      "Epoch 45/49\n",
      "1182/1182 [==============================] - 0s 224us/sample - loss: 0.2922 - val_loss: 0.4518\n",
      "Epoch 46/49\n",
      "1182/1182 [==============================] - 0s 226us/sample - loss: 0.2242 - val_loss: 0.4367\n",
      "Epoch 47/49\n",
      "1182/1182 [==============================] - 0s 223us/sample - loss: 0.2135 - val_loss: 0.4270\n",
      "Epoch 48/49\n",
      "1182/1182 [==============================] - 0s 219us/sample - loss: 0.1962 - val_loss: 0.4837\n",
      "Epoch 49/49\n",
      "1182/1182 [==============================] - 0s 219us/sample - loss: 0.1823 - val_loss: 0.5712\n",
      "1182 148 148\n",
      "Train on 1182 samples, validate on 148 samples\n",
      "Epoch 1/49\n",
      "1182/1182 [==============================] - 2s 2ms/sample - loss: 0.6853 - val_loss: 0.5704\n",
      "Epoch 2/49\n",
      "1182/1182 [==============================] - 0s 234us/sample - loss: 0.6628 - val_loss: 0.5469\n",
      "Epoch 3/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.6451 - val_loss: 0.5487\n",
      "Epoch 4/49\n",
      "1182/1182 [==============================] - 0s 241us/sample - loss: 0.6286 - val_loss: 0.5492\n",
      "Epoch 5/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.6149 - val_loss: 0.5336\n",
      "Epoch 6/49\n",
      "1182/1182 [==============================] - 0s 235us/sample - loss: 0.5988 - val_loss: 0.5368\n",
      "Epoch 7/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.5869 - val_loss: 0.5146\n",
      "Epoch 8/49\n",
      "1182/1182 [==============================] - 0s 238us/sample - loss: 0.5659 - val_loss: 0.5206\n",
      "Epoch 9/49\n",
      "1182/1182 [==============================] - 0s 237us/sample - loss: 0.5583 - val_loss: 0.5273\n",
      "Epoch 10/49\n",
      "1182/1182 [==============================] - 0s 243us/sample - loss: 0.5448 - val_loss: 0.5207\n",
      "Epoch 11/49\n",
      "1182/1182 [==============================] - 0s 237us/sample - loss: 0.5428 - val_loss: 0.5061\n",
      "Epoch 12/49\n",
      "1182/1182 [==============================] - 0s 227us/sample - loss: 0.5211 - val_loss: 0.5211\n",
      "Epoch 13/49\n",
      "1182/1182 [==============================] - 0s 227us/sample - loss: 0.5254 - val_loss: 0.5573\n",
      "Epoch 14/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.5132 - val_loss: 0.5107\n",
      "Epoch 15/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.4947 - val_loss: 0.5160\n",
      "Epoch 16/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.4870 - val_loss: 0.5118\n",
      "Epoch 17/49\n",
      "1182/1182 [==============================] - 0s 231us/sample - loss: 0.4813 - val_loss: 0.5309\n",
      "Epoch 18/49\n",
      "1182/1182 [==============================] - 0s 237us/sample - loss: 0.4685 - val_loss: 0.5237\n",
      "Epoch 19/49\n",
      "1182/1182 [==============================] - 0s 238us/sample - loss: 0.4731 - val_loss: 0.5153\n",
      "Epoch 20/49\n",
      "1182/1182 [==============================] - 0s 237us/sample - loss: 0.4525 - val_loss: 0.5161\n",
      "Epoch 21/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.4405 - val_loss: 0.5270\n",
      "Epoch 22/49\n",
      "1182/1182 [==============================] - 0s 239us/sample - loss: 0.4333 - val_loss: 0.5384\n",
      "Epoch 23/49\n",
      "1182/1182 [==============================] - 0s 234us/sample - loss: 0.4269 - val_loss: 0.5210\n",
      "Epoch 24/49\n",
      "1182/1182 [==============================] - 0s 243us/sample - loss: 0.4200 - val_loss: 0.5758\n",
      "Epoch 25/49\n",
      "1182/1182 [==============================] - 0s 235us/sample - loss: 0.4750 - val_loss: 0.5392\n",
      "Epoch 26/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.4316 - val_loss: 0.5626\n",
      "Epoch 27/49\n",
      "1182/1182 [==============================] - 0s 232us/sample - loss: 0.4239 - val_loss: 0.5284\n",
      "Epoch 28/49\n",
      "1182/1182 [==============================] - 0s 230us/sample - loss: 0.3849 - val_loss: 0.5212\n",
      "Epoch 29/49\n",
      "1182/1182 [==============================] - 0s 239us/sample - loss: 0.3857 - val_loss: 0.5500\n",
      "Epoch 30/49\n",
      "1182/1182 [==============================] - 0s 236us/sample - loss: 0.3743 - val_loss: 0.5210\n",
      "Epoch 31/49\n",
      "1182/1182 [==============================] - 0s 238us/sample - loss: 0.3588 - val_loss: 0.5294\n",
      "Epoch 32/49\n",
      "1182/1182 [==============================] - 0s 243us/sample - loss: 0.3552 - val_loss: 0.5574\n",
      "Epoch 33/49\n",
      "1182/1182 [==============================] - 0s 244us/sample - loss: 0.3535 - val_loss: 0.5487\n",
      "Epoch 34/49\n",
      "1182/1182 [==============================] - 0s 242us/sample - loss: 0.3773 - val_loss: 0.5296\n",
      "Epoch 35/49\n",
      "1182/1182 [==============================] - 0s 240us/sample - loss: 0.3368 - val_loss: 0.5342\n",
      "Epoch 36/49\n",
      "1182/1182 [==============================] - 0s 237us/sample - loss: 0.3253 - val_loss: 0.5326\n",
      "Epoch 37/49\n",
      "1182/1182 [==============================] - 0s 242us/sample - loss: 0.3121 - val_loss: 0.5351\n",
      "Epoch 38/49\n",
      "1182/1182 [==============================] - 0s 253us/sample - loss: 0.2933 - val_loss: 0.5430\n",
      "Epoch 39/49\n",
      "1182/1182 [==============================] - 0s 242us/sample - loss: 0.2860 - val_loss: 0.5663\n",
      "Epoch 40/49\n",
      "1182/1182 [==============================] - 0s 240us/sample - loss: 0.2784 - val_loss: 0.5246\n",
      "Epoch 41/49\n",
      "1182/1182 [==============================] - 0s 235us/sample - loss: 0.2612 - val_loss: 0.5298\n",
      "Epoch 42/49\n",
      "1182/1182 [==============================] - 0s 252us/sample - loss: 0.2549 - val_loss: 0.5274\n",
      "Epoch 43/49\n",
      "1182/1182 [==============================] - 0s 247us/sample - loss: 0.2527 - val_loss: 0.5364\n",
      "Epoch 44/49\n",
      "1182/1182 [==============================] - 0s 248us/sample - loss: 0.2522 - val_loss: 0.5350\n",
      "Epoch 45/49\n",
      "1182/1182 [==============================] - 0s 251us/sample - loss: 0.2589 - val_loss: 0.5560\n",
      "Epoch 46/49\n",
      "1182/1182 [==============================] - 0s 247us/sample - loss: 0.2493 - val_loss: 0.5408\n",
      "Epoch 47/49\n",
      "1182/1182 [==============================] - 0s 253us/sample - loss: 0.2258 - val_loss: 0.5830\n",
      "Epoch 48/49\n",
      "1182/1182 [==============================] - 0s 243us/sample - loss: 0.2246 - val_loss: 0.5794\n",
      "Epoch 49/49\n",
      "1182/1182 [==============================] - 0s 249us/sample - loss: 0.2186 - val_loss: 0.5918\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "        performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7fa47d19e8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['auc', 'val_auc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887927476610375"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07838638656617357"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ClinTox</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.917765</td>\n",
       "      <td>ROC</td>\n",
       "      <td>717602</td>\n",
       "      <td>48</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ClinTox</td>\n",
       "      <td>0.989523</td>\n",
       "      <td>0.877255</td>\n",
       "      <td>0.947014</td>\n",
       "      <td>ROC</td>\n",
       "      <td>717602</td>\n",
       "      <td>48</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ClinTox</td>\n",
       "      <td>0.986227</td>\n",
       "      <td>0.717579</td>\n",
       "      <td>0.799004</td>\n",
       "      <td>ROC</td>\n",
       "      <td>717602</td>\n",
       "      <td>48</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0   ClinTox   0.989433   0.878571  0.917765    ROC              717602   \n",
       "1   ClinTox   0.989523   0.877255  0.947014    ROC              717602   \n",
       "2   ClinTox   0.986227   0.717579  0.799004    ROC              717602   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0          48         128  0.0001             0  \n",
       "1          48         128  0.0001             0  \n",
       "2          48         128  0.0001             0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
