{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [19:37:32] Enabling RDKit 2019.09.2 jupyter extensions\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: ESOL number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'ESOL'\n",
    "\n",
    "from chembench import load_data\n",
    "\n",
    "df, induces = load_data(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').values\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256, 128, 32]\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "loss = 'mse'\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = 'linear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902 113 113\n",
      "WARNING:tensorflow:From /home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "epoch: 0001, loss: 12.7995 - val_loss: 9.8648; rmse: 3.3294 - rmse_val: 3.1408;  r2: 0.0765 - r2_val: 0.0692                                                                                                    \n",
      "epoch: 0002, loss: 9.5679 - val_loss: 5.8446; rmse: 2.6022 - rmse_val: 2.4176;  r2: 0.0950 - r2_val: 0.0980                                                                                                    \n",
      "epoch: 0003, loss: 5.2939 - val_loss: 3.8008; rmse: 2.0448 - rmse_val: 1.9496;  r2: 0.1045 - r2_val: 0.1122                                                                                                    \n",
      "epoch: 0004, loss: 4.3866 - val_loss: 3.8655; rmse: 2.0495 - rmse_val: 1.9661;  r2: 0.1265 - r2_val: 0.1409                                                                                                    \n",
      "epoch: 0005, loss: 4.0359 - val_loss: 3.4995; rmse: 2.0170 - rmse_val: 1.8707;  r2: 0.1574 - r2_val: 0.1833                                                                                                    \n",
      "epoch: 0006, loss: 4.0469 - val_loss: 3.3202; rmse: 1.9596 - rmse_val: 1.8221;  r2: 0.1793 - r2_val: 0.2151                                                                                                    \n",
      "epoch: 0007, loss: 3.8017 - val_loss: 3.3880; rmse: 1.9451 - rmse_val: 1.8406;  r2: 0.2030 - r2_val: 0.2455                                                                                                    \n",
      "epoch: 0008, loss: 3.6847 - val_loss: 3.0611; rmse: 1.8962 - rmse_val: 1.7496;  r2: 0.2400 - r2_val: 0.2911                                                                                                    \n",
      "epoch: 0009, loss: 3.6358 - val_loss: 2.9121; rmse: 1.8598 - rmse_val: 1.7065;  r2: 0.2733 - r2_val: 0.3346                                                                                                    \n",
      "epoch: 0010, loss: 3.4169 - val_loss: 2.8713; rmse: 1.8262 - rmse_val: 1.6945;  r2: 0.2963 - r2_val: 0.3649                                                                                                    \n",
      "epoch: 0011, loss: 3.2792 - val_loss: 2.6335; rmse: 1.7965 - rmse_val: 1.6228;  r2: 0.3457 - r2_val: 0.4202                                                                                                    \n",
      "epoch: 0012, loss: 3.1397 - val_loss: 2.5363; rmse: 1.7421 - rmse_val: 1.5926;  r2: 0.3666 - r2_val: 0.4432                                                                                                    \n",
      "epoch: 0013, loss: 2.9668 - val_loss: 2.3134; rmse: 1.7041 - rmse_val: 1.5210;  r2: 0.4063 - r2_val: 0.4841                                                                                                    \n",
      "epoch: 0014, loss: 2.8819 - val_loss: 2.2420; rmse: 1.6554 - rmse_val: 1.4973;  r2: 0.4266 - r2_val: 0.5032                                                                                                    \n",
      "epoch: 0015, loss: 2.6881 - val_loss: 2.0623; rmse: 1.6039 - rmse_val: 1.4361;  r2: 0.4640 - r2_val: 0.5377                                                                                                    \n",
      "epoch: 0016, loss: 2.5235 - val_loss: 1.9158; rmse: 1.5541 - rmse_val: 1.3841;  r2: 0.4906 - r2_val: 0.5583                                                                                                    \n",
      "epoch: 0017, loss: 2.3610 - val_loss: 1.7721; rmse: 1.5041 - rmse_val: 1.3312;  r2: 0.5218 - r2_val: 0.5827                                                                                                    \n",
      "epoch: 0018, loss: 2.2237 - val_loss: 1.9038; rmse: 1.4843 - rmse_val: 1.3798;  r2: 0.5398 - r2_val: 0.5945                                                                                                    \n",
      "epoch: 0019, loss: 2.0869 - val_loss: 1.5400; rmse: 1.4309 - rmse_val: 1.2410;  r2: 0.5854 - r2_val: 0.6273                                                                                                    \n",
      "epoch: 0020, loss: 1.9705 - val_loss: 1.4368; rmse: 1.3631 - rmse_val: 1.1987;  r2: 0.6156 - r2_val: 0.6464                                                                                                    \n",
      "epoch: 0021, loss: 1.9129 - val_loss: 1.7058; rmse: 1.3490 - rmse_val: 1.3061;  r2: 0.6272 - r2_val: 0.6528                                                                                                    \n",
      "epoch: 0022, loss: 1.7268 - val_loss: 1.3395; rmse: 1.2750 - rmse_val: 1.1574;  r2: 0.6541 - r2_val: 0.6674                                                                                                    \n",
      "epoch: 0023, loss: 1.5860 - val_loss: 1.2825; rmse: 1.2320 - rmse_val: 1.1325;  r2: 0.6773 - r2_val: 0.6853                                                                                                    \n",
      "epoch: 0024, loss: 1.5259 - val_loss: 1.5230; rmse: 1.2414 - rmse_val: 1.2341;  r2: 0.6903 - r2_val: 0.6986                                                                                                    \n",
      "epoch: 0025, loss: 1.4129 - val_loss: 1.1467; rmse: 1.1744 - rmse_val: 1.0708;  r2: 0.7142 - r2_val: 0.7142                                                                                                    \n",
      "epoch: 0026, loss: 1.3530 - val_loss: 1.0788; rmse: 1.1393 - rmse_val: 1.0386;  r2: 0.7308 - r2_val: 0.7320                                                                                                    \n",
      "epoch: 0027, loss: 1.2651 - val_loss: 1.0390; rmse: 1.1006 - rmse_val: 1.0193;  r2: 0.7440 - r2_val: 0.7431                                                                                                    \n",
      "epoch: 0028, loss: 1.1658 - val_loss: 1.4100; rmse: 1.1437 - rmse_val: 1.1874;  r2: 0.7514 - r2_val: 0.7509                                                                                                    \n",
      "epoch: 0029, loss: 1.2064 - val_loss: 1.0283; rmse: 1.0331 - rmse_val: 1.0141;  r2: 0.7680 - r2_val: 0.7631                                                                                                    \n",
      "epoch: 0030, loss: 1.0702 - val_loss: 0.9093; rmse: 1.0156 - rmse_val: 0.9536;  r2: 0.7816 - r2_val: 0.7748                                                                                                    \n",
      "epoch: 0031, loss: 1.0436 - val_loss: 1.2023; rmse: 1.0458 - rmse_val: 1.0965;  r2: 0.7882 - r2_val: 0.7856                                                                                                    \n",
      "epoch: 0032, loss: 1.0690 - val_loss: 0.9417; rmse: 0.9622 - rmse_val: 0.9704;  r2: 0.7978 - r2_val: 0.7923                                                                                                    \n",
      "epoch: 0033, loss: 0.9156 - val_loss: 1.0964; rmse: 0.9872 - rmse_val: 1.0471;  r2: 0.8055 - r2_val: 0.7987                                                                                                    \n",
      "epoch: 0034, loss: 0.9967 - val_loss: 0.9933; rmse: 0.9387 - rmse_val: 0.9966;  r2: 0.8158 - r2_val: 0.8042                                                                                                    \n",
      "epoch: 0035, loss: 1.0266 - val_loss: 0.8160; rmse: 0.8963 - rmse_val: 0.9033;  r2: 0.8254 - r2_val: 0.8095                                                                                                    \n",
      "epoch: 0036, loss: 0.8793 - val_loss: 0.8350; rmse: 1.0126 - rmse_val: 0.9138;  r2: 0.8352 - r2_val: 0.8140                                                                                                    \n",
      "epoch: 0037, loss: 0.8969 - val_loss: 0.7171; rmse: 0.8929 - rmse_val: 0.8468;  r2: 0.8430 - r2_val: 0.8222                                                                                                    \n",
      "epoch: 0038, loss: 0.7989 - val_loss: 0.7060; rmse: 0.8408 - rmse_val: 0.8403;  r2: 0.8474 - r2_val: 0.8285                                                                                                    \n",
      "epoch: 0039, loss: 0.7515 - val_loss: 0.9439; rmse: 0.8855 - rmse_val: 0.9716;  r2: 0.8503 - r2_val: 0.8312                                                                                                    \n",
      "epoch: 0040, loss: 0.7357 - val_loss: 0.6772; rmse: 0.8066 - rmse_val: 0.8229;  r2: 0.8623 - r2_val: 0.8382                                                                                                    \n",
      "epoch: 0041, loss: 0.6895 - val_loss: 0.6912; rmse: 0.7982 - rmse_val: 0.8314;  r2: 0.8682 - r2_val: 0.8459                                                                                                    \n",
      "epoch: 0042, loss: 0.6318 - val_loss: 0.6199; rmse: 0.7775 - rmse_val: 0.7873;  r2: 0.8705 - r2_val: 0.8495                                                                                                    \n",
      "epoch: 0043, loss: 0.6106 - val_loss: 0.6056; rmse: 0.7585 - rmse_val: 0.7782;  r2: 0.8754 - r2_val: 0.8532                                                                                                    \n",
      "epoch: 0044, loss: 0.5740 - val_loss: 0.6307; rmse: 0.7426 - rmse_val: 0.7942;  r2: 0.8809 - r2_val: 0.8581                                                                                                    \n",
      "epoch: 0045, loss: 0.5462 - val_loss: 0.5967; rmse: 0.7226 - rmse_val: 0.7725;  r2: 0.8864 - r2_val: 0.8633                                                                                                    \n",
      "epoch: 0046, loss: 0.5193 - val_loss: 0.7334; rmse: 0.7548 - rmse_val: 0.8564;  r2: 0.8874 - r2_val: 0.8654                                                                                                    \n",
      "epoch: 0047, loss: 0.5410 - val_loss: 1.1307; rmse: 0.9199 - rmse_val: 1.0633;  r2: 0.8878 - r2_val: 0.8660                                                                                                    \n",
      "epoch: 0048, loss: 0.6781 - val_loss: 1.2127; rmse: 0.9614 - rmse_val: 1.1012;  r2: 0.8910 - r2_val: 0.8698                                                                                                    \n",
      "epoch: 0049, loss: 0.7387 - val_loss: 0.6476; rmse: 0.7196 - rmse_val: 0.8047;  r2: 0.8985 - r2_val: 0.8759                                                                                                    \n",
      "epoch: 0050, loss: 0.5805 - val_loss: 0.4926; rmse: 0.6750 - rmse_val: 0.7018;  r2: 0.9048 - r2_val: 0.8798                                                                                                    \n",
      "epoch: 0051, loss: 0.4935 - val_loss: 0.4836; rmse: 0.7040 - rmse_val: 0.6954;  r2: 0.9100 - r2_val: 0.8820                                                                                                    \n",
      "epoch: 0052, loss: 0.4368 - val_loss: 0.4664; rmse: 0.6661 - rmse_val: 0.6829;  r2: 0.9125 - r2_val: 0.8837                                                                                                    \n",
      "epoch: 0053, loss: 0.4317 - val_loss: 0.4753; rmse: 0.6824 - rmse_val: 0.6894;  r2: 0.9150 - r2_val: 0.8844                                                                                                    \n",
      "epoch: 0054, loss: 0.4831 - val_loss: 0.4543; rmse: 0.6263 - rmse_val: 0.6741;  r2: 0.9170 - r2_val: 0.8876                                                                                                    \n",
      "epoch: 0055, loss: 0.3973 - val_loss: 0.5348; rmse: 0.6255 - rmse_val: 0.7313;  r2: 0.9197 - r2_val: 0.8887                                                                                                    \n",
      "epoch: 0056, loss: 0.3757 - val_loss: 0.5817; rmse: 0.6392 - rmse_val: 0.7627;  r2: 0.9212 - r2_val: 0.8907                                                                                                    \n",
      "epoch: 0057, loss: 0.3740 - val_loss: 0.7168; rmse: 0.6986 - rmse_val: 0.8466;  r2: 0.9228 - r2_val: 0.8914                                                                                                    \n",
      "epoch: 0058, loss: 0.4127 - val_loss: 0.5056; rmse: 0.5876 - rmse_val: 0.7111;  r2: 0.9269 - r2_val: 0.8932                                                                                                    \n",
      "epoch: 0059, loss: 0.3424 - val_loss: 0.4273; rmse: 0.6055 - rmse_val: 0.6537;  r2: 0.9311 - r2_val: 0.8943                                                                                                    \n",
      "epoch: 0060, loss: 0.4027 - val_loss: 0.4588; rmse: 0.6601 - rmse_val: 0.6773;  r2: 0.9337 - r2_val: 0.8976                                                                                                    \n",
      "epoch: 0061, loss: 0.3561 - val_loss: 0.4191; rmse: 0.6039 - rmse_val: 0.6474;  r2: 0.9350 - r2_val: 0.8987                                                                                                    \n",
      "epoch: 0062, loss: 0.3256 - val_loss: 0.4040; rmse: 0.5532 - rmse_val: 0.6356;  r2: 0.9378 - r2_val: 0.8993                                                                                                    \n",
      "epoch: 0063, loss: 0.3002 - val_loss: 0.4128; rmse: 0.5716 - rmse_val: 0.6425;  r2: 0.9401 - r2_val: 0.8996                                                                                                    \n",
      "epoch: 0064, loss: 0.2978 - val_loss: 0.4510; rmse: 0.5254 - rmse_val: 0.6716;  r2: 0.9398 - r2_val: 0.8994                                                                                                    \n",
      "epoch: 0065, loss: 0.3052 - val_loss: 0.7278; rmse: 0.6612 - rmse_val: 0.8531;  r2: 0.9390 - r2_val: 0.8980                                                                                                    \n",
      "epoch: 0066, loss: 0.3833 - val_loss: 0.5945; rmse: 0.5941 - rmse_val: 0.7710;  r2: 0.9440 - r2_val: 0.9004                                                                                                    \n",
      "epoch: 0067, loss: 0.3429 - val_loss: 0.4140; rmse: 0.5097 - rmse_val: 0.6434;  r2: 0.9459 - r2_val: 0.9006                                                                                                    \n",
      "epoch: 0068, loss: 0.3024 - val_loss: 0.4011; rmse: 0.5350 - rmse_val: 0.6333;  r2: 0.9470 - r2_val: 0.9027                                                                                                    \n",
      "epoch: 0069, loss: 0.2884 - val_loss: 0.3903; rmse: 0.4996 - rmse_val: 0.6248;  r2: 0.9481 - r2_val: 0.9037                                                                                                    \n",
      "epoch: 0070, loss: 0.2531 - val_loss: 0.3873; rmse: 0.4953 - rmse_val: 0.6223;  r2: 0.9495 - r2_val: 0.9046                                                                                                    \n",
      "epoch: 0071, loss: 0.2400 - val_loss: 0.3861; rmse: 0.4723 - rmse_val: 0.6214;  r2: 0.9515 - r2_val: 0.9062                                                                                                    \n",
      "epoch: 0072, loss: 0.2420 - val_loss: 0.4733; rmse: 0.5078 - rmse_val: 0.6879;  r2: 0.9518 - r2_val: 0.9083                                                                                                    \n",
      "epoch: 0073, loss: 0.2502 - val_loss: 0.4663; rmse: 0.4972 - rmse_val: 0.6829;  r2: 0.9525 - r2_val: 0.9102                                                                                                    \n",
      "epoch: 0074, loss: 0.2266 - val_loss: 0.4075; rmse: 0.4586 - rmse_val: 0.6384;  r2: 0.9551 - r2_val: 0.9102                                                                                                    \n",
      "epoch: 0075, loss: 0.2138 - val_loss: 0.3686; rmse: 0.4500 - rmse_val: 0.6071;  r2: 0.9568 - r2_val: 0.9091                                                                                                    \n",
      "epoch: 0076, loss: 0.2010 - val_loss: 0.3645; rmse: 0.4587 - rmse_val: 0.6037;  r2: 0.9585 - r2_val: 0.9090                                                                                                    \n",
      "epoch: 0077, loss: 0.2112 - val_loss: 0.3819; rmse: 0.5022 - rmse_val: 0.6180;  r2: 0.9589 - r2_val: 0.9099                                                                                                    \n",
      "epoch: 0078, loss: 0.2214 - val_loss: 0.4402; rmse: 0.5779 - rmse_val: 0.6635;  r2: 0.9595 - r2_val: 0.9104                                                                                                    \n",
      "epoch: 0079, loss: 0.2370 - val_loss: 0.4925; rmse: 0.4923 - rmse_val: 0.7018;  r2: 0.9610 - r2_val: 0.9111                                                                                                    \n",
      "epoch: 0080, loss: 0.2664 - val_loss: 0.8124; rmse: 0.7017 - rmse_val: 0.9013;  r2: 0.9572 - r2_val: 0.9060                                                                                                    \n",
      "epoch: 0081, loss: 0.2902 - val_loss: 0.4757; rmse: 0.4670 - rmse_val: 0.6897;  r2: 0.9605 - r2_val: 0.9084                                                                                                    \n",
      "epoch: 0082, loss: 0.2321 - val_loss: 0.3741; rmse: 0.4378 - rmse_val: 0.6117;  r2: 0.9610 - r2_val: 0.9091                                                                                                    \n",
      "epoch: 0083, loss: 0.1961 - val_loss: 0.3621; rmse: 0.4105 - rmse_val: 0.6017;  r2: 0.9636 - r2_val: 0.9119                                                                                                    \n",
      "epoch: 0084, loss: 0.1792 - val_loss: 0.3813; rmse: 0.4080 - rmse_val: 0.6175;  r2: 0.9643 - r2_val: 0.9116                                                                                                    \n",
      "epoch: 0085, loss: 0.1665 - val_loss: 0.4061; rmse: 0.4143 - rmse_val: 0.6373;  r2: 0.9650 - r2_val: 0.9112                                                                                                    \n",
      "epoch: 0086, loss: 0.1641 - val_loss: 0.3885; rmse: 0.3972 - rmse_val: 0.6233;  r2: 0.9660 - r2_val: 0.9117                                                                                                    \n",
      "epoch: 0087, loss: 0.1548 - val_loss: 0.3559; rmse: 0.4210 - rmse_val: 0.5966;  r2: 0.9678 - r2_val: 0.9126                                                                                                    \n",
      "epoch: 0088, loss: 0.1693 - val_loss: 0.3461; rmse: 0.3907 - rmse_val: 0.5883;  r2: 0.9673 - r2_val: 0.9145                                                                                                    \n",
      "epoch: 0089, loss: 0.1592 - val_loss: 0.3968; rmse: 0.4031 - rmse_val: 0.6299;  r2: 0.9669 - r2_val: 0.9158                                                                                                    \n",
      "epoch: 0090, loss: 0.1520 - val_loss: 0.3463; rmse: 0.3728 - rmse_val: 0.5885;  r2: 0.9695 - r2_val: 0.9162                                                                                                    \n",
      "epoch: 0091, loss: 0.1386 - val_loss: 0.3785; rmse: 0.3850 - rmse_val: 0.6152;  r2: 0.9700 - r2_val: 0.9153                                                                                                    \n",
      "epoch: 0092, loss: 0.1433 - val_loss: 0.3415; rmse: 0.3625 - rmse_val: 0.5844;  r2: 0.9715 - r2_val: 0.9161                                                                                                    \n",
      "epoch: 0093, loss: 0.1335 - val_loss: 0.3382; rmse: 0.3688 - rmse_val: 0.5815;  r2: 0.9718 - r2_val: 0.9166                                                                                                    \n",
      "epoch: 0094, loss: 0.1339 - val_loss: 0.3433; rmse: 0.3539 - rmse_val: 0.5859;  r2: 0.9729 - r2_val: 0.9157                                                                                                    \n",
      "epoch: 0095, loss: 0.1450 - val_loss: 0.3489; rmse: 0.3483 - rmse_val: 0.5906;  r2: 0.9732 - r2_val: 0.9168                                                                                                    \n",
      "epoch: 0096, loss: 0.1335 - val_loss: 0.4526; rmse: 0.5513 - rmse_val: 0.6727;  r2: 0.9727 - r2_val: 0.9160                                                                                                    \n",
      "epoch: 0097, loss: 0.2214 - val_loss: 0.5483; rmse: 0.5081 - rmse_val: 0.7405;  r2: 0.9725 - r2_val: 0.9178                                                                                                    \n",
      "epoch: 0098, loss: 0.1941 - val_loss: 0.3524; rmse: 0.3474 - rmse_val: 0.5936;  r2: 0.9736 - r2_val: 0.9155                                                                                                    \n",
      "epoch: 0099, loss: 0.1564 - val_loss: 0.3380; rmse: 0.3657 - rmse_val: 0.5813;  r2: 0.9739 - r2_val: 0.9159                                                                                                    \n",
      "epoch: 0100, loss: 0.1316 - val_loss: 0.3643; rmse: 0.3466 - rmse_val: 0.6035;  r2: 0.9742 - r2_val: 0.9164                                                                                                    \n",
      "epoch: 0101, loss: 0.1281 - val_loss: 0.4069; rmse: 0.3834 - rmse_val: 0.6379;  r2: 0.9744 - r2_val: 0.9184                                                                                                    \n",
      "epoch: 0102, loss: 0.1290 - val_loss: 0.3327; rmse: 0.3282 - rmse_val: 0.5768;  r2: 0.9763 - r2_val: 0.9189                                                                                                    \n",
      "epoch: 0103, loss: 0.1132 - val_loss: 0.3340; rmse: 0.3283 - rmse_val: 0.5779;  r2: 0.9770 - r2_val: 0.9203                                                                                                    \n",
      "epoch: 0104, loss: 0.1176 - val_loss: 0.3375; rmse: 0.3293 - rmse_val: 0.5809;  r2: 0.9769 - r2_val: 0.9202                                                                                                    \n",
      "epoch: 0105, loss: 0.1107 - val_loss: 0.3475; rmse: 0.3242 - rmse_val: 0.5895;  r2: 0.9770 - r2_val: 0.9193                                                                                                    \n",
      "epoch: 0106, loss: 0.1047 - val_loss: 0.3406; rmse: 0.3494 - rmse_val: 0.5836;  r2: 0.9778 - r2_val: 0.9167                                                                                                    \n",
      "epoch: 0107, loss: 0.1157 - val_loss: 0.4378; rmse: 0.3953 - rmse_val: 0.6617;  r2: 0.9784 - r2_val: 0.9187                                                                                                    \n",
      "epoch: 0108, loss: 0.1401 - val_loss: 0.3446; rmse: 0.3518 - rmse_val: 0.5870;  r2: 0.9775 - r2_val: 0.9164                                                                                                    \n",
      "epoch: 0109, loss: 0.1446 - val_loss: 0.3818; rmse: 0.4249 - rmse_val: 0.6179;  r2: 0.9784 - r2_val: 0.9166                                                                                                    \n",
      "epoch: 0110, loss: 0.1222 - val_loss: 0.3313; rmse: 0.3090 - rmse_val: 0.5756;  r2: 0.9791 - r2_val: 0.9191                                                                                                    \n",
      "epoch: 0111, loss: 0.0967 - val_loss: 0.3222; rmse: 0.3084 - rmse_val: 0.5676;  r2: 0.9799 - r2_val: 0.9204                                                                                                    \n",
      "epoch: 0112, loss: 0.1009 - val_loss: 0.4295; rmse: 0.3937 - rmse_val: 0.6553;  r2: 0.9796 - r2_val: 0.9216                                                                                                    \n",
      "epoch: 0113, loss: 0.1150 - val_loss: 0.3197; rmse: 0.3098 - rmse_val: 0.5654;  r2: 0.9806 - r2_val: 0.9219                                                                                                    \n",
      "epoch: 0114, loss: 0.1132 - val_loss: 0.3291; rmse: 0.3146 - rmse_val: 0.5737;  r2: 0.9786 - r2_val: 0.9233                                                                                                    \n",
      "epoch: 0115, loss: 0.1044 - val_loss: 0.3391; rmse: 0.3116 - rmse_val: 0.5823;  r2: 0.9811 - r2_val: 0.9232                                                                                                    \n",
      "epoch: 0116, loss: 0.0958 - val_loss: 0.3322; rmse: 0.2930 - rmse_val: 0.5764;  r2: 0.9820 - r2_val: 0.9224                                                                                                    \n",
      "epoch: 0117, loss: 0.1032 - val_loss: 0.3972; rmse: 0.4493 - rmse_val: 0.6303;  r2: 0.9808 - r2_val: 0.9190                                                                                                    \n",
      "epoch: 0118, loss: 0.1590 - val_loss: 0.3161; rmse: 0.3000 - rmse_val: 0.5622;  r2: 0.9812 - r2_val: 0.9215                                                                                                    \n",
      "epoch: 0119, loss: 0.1012 - val_loss: 0.3328; rmse: 0.2935 - rmse_val: 0.5769;  r2: 0.9825 - r2_val: 0.9225                                                                                                    \n",
      "epoch: 0120, loss: 0.0871 - val_loss: 0.3145; rmse: 0.2926 - rmse_val: 0.5608;  r2: 0.9823 - r2_val: 0.9225                                                                                                    \n",
      "epoch: 0121, loss: 0.0805 - val_loss: 0.3111; rmse: 0.2774 - rmse_val: 0.5577;  r2: 0.9833 - r2_val: 0.9239                                                                                                    \n",
      "epoch: 0122, loss: 0.0781 - val_loss: 0.3317; rmse: 0.2890 - rmse_val: 0.5759;  r2: 0.9834 - r2_val: 0.9240                                                                                                    \n",
      "epoch: 0123, loss: 0.0803 - val_loss: 0.3071; rmse: 0.2708 - rmse_val: 0.5542;  r2: 0.9841 - r2_val: 0.9246                                                                                                    \n",
      "epoch: 0124, loss: 0.0759 - val_loss: 0.3057; rmse: 0.2844 - rmse_val: 0.5530;  r2: 0.9844 - r2_val: 0.9248                                                                                                    \n",
      "epoch: 0125, loss: 0.0755 - val_loss: 0.3230; rmse: 0.2676 - rmse_val: 0.5683;  r2: 0.9848 - r2_val: 0.9234                                                                                                    \n",
      "epoch: 0126, loss: 0.0768 - val_loss: 0.3364; rmse: 0.2738 - rmse_val: 0.5800;  r2: 0.9851 - r2_val: 0.9231                                                                                                    \n",
      "epoch: 0127, loss: 0.0748 - val_loss: 0.3112; rmse: 0.2723 - rmse_val: 0.5579;  r2: 0.9853 - r2_val: 0.9239                                                                                                    \n",
      "epoch: 0128, loss: 0.0737 - val_loss: 0.3247; rmse: 0.2612 - rmse_val: 0.5698;  r2: 0.9854 - r2_val: 0.9228                                                                                                    \n",
      "epoch: 0129, loss: 0.0687 - val_loss: 0.3221; rmse: 0.2585 - rmse_val: 0.5676;  r2: 0.9854 - r2_val: 0.9240                                                                                                    \n",
      "epoch: 0130, loss: 0.0677 - val_loss: 0.3485; rmse: 0.2808 - rmse_val: 0.5903;  r2: 0.9859 - r2_val: 0.9237                                                                                                    \n",
      "epoch: 0131, loss: 0.0735 - val_loss: 0.3201; rmse: 0.2525 - rmse_val: 0.5658;  r2: 0.9862 - r2_val: 0.9228                                                                                                    \n",
      "epoch: 0132, loss: 0.0647 - val_loss: 0.3118; rmse: 0.2507 - rmse_val: 0.5584;  r2: 0.9866 - r2_val: 0.9232                                                                                                    \n",
      "epoch: 0133, loss: 0.0661 - val_loss: 0.3126; rmse: 0.2433 - rmse_val: 0.5591;  r2: 0.9869 - r2_val: 0.9242                                                                                                    \n",
      "epoch: 0134, loss: 0.0602 - val_loss: 0.3091; rmse: 0.2587 - rmse_val: 0.5560;  r2: 0.9873 - r2_val: 0.9236                                                                                                    \n",
      "epoch: 0135, loss: 0.0634 - val_loss: 0.3399; rmse: 0.2645 - rmse_val: 0.5830;  r2: 0.9873 - r2_val: 0.9241                                                                                                    \n",
      "epoch: 0136, loss: 0.0621 - val_loss: 0.3353; rmse: 0.3329 - rmse_val: 0.5791;  r2: 0.9870 - r2_val: 0.9241                                                                                                    \n",
      "epoch: 0137, loss: 0.0789 - val_loss: 0.4462; rmse: 0.3826 - rmse_val: 0.6679;  r2: 0.9865 - r2_val: 0.9226                                                                                                    \n",
      "epoch: 0138, loss: 0.0899 - val_loss: 0.3105; rmse: 0.2520 - rmse_val: 0.5573;  r2: 0.9875 - r2_val: 0.9234                                                                                                    \n",
      "epoch: 0139, loss: 0.0650 - val_loss: 0.3065; rmse: 0.2548 - rmse_val: 0.5536;  r2: 0.9878 - r2_val: 0.9244                                                                                                    \n",
      "epoch: 0140, loss: 0.0639 - val_loss: 0.3060; rmse: 0.2316 - rmse_val: 0.5532;  r2: 0.9882 - r2_val: 0.9251                                                                                                    \n",
      "epoch: 0141, loss: 0.0568 - val_loss: 0.3056; rmse: 0.2352 - rmse_val: 0.5529;  r2: 0.9883 - r2_val: 0.9250                                                                                                    \n",
      "epoch: 0142, loss: 0.0566 - val_loss: 0.3068; rmse: 0.2526 - rmse_val: 0.5539;  r2: 0.9886 - r2_val: 0.9250                                                                                                    \n",
      "epoch: 0143, loss: 0.0600 - val_loss: 0.3010; rmse: 0.2339 - rmse_val: 0.5486;  r2: 0.9887 - r2_val: 0.9254                                                                                                    \n",
      "epoch: 0144, loss: 0.0608 - val_loss: 0.3475; rmse: 0.2657 - rmse_val: 0.5895;  r2: 0.9887 - r2_val: 0.9259                                                                                                    \n",
      "epoch: 0145, loss: 0.0598 - val_loss: 0.3097; rmse: 0.2574 - rmse_val: 0.5565;  r2: 0.9889 - r2_val: 0.9240                                                                                                    \n",
      "epoch: 0146, loss: 0.0640 - val_loss: 0.3291; rmse: 0.2423 - rmse_val: 0.5736;  r2: 0.9891 - r2_val: 0.9265                                                                                                    \n",
      "epoch: 0147, loss: 0.0574 - val_loss: 0.3061; rmse: 0.2210 - rmse_val: 0.5532;  r2: 0.9895 - r2_val: 0.9273                                                                                                    \n",
      "epoch: 0148, loss: 0.0551 - val_loss: 0.3463; rmse: 0.3593 - rmse_val: 0.5884;  r2: 0.9893 - r2_val: 0.9257                                                                                                    \n",
      "epoch: 0149, loss: 0.0897 - val_loss: 0.3617; rmse: 0.3014 - rmse_val: 0.6014;  r2: 0.9884 - r2_val: 0.9265                                                                                                    \n",
      "epoch: 0150, loss: 0.0719 - val_loss: 0.2975; rmse: 0.2177 - rmse_val: 0.5454;  r2: 0.9897 - r2_val: 0.9273                                                                                                    \n",
      "epoch: 0151, loss: 0.0499 - val_loss: 0.3224; rmse: 0.2382 - rmse_val: 0.5678;  r2: 0.9897 - r2_val: 0.9282                                                                                                    \n",
      "epoch: 0152, loss: 0.0516 - val_loss: 0.3377; rmse: 0.3400 - rmse_val: 0.5811;  r2: 0.9897 - r2_val: 0.9253                                                                                                    \n",
      "epoch: 0153, loss: 0.0793 - val_loss: 0.2965; rmse: 0.2191 - rmse_val: 0.5445;  r2: 0.9903 - r2_val: 0.9262                                                                                                    \n",
      "epoch: 0154, loss: 0.0507 - val_loss: 0.3276; rmse: 0.3267 - rmse_val: 0.5724;  r2: 0.9894 - r2_val: 0.9270                                                                                                    \n",
      "epoch: 0155, loss: 0.0868 - val_loss: 0.3586; rmse: 0.2976 - rmse_val: 0.5989;  r2: 0.9891 - r2_val: 0.9259                                                                                                    \n",
      "epoch: 0156, loss: 0.0615 - val_loss: 0.2999; rmse: 0.2125 - rmse_val: 0.5477;  r2: 0.9901 - r2_val: 0.9272                                                                                                    \n",
      "epoch: 0157, loss: 0.0475 - val_loss: 0.3210; rmse: 0.2286 - rmse_val: 0.5666;  r2: 0.9900 - r2_val: 0.9270                                                                                                    \n",
      "epoch: 0158, loss: 0.0492 - val_loss: 0.2959; rmse: 0.2224 - rmse_val: 0.5440;  r2: 0.9906 - r2_val: 0.9270                                                                                                    \n",
      "epoch: 0159, loss: 0.0459 - val_loss: 0.3617; rmse: 0.2814 - rmse_val: 0.6014;  r2: 0.9910 - r2_val: 0.9271                                                                                                    \n",
      "epoch: 0160, loss: 0.0613 - val_loss: 0.3249; rmse: 0.2795 - rmse_val: 0.5700;  r2: 0.9899 - r2_val: 0.9249                                                                                                    \n",
      "epoch: 0161, loss: 0.0584 - val_loss: 0.3285; rmse: 0.2337 - rmse_val: 0.5732;  r2: 0.9906 - r2_val: 0.9263                                                                                                    \n",
      "epoch: 0162, loss: 0.0527 - val_loss: 0.3306; rmse: 0.2366 - rmse_val: 0.5750;  r2: 0.9912 - r2_val: 0.9273                                                                                                    \n",
      "epoch: 0163, loss: 0.0557 - val_loss: 0.3157; rmse: 0.2365 - rmse_val: 0.5619;  r2: 0.9897 - r2_val: 0.9235                                                                                                    \n",
      "epoch: 0164, loss: 0.0478 - val_loss: 0.2986; rmse: 0.2081 - rmse_val: 0.5464;  r2: 0.9909 - r2_val: 0.9267                                                                                                    \n",
      "epoch: 0165, loss: 0.0477 - val_loss: 0.4079; rmse: 0.3573 - rmse_val: 0.6387;  r2: 0.9900 - r2_val: 0.9291                                                                                                    \n",
      "epoch: 0166, loss: 0.0787 - val_loss: 0.2884; rmse: 0.2026 - rmse_val: 0.5370;  r2: 0.9916 - r2_val: 0.9284                                                                                                    \n",
      "epoch: 0167, loss: 0.0562 - val_loss: 0.3035; rmse: 0.2061 - rmse_val: 0.5509;  r2: 0.9914 - r2_val: 0.9299                                                                                                    \n",
      "epoch: 0168, loss: 0.0423 - val_loss: 0.2970; rmse: 0.2266 - rmse_val: 0.5449;  r2: 0.9915 - r2_val: 0.9286                                                                                                    \n",
      "epoch: 0169, loss: 0.0461 - val_loss: 0.2944; rmse: 0.2035 - rmse_val: 0.5426;  r2: 0.9922 - r2_val: 0.9268                                                                                                    \n",
      "epoch: 0170, loss: 0.0471 - val_loss: 0.3258; rmse: 0.2261 - rmse_val: 0.5708;  r2: 0.9921 - r2_val: 0.9276                                                                                                    \n",
      "epoch: 0171, loss: 0.0430 - val_loss: 0.3004; rmse: 0.1891 - rmse_val: 0.5481;  r2: 0.9922 - r2_val: 0.9280                                                                                                    \n",
      "epoch: 0172, loss: 0.0371 - val_loss: 0.2940; rmse: 0.1894 - rmse_val: 0.5422;  r2: 0.9927 - r2_val: 0.9276                                                                                                    \n",
      "epoch: 0173, loss: 0.0375 - val_loss: 0.3460; rmse: 0.2536 - rmse_val: 0.5882;  r2: 0.9925 - r2_val: 0.9283                                                                                                    \n",
      "epoch: 0174, loss: 0.0502 - val_loss: 0.3223; rmse: 0.3002 - rmse_val: 0.5677;  r2: 0.9916 - r2_val: 0.9281                                                                                                    \n",
      "epoch: 0175, loss: 0.0611 - val_loss: 0.3407; rmse: 0.2506 - rmse_val: 0.5837;  r2: 0.9916 - r2_val: 0.9270                                                                                                    \n",
      "epoch: 0176, loss: 0.0585 - val_loss: 0.2943; rmse: 0.1843 - rmse_val: 0.5425;  r2: 0.9930 - r2_val: 0.9293                                                                                                    \n",
      "epoch: 0177, loss: 0.0416 - val_loss: 0.2873; rmse: 0.1811 - rmse_val: 0.5360;  r2: 0.9928 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0178, loss: 0.0345 - val_loss: 0.2891; rmse: 0.1830 - rmse_val: 0.5377;  r2: 0.9927 - r2_val: 0.9293                                                                                                    \n",
      "epoch: 0179, loss: 0.0343 - val_loss: 0.3261; rmse: 0.2198 - rmse_val: 0.5711;  r2: 0.9926 - r2_val: 0.9290                                                                                                    \n",
      "epoch: 0180, loss: 0.0449 - val_loss: 0.3201; rmse: 0.2981 - rmse_val: 0.5658;  r2: 0.9926 - r2_val: 0.9287                                                                                                    \n",
      "epoch: 0181, loss: 0.0607 - val_loss: 0.2914; rmse: 0.1871 - rmse_val: 0.5398;  r2: 0.9928 - r2_val: 0.9297                                                                                                    \n",
      "epoch: 0182, loss: 0.0464 - val_loss: 0.2871; rmse: 0.1998 - rmse_val: 0.5358;  r2: 0.9926 - r2_val: 0.9286                                                                                                    \n",
      "epoch: 0183, loss: 0.0413 - val_loss: 0.2853; rmse: 0.1866 - rmse_val: 0.5341;  r2: 0.9928 - r2_val: 0.9296                                                                                                    \n",
      "epoch: 0184, loss: 0.0349 - val_loss: 0.2987; rmse: 0.1842 - rmse_val: 0.5465;  r2: 0.9932 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0185, loss: 0.0328 - val_loss: 0.2875; rmse: 0.2053 - rmse_val: 0.5362;  r2: 0.9934 - r2_val: 0.9300                                                                                                    \n",
      "epoch: 0186, loss: 0.0363 - val_loss: 0.2918; rmse: 0.1716 - rmse_val: 0.5402;  r2: 0.9937 - r2_val: 0.9296                                                                                                    \n",
      "epoch: 0187, loss: 0.0430 - val_loss: 0.5826; rmse: 0.5134 - rmse_val: 0.7633;  r2: 0.9928 - r2_val: 0.9303                                                                                                    \n",
      "epoch: 0188, loss: 0.1257 - val_loss: 0.3072; rmse: 0.2420 - rmse_val: 0.5542;  r2: 0.9926 - r2_val: 0.9279                                                                                                    \n",
      "epoch: 0189, loss: 0.0636 - val_loss: 0.3685; rmse: 0.2926 - rmse_val: 0.6071;  r2: 0.9915 - r2_val: 0.9281                                                                                                    \n",
      "epoch: 0190, loss: 0.0566 - val_loss: 0.2874; rmse: 0.1927 - rmse_val: 0.5361;  r2: 0.9928 - r2_val: 0.9289                                                                                                    \n",
      "epoch: 0191, loss: 0.0416 - val_loss: 0.2916; rmse: 0.1813 - rmse_val: 0.5400;  r2: 0.9929 - r2_val: 0.9308                                                                                                    \n",
      "epoch: 0192, loss: 0.0348 - val_loss: 0.2850; rmse: 0.1973 - rmse_val: 0.5339;  r2: 0.9929 - r2_val: 0.9296                                                                                                    \n",
      "epoch: 0193, loss: 0.0379 - val_loss: 0.2982; rmse: 0.1840 - rmse_val: 0.5461;  r2: 0.9934 - r2_val: 0.9308                                                                                                    \n",
      "epoch: 0194, loss: 0.0319 - val_loss: 0.2808; rmse: 0.1962 - rmse_val: 0.5299;  r2: 0.9938 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0195, loss: 0.0364 - val_loss: 0.3122; rmse: 0.2042 - rmse_val: 0.5588;  r2: 0.9937 - r2_val: 0.9312                                                                                                    \n",
      "epoch: 0196, loss: 0.0358 - val_loss: 0.2850; rmse: 0.1722 - rmse_val: 0.5339;  r2: 0.9942 - r2_val: 0.9299                                                                                                    \n",
      "epoch: 0197, loss: 0.0355 - val_loss: 0.2857; rmse: 0.1632 - rmse_val: 0.5345;  r2: 0.9943 - r2_val: 0.9297                                                                                                    \n",
      "epoch: 0198, loss: 0.0272 - val_loss: 0.2888; rmse: 0.1613 - rmse_val: 0.5374;  r2: 0.9943 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0199, loss: 0.0260 - val_loss: 0.2825; rmse: 0.1693 - rmse_val: 0.5315;  r2: 0.9946 - r2_val: 0.9303                                                                                                    \n",
      "epoch: 0200, loss: 0.0268 - val_loss: 0.2862; rmse: 0.1847 - rmse_val: 0.5350;  r2: 0.9946 - r2_val: 0.9301                                                                                                    \n",
      "epoch: 0201, loss: 0.0295 - val_loss: 0.3031; rmse: 0.1765 - rmse_val: 0.5505;  r2: 0.9946 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0202, loss: 0.0276 - val_loss: 0.2861; rmse: 0.1540 - rmse_val: 0.5349;  r2: 0.9949 - r2_val: 0.9299                                                                                                    \n",
      "epoch: 0203, loss: 0.0281 - val_loss: 0.2908; rmse: 0.2131 - rmse_val: 0.5393;  r2: 0.9948 - r2_val: 0.9305                                                                                                    \n",
      "epoch: 0204, loss: 0.0353 - val_loss: 0.3086; rmse: 0.1840 - rmse_val: 0.5555;  r2: 0.9948 - r2_val: 0.9308                                                                                                    \n",
      "epoch: 0205, loss: 0.0290 - val_loss: 0.2898; rmse: 0.1529 - rmse_val: 0.5383;  r2: 0.9949 - r2_val: 0.9303                                                                                                    \n",
      "epoch: 0206, loss: 0.0247 - val_loss: 0.3019; rmse: 0.2416 - rmse_val: 0.5495;  r2: 0.9949 - r2_val: 0.9296                                                                                                    \n",
      "epoch: 0207, loss: 0.0350 - val_loss: 0.2980; rmse: 0.1644 - rmse_val: 0.5459;  r2: 0.9947 - r2_val: 0.9298                                                                                                    \n",
      "epoch: 0208, loss: 0.0266 - val_loss: 0.2903; rmse: 0.1554 - rmse_val: 0.5388;  r2: 0.9949 - r2_val: 0.9288                                                                                                    \n",
      "epoch: 0209, loss: 0.0248 - val_loss: 0.3557; rmse: 0.2547 - rmse_val: 0.5964;  r2: 0.9948 - r2_val: 0.9295                                                                                                    \n",
      "epoch: 0210, loss: 0.0366 - val_loss: 0.3251; rmse: 0.1991 - rmse_val: 0.5702;  r2: 0.9947 - r2_val: 0.9293                                                                                                    \n",
      "epoch: 0211, loss: 0.0304 - val_loss: 0.2862; rmse: 0.1602 - rmse_val: 0.5350;  r2: 0.9951 - r2_val: 0.9293                                                                                                    \n",
      "epoch: 0212, loss: 0.0234 - val_loss: 0.2902; rmse: 0.1488 - rmse_val: 0.5387;  r2: 0.9953 - r2_val: 0.9308                                                                                                    \n",
      "epoch: 0213, loss: 0.0215 - val_loss: 0.3196; rmse: 0.1962 - rmse_val: 0.5653;  r2: 0.9952 - r2_val: 0.9313                                                                                                    \n",
      "epoch: 0214, loss: 0.0306 - val_loss: 0.3254; rmse: 0.2080 - rmse_val: 0.5704;  r2: 0.9950 - r2_val: 0.9309                                                                                                    \n",
      "epoch: 0215, loss: 0.0418 - val_loss: 0.2950; rmse: 0.1574 - rmse_val: 0.5431;  r2: 0.9946 - r2_val: 0.9285                                                                                                    \n",
      "epoch: 0216, loss: 0.0344 - val_loss: 0.3112; rmse: 0.2571 - rmse_val: 0.5578;  r2: 0.9946 - r2_val: 0.9292                                                                                                    \n",
      "epoch: 0217, loss: 0.0402 - val_loss: 0.2877; rmse: 0.1620 - rmse_val: 0.5364;  r2: 0.9949 - r2_val: 0.9283                                                                                                    \n",
      "epoch: 0218, loss: 0.0329 - val_loss: 0.2937; rmse: 0.1586 - rmse_val: 0.5419;  r2: 0.9953 - r2_val: 0.9317                                                                                                    \n",
      "epoch: 0219, loss: 0.0253 - val_loss: 0.2848; rmse: 0.1460 - rmse_val: 0.5337;  r2: 0.9953 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0220, loss: 0.0264 - val_loss: 0.2954; rmse: 0.2148 - rmse_val: 0.5435;  r2: 0.9956 - r2_val: 0.9293                                                                                                    \n",
      "epoch: 0221, loss: 0.0330 - val_loss: 0.2863; rmse: 0.1501 - rmse_val: 0.5351;  r2: 0.9955 - r2_val: 0.9301                                                                                                    \n",
      "epoch: 0222, loss: 0.0282 - val_loss: 0.2877; rmse: 0.1412 - rmse_val: 0.5364;  r2: 0.9956 - r2_val: 0.9305                                                                                                    \n",
      "epoch: 0223, loss: 0.0260 - val_loss: 0.3081; rmse: 0.1807 - rmse_val: 0.5551;  r2: 0.9954 - r2_val: 0.9313                                                                                                    \n",
      "epoch: 0224, loss: 0.0258 - val_loss: 0.3331; rmse: 0.2192 - rmse_val: 0.5771;  r2: 0.9953 - r2_val: 0.9324                                                                                                    \n",
      "epoch: 0225, loss: 0.0368 - val_loss: 0.2806; rmse: 0.1542 - rmse_val: 0.5298;  r2: 0.9957 - r2_val: 0.9308                                                                                                    \n",
      "epoch: 0226, loss: 0.0261 - val_loss: 0.2930; rmse: 0.1447 - rmse_val: 0.5413;  r2: 0.9958 - r2_val: 0.9312                                                                                                    \n",
      "epoch: 0227, loss: 0.0197 - val_loss: 0.2850; rmse: 0.1358 - rmse_val: 0.5338;  r2: 0.9959 - r2_val: 0.9311                                                                                                    \n",
      "epoch: 0228, loss: 0.0186 - val_loss: 0.2818; rmse: 0.1585 - rmse_val: 0.5308;  r2: 0.9960 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0229, loss: 0.0243 - val_loss: 0.2884; rmse: 0.1411 - rmse_val: 0.5371;  r2: 0.9960 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0230, loss: 0.0202 - val_loss: 0.2878; rmse: 0.1369 - rmse_val: 0.5365;  r2: 0.9959 - r2_val: 0.9311                                                                                                    \n",
      "epoch: 0231, loss: 0.0187 - val_loss: 0.2867; rmse: 0.1343 - rmse_val: 0.5354;  r2: 0.9961 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0232, loss: 0.0182 - val_loss: 0.2817; rmse: 0.1411 - rmse_val: 0.5308;  r2: 0.9963 - r2_val: 0.9310                                                                                                    \n",
      "epoch: 0233, loss: 0.0251 - val_loss: 0.2890; rmse: 0.1817 - rmse_val: 0.5376;  r2: 0.9963 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0234, loss: 0.0232 - val_loss: 0.2869; rmse: 0.1333 - rmse_val: 0.5356;  r2: 0.9963 - r2_val: 0.9316                                                                                                    \n",
      "epoch: 0235, loss: 0.0194 - val_loss: 0.2927; rmse: 0.2081 - rmse_val: 0.5411;  r2: 0.9962 - r2_val: 0.9302                                                                                                    \n",
      "epoch: 0236, loss: 0.0304 - val_loss: 0.2857; rmse: 0.1425 - rmse_val: 0.5345;  r2: 0.9958 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0237, loss: 0.0268 - val_loss: 0.3381; rmse: 0.2168 - rmse_val: 0.5815;  r2: 0.9958 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0238, loss: 0.0330 - val_loss: 0.2893; rmse: 0.1312 - rmse_val: 0.5378;  r2: 0.9963 - r2_val: 0.9297                                                                                                    \n",
      "epoch: 0239, loss: 0.0212 - val_loss: 0.2839; rmse: 0.1322 - rmse_val: 0.5328;  r2: 0.9963 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0240, loss: 0.0194 - val_loss: 0.2869; rmse: 0.1280 - rmse_val: 0.5356;  r2: 0.9964 - r2_val: 0.9311                                                                                                    \n",
      "epoch: 0241, loss: 0.0175 - val_loss: 0.2833; rmse: 0.1313 - rmse_val: 0.5323;  r2: 0.9965 - r2_val: 0.9304                                                                                                    \n",
      "epoch: 0242, loss: 0.0185 - val_loss: 0.2890; rmse: 0.1307 - rmse_val: 0.5376;  r2: 0.9965 - r2_val: 0.9316                                                                                                    \n",
      "epoch: 0243, loss: 0.0166 - val_loss: 0.2942; rmse: 0.2124 - rmse_val: 0.5424;  r2: 0.9965 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0244, loss: 0.0276 - val_loss: 0.2849; rmse: 0.1830 - rmse_val: 0.5338;  r2: 0.9965 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0245, loss: 0.0218 - val_loss: 0.2835; rmse: 0.1589 - rmse_val: 0.5325;  r2: 0.9965 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0246, loss: 0.0217 - val_loss: 0.2838; rmse: 0.1270 - rmse_val: 0.5327;  r2: 0.9965 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0247, loss: 0.0192 - val_loss: 0.3717; rmse: 0.2770 - rmse_val: 0.6097;  r2: 0.9963 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0248, loss: 0.0481 - val_loss: 0.3525; rmse: 0.2484 - rmse_val: 0.5938;  r2: 0.9945 - r2_val: 0.9295                                                                                                    \n",
      "epoch: 0249, loss: 0.0385 - val_loss: 0.2870; rmse: 0.1700 - rmse_val: 0.5357;  r2: 0.9959 - r2_val: 0.9295                                                                                                    \n",
      "epoch: 0250, loss: 0.0277 - val_loss: 0.2888; rmse: 0.1712 - rmse_val: 0.5374;  r2: 0.9961 - r2_val: 0.9297                                                                                                    \n",
      "epoch: 0251, loss: 0.0251 - val_loss: 0.3178; rmse: 0.1822 - rmse_val: 0.5637;  r2: 0.9962 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0252, loss: 0.0263 - val_loss: 0.2816; rmse: 0.1348 - rmse_val: 0.5307;  r2: 0.9965 - r2_val: 0.9307                                                                                                    \n",
      "epoch: 0253, loss: 0.0188 - val_loss: 0.2834; rmse: 0.1374 - rmse_val: 0.5324;  r2: 0.9965 - r2_val: 0.9306                                                                                                    \n",
      "epoch: 0254, loss: 0.0195 - val_loss: 0.2783; rmse: 0.1377 - rmse_val: 0.5275;  r2: 0.9967 - r2_val: 0.9315                                                                                                    \n",
      "epoch: 0255, loss: 0.0183 - val_loss: 0.2835; rmse: 0.1243 - rmse_val: 0.5325;  r2: 0.9968 - r2_val: 0.9309                                                                                                    \n",
      "epoch: 0256, loss: 0.0159 - val_loss: 0.2966; rmse: 0.1381 - rmse_val: 0.5446;  r2: 0.9968 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0257, loss: 0.0197 - val_loss: 0.2853; rmse: 0.1203 - rmse_val: 0.5341;  r2: 0.9969 - r2_val: 0.9316                                                                                                    \n",
      "epoch: 0258, loss: 0.0150 - val_loss: 0.3009; rmse: 0.1405 - rmse_val: 0.5485;  r2: 0.9968 - r2_val: 0.9316                                                                                                    \n",
      "epoch: 0259, loss: 0.0206 - val_loss: 0.3089; rmse: 0.1606 - rmse_val: 0.5558;  r2: 0.9966 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0260, loss: 0.0197 - val_loss: 0.2805; rmse: 0.1671 - rmse_val: 0.5296;  r2: 0.9968 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0261, loss: 0.0239 - val_loss: 0.2813; rmse: 0.1599 - rmse_val: 0.5303;  r2: 0.9965 - r2_val: 0.9317                                                                                                    \n",
      "epoch: 0262, loss: 0.0189 - val_loss: 0.3028; rmse: 0.2415 - rmse_val: 0.5503;  r2: 0.9964 - r2_val: 0.9310                                                                                                    \n",
      "epoch: 0263, loss: 0.0340 - val_loss: 0.2784; rmse: 0.1359 - rmse_val: 0.5277;  r2: 0.9960 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0264, loss: 0.0173 - val_loss: 0.2792; rmse: 0.1350 - rmse_val: 0.5284;  r2: 0.9964 - r2_val: 0.9328                                                                                                    \n",
      "epoch: 0265, loss: 0.0181 - val_loss: 0.2794; rmse: 0.1261 - rmse_val: 0.5286;  r2: 0.9969 - r2_val: 0.9312                                                                                                    \n",
      "epoch: 0266, loss: 0.0161 - val_loss: 0.3052; rmse: 0.1526 - rmse_val: 0.5525;  r2: 0.9968 - r2_val: 0.9315                                                                                                    \n",
      "epoch: 0267, loss: 0.0170 - val_loss: 0.2860; rmse: 0.1158 - rmse_val: 0.5348;  r2: 0.9971 - r2_val: 0.9312                                                                                                    \n",
      "epoch: 0268, loss: 0.0148 - val_loss: 0.2794; rmse: 0.1268 - rmse_val: 0.5286;  r2: 0.9972 - r2_val: 0.9316                                                                                                    \n",
      "epoch: 0269, loss: 0.0171 - val_loss: 0.2798; rmse: 0.1249 - rmse_val: 0.5289;  r2: 0.9968 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0270, loss: 0.0163 - val_loss: 0.2796; rmse: 0.1254 - rmse_val: 0.5288;  r2: 0.9970 - r2_val: 0.9317                                                                                                    \n",
      "epoch: 0271, loss: 0.0146 - val_loss: 0.2783; rmse: 0.1192 - rmse_val: 0.5276;  r2: 0.9971 - r2_val: 0.9321                                                                                                    \n",
      "epoch: 0272, loss: 0.0157 - val_loss: 0.2988; rmse: 0.1453 - rmse_val: 0.5466;  r2: 0.9971 - r2_val: 0.9329                                                                                                    \n",
      "epoch: 0273, loss: 0.0208 - val_loss: 0.3224; rmse: 0.1847 - rmse_val: 0.5678;  r2: 0.9969 - r2_val: 0.9315                                                                                                    \n",
      "epoch: 0274, loss: 0.0276 - val_loss: 0.3768; rmse: 0.2826 - rmse_val: 0.6138;  r2: 0.9968 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0275, loss: 0.0586 - val_loss: 0.2849; rmse: 0.1473 - rmse_val: 0.5338;  r2: 0.9952 - r2_val: 0.9310                                                                                                    \n",
      "epoch: 0276, loss: 0.0345 - val_loss: 0.2738; rmse: 0.1462 - rmse_val: 0.5233;  r2: 0.9960 - r2_val: 0.9322                                                                                                    \n",
      "epoch: 0277, loss: 0.0267 - val_loss: 0.3141; rmse: 0.1872 - rmse_val: 0.5604;  r2: 0.9957 - r2_val: 0.9343                                                                                                    \n",
      "epoch: 0278, loss: 0.0395 - val_loss: 0.2961; rmse: 0.2381 - rmse_val: 0.5441;  r2: 0.9952 - r2_val: 0.9312                                                                                                    \n",
      "epoch: 0279, loss: 0.0547 - val_loss: 0.2918; rmse: 0.1985 - rmse_val: 0.5402;  r2: 0.9962 - r2_val: 0.9292                                                                                                    \n",
      "epoch: 0280, loss: 0.0252 - val_loss: 0.2876; rmse: 0.1442 - rmse_val: 0.5363;  r2: 0.9958 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0281, loss: 0.0178 - val_loss: 0.2857; rmse: 0.1407 - rmse_val: 0.5345;  r2: 0.9966 - r2_val: 0.9296                                                                                                    \n",
      "epoch: 0282, loss: 0.0193 - val_loss: 0.2890; rmse: 0.1356 - rmse_val: 0.5376;  r2: 0.9964 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0283, loss: 0.0178 - val_loss: 0.2895; rmse: 0.1249 - rmse_val: 0.5380;  r2: 0.9968 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0284, loss: 0.0162 - val_loss: 0.3478; rmse: 0.2346 - rmse_val: 0.5897;  r2: 0.9964 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0285, loss: 0.0348 - val_loss: 0.2973; rmse: 0.1411 - rmse_val: 0.5453;  r2: 0.9966 - r2_val: 0.9312                                                                                                    \n",
      "epoch: 0286, loss: 0.0230 - val_loss: 0.2797; rmse: 0.1352 - rmse_val: 0.5289;  r2: 0.9963 - r2_val: 0.9317                                                                                                    \n",
      "epoch: 0287, loss: 0.0173 - val_loss: 0.3412; rmse: 0.2265 - rmse_val: 0.5842;  r2: 0.9966 - r2_val: 0.9311                                                                                                    \n",
      "epoch: 0288, loss: 0.0347 - val_loss: 0.3048; rmse: 0.1627 - rmse_val: 0.5521;  r2: 0.9958 - r2_val: 0.9310                                                                                                    \n",
      "epoch: 0289, loss: 0.0252 - val_loss: 0.2798; rmse: 0.1840 - rmse_val: 0.5290;  r2: 0.9965 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0290, loss: 0.0364 - val_loss: 0.2911; rmse: 0.2202 - rmse_val: 0.5395;  r2: 0.9965 - r2_val: 0.9323                                                                                                    \n",
      "epoch: 0291, loss: 0.0290 - val_loss: 0.2785; rmse: 0.1403 - rmse_val: 0.5278;  r2: 0.9968 - r2_val: 0.9322                                                                                                    \n",
      "epoch: 0292, loss: 0.0191 - val_loss: 0.2825; rmse: 0.1526 - rmse_val: 0.5315;  r2: 0.9970 - r2_val: 0.9309                                                                                                    \n",
      "epoch: 0293, loss: 0.0239 - val_loss: 0.2955; rmse: 0.1826 - rmse_val: 0.5435;  r2: 0.9968 - r2_val: 0.9296                                                                                                    \n",
      "epoch: 0294, loss: 0.0263 - val_loss: 0.2986; rmse: 0.1331 - rmse_val: 0.5464;  r2: 0.9968 - r2_val: 0.9301                                                                                                    \n",
      "epoch: 0295, loss: 0.0215 - val_loss: 0.2945; rmse: 0.1340 - rmse_val: 0.5427;  r2: 0.9970 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0296, loss: 0.0171 - val_loss: 0.2856; rmse: 0.1194 - rmse_val: 0.5344;  r2: 0.9972 - r2_val: 0.9328                                                                                                    \n",
      "epoch: 0297, loss: 0.0137 - val_loss: 0.2753; rmse: 0.1269 - rmse_val: 0.5247;  r2: 0.9973 - r2_val: 0.9326                                                                                                    \n",
      "epoch: 0298, loss: 0.0145 - val_loss: 0.2782; rmse: 0.1148 - rmse_val: 0.5274;  r2: 0.9975 - r2_val: 0.9321                                                                                                    \n",
      "epoch: 0299, loss: 0.0127 - val_loss: 0.2797; rmse: 0.1069 - rmse_val: 0.5289;  r2: 0.9975 - r2_val: 0.9329                                                                                                    \n",
      "epoch: 0300, loss: 0.0120 - val_loss: 0.2790; rmse: 0.1432 - rmse_val: 0.5282;  r2: 0.9975 - r2_val: 0.9323                                                                                                    \n",
      "epoch: 0301, loss: 0.0168 - val_loss: 0.2819; rmse: 0.1524 - rmse_val: 0.5309;  r2: 0.9975 - r2_val: 0.9317                                                                                                    \n",
      "epoch: 0302, loss: 0.0161 - val_loss: 0.2794; rmse: 0.1353 - rmse_val: 0.5286;  r2: 0.9974 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0303, loss: 0.0130 - val_loss: 0.2875; rmse: 0.1180 - rmse_val: 0.5362;  r2: 0.9975 - r2_val: 0.9331                                                                                                    \n",
      "epoch: 0304, loss: 0.0130 - val_loss: 0.2777; rmse: 0.1047 - rmse_val: 0.5269;  r2: 0.9977 - r2_val: 0.9325                                                                                                    \n",
      "epoch: 0305, loss: 0.0116 - val_loss: 0.2799; rmse: 0.1065 - rmse_val: 0.5291;  r2: 0.9976 - r2_val: 0.9325                                                                                                    \n",
      "epoch: 0306, loss: 0.0134 - val_loss: 0.2962; rmse: 0.1278 - rmse_val: 0.5443;  r2: 0.9976 - r2_val: 0.9319                                                                                                    \n",
      "epoch: 0307, loss: 0.0141 - val_loss: 0.2946; rmse: 0.1353 - rmse_val: 0.5428;  r2: 0.9976 - r2_val: 0.9324                                                                                                    \n",
      "epoch: 0308, loss: 0.0146 - val_loss: 0.2932; rmse: 0.1183 - rmse_val: 0.5415;  r2: 0.9977 - r2_val: 0.9324                                                                                                    \n",
      "epoch: 0309, loss: 0.0121 - val_loss: 0.2837; rmse: 0.1009 - rmse_val: 0.5327;  r2: 0.9978 - r2_val: 0.9313                                                                                                    \n",
      "epoch: 0310, loss: 0.0107 - val_loss: 0.2816; rmse: 0.1032 - rmse_val: 0.5306;  r2: 0.9978 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0311, loss: 0.0110 - val_loss: 0.2919; rmse: 0.2025 - rmse_val: 0.5403;  r2: 0.9977 - r2_val: 0.9320                                                                                                    \n",
      "epoch: 0312, loss: 0.0277 - val_loss: 0.3451; rmse: 0.3397 - rmse_val: 0.5874;  r2: 0.9971 - r2_val: 0.9320                                                                                                    \n",
      "epoch: 0313, loss: 0.0656 - val_loss: 0.3781; rmse: 0.3957 - rmse_val: 0.6149;  r2: 0.9967 - r2_val: 0.9301                                                                                                    \n",
      "epoch: 0314, loss: 0.0811 - val_loss: 0.2849; rmse: 0.1382 - rmse_val: 0.5337;  r2: 0.9966 - r2_val: 0.9324                                                                                                    \n",
      "epoch: 0315, loss: 0.0472 - val_loss: 0.3672; rmse: 0.2585 - rmse_val: 0.6060;  r2: 0.9965 - r2_val: 0.9318                                                                                                    \n",
      "epoch: 0316, loss: 0.0505 - val_loss: 0.3163; rmse: 0.1746 - rmse_val: 0.5624;  r2: 0.9954 - r2_val: 0.9288                                                                                                    \n",
      "epoch: 0317, loss: 0.0251 - val_loss: 0.2863; rmse: 0.1331 - rmse_val: 0.5351;  r2: 0.9964 - r2_val: 0.9301                                                                                                    \n",
      "epoch: 0318, loss: 0.0171 - val_loss: 0.2838; rmse: 0.1202 - rmse_val: 0.5328;  r2: 0.9969 - r2_val: 0.9309                                                                                                    \n",
      "epoch: 0319, loss: 0.0148 - val_loss: 0.2772; rmse: 0.1444 - rmse_val: 0.5265;  r2: 0.9972 - r2_val: 0.9322                                                                                                    \n",
      "epoch: 0320, loss: 0.0170 - val_loss: 0.2994; rmse: 0.1411 - rmse_val: 0.5472;  r2: 0.9973 - r2_val: 0.9327                                                                                                    \n",
      "epoch: 0321, loss: 0.0159 - val_loss: 0.2804; rmse: 0.1611 - rmse_val: 0.5295;  r2: 0.9976 - r2_val: 0.9322                                                                                                    \n",
      "epoch: 0322, loss: 0.0197 - val_loss: 0.2773; rmse: 0.1323 - rmse_val: 0.5266;  r2: 0.9970 - r2_val: 0.9316                                                                                                    \n",
      "epoch: 0323, loss: 0.0165 - val_loss: 0.2822; rmse: 0.1139 - rmse_val: 0.5312;  r2: 0.9974 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0324, loss: 0.0140 - val_loss: 0.2821; rmse: 0.1067 - rmse_val: 0.5311;  r2: 0.9975 - r2_val: 0.9314                                                                                                    \n",
      "epoch: 0325, loss: 0.0152 - val_loss: 0.2768; rmse: 0.1154 - rmse_val: 0.5261;  r2: 0.9976 - r2_val: 0.9325                                                                                                    \n",
      "epoch: 0326, loss: 0.0126 - val_loss: 0.2903; rmse: 0.1171 - rmse_val: 0.5388;  r2: 0.9977 - r2_val: 0.9325                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00326: early stopping\n",
      "902 113 113\n",
      "Train on 902 samples, validate on 113 samples\n",
      "Epoch 1/276\n",
      "902/902 [==============================] - 1s 1ms/sample - loss: 12.1160 - val_loss: 11.0562\n",
      "Epoch 2/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 8.4720 - val_loss: 6.7065\n",
      "Epoch 3/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 4.5766 - val_loss: 4.5405\n",
      "Epoch 4/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 3.9502 - val_loss: 4.3881\n",
      "Epoch 5/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 3.7065 - val_loss: 4.3210\n",
      "Epoch 6/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 3.5496 - val_loss: 4.0921\n",
      "Epoch 7/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 3.4000 - val_loss: 3.9394\n",
      "Epoch 8/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 3.2704 - val_loss: 3.7043\n",
      "Epoch 9/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 3.2265 - val_loss: 3.5295\n",
      "Epoch 10/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 2.9569 - val_loss: 3.3104\n",
      "Epoch 11/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 2.7858 - val_loss: 3.1163\n",
      "Epoch 12/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 2.6320 - val_loss: 2.9594\n",
      "Epoch 13/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 2.4954 - val_loss: 2.8065\n",
      "Epoch 14/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 2.3664 - val_loss: 2.6338\n",
      "Epoch 15/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 2.2384 - val_loss: 2.4903\n",
      "Epoch 16/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 2.0477 - val_loss: 2.2928\n",
      "Epoch 17/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 1.9085 - val_loss: 2.3594\n",
      "Epoch 18/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 1.8451 - val_loss: 2.0831\n",
      "Epoch 19/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 1.6585 - val_loss: 1.9557\n",
      "Epoch 20/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 1.5199 - val_loss: 1.8700\n",
      "Epoch 21/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 1.4270 - val_loss: 1.7519\n",
      "Epoch 22/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 1.4199 - val_loss: 1.7808\n",
      "Epoch 23/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 1.3309 - val_loss: 1.7551\n",
      "Epoch 24/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 1.2143 - val_loss: 1.5193\n",
      "Epoch 25/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 1.1079 - val_loss: 1.4545\n",
      "Epoch 26/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 1.0724 - val_loss: 1.6773\n",
      "Epoch 27/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 1.3398 - val_loss: 1.4485\n",
      "Epoch 28/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 1.0398 - val_loss: 1.4089\n",
      "Epoch 29/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 0.9571 - val_loss: 1.2579\n",
      "Epoch 30/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.8973 - val_loss: 1.4042\n",
      "Epoch 31/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 1.0098 - val_loss: 1.1548\n",
      "Epoch 32/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 0.9751 - val_loss: 1.1840\n",
      "Epoch 33/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.8620 - val_loss: 1.2787\n",
      "Epoch 34/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.8332 - val_loss: 1.1728\n",
      "Epoch 35/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.8375 - val_loss: 1.0182\n",
      "Epoch 36/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.6917 - val_loss: 1.0534\n",
      "Epoch 37/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.7344 - val_loss: 0.9837\n",
      "Epoch 38/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.7627 - val_loss: 1.1866\n",
      "Epoch 39/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.9135 - val_loss: 0.8926\n",
      "Epoch 40/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.6358 - val_loss: 0.8824\n",
      "Epoch 41/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.6010 - val_loss: 0.8752\n",
      "Epoch 42/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.5962 - val_loss: 0.8218\n",
      "Epoch 43/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.5513 - val_loss: 0.7889\n",
      "Epoch 44/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.5799 - val_loss: 0.7998\n",
      "Epoch 45/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.5588 - val_loss: 0.7739\n",
      "Epoch 46/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.4924 - val_loss: 0.7307\n",
      "Epoch 47/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.4731 - val_loss: 0.7490\n",
      "Epoch 48/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.4793 - val_loss: 0.6957\n",
      "Epoch 49/276\n",
      "902/902 [==============================] - 0s 275us/sample - loss: 0.4291 - val_loss: 0.6967\n",
      "Epoch 50/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.4350 - val_loss: 0.6345\n",
      "Epoch 51/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.4008 - val_loss: 0.7206\n",
      "Epoch 52/276\n",
      "902/902 [==============================] - 0s 270us/sample - loss: 0.4281 - val_loss: 0.6519\n",
      "Epoch 53/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.3803 - val_loss: 0.5902\n",
      "Epoch 54/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.3514 - val_loss: 0.5922\n",
      "Epoch 55/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.3715 - val_loss: 0.5980\n",
      "Epoch 56/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.3477 - val_loss: 0.5701\n",
      "Epoch 57/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.4335 - val_loss: 0.6684\n",
      "Epoch 58/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.4021 - val_loss: 0.5329\n",
      "Epoch 59/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.3259 - val_loss: 0.5426\n",
      "Epoch 60/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.3085 - val_loss: 0.5088\n",
      "Epoch 61/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.2985 - val_loss: 0.5445\n",
      "Epoch 62/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.2871 - val_loss: 0.5455\n",
      "Epoch 63/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.3124 - val_loss: 0.5614\n",
      "Epoch 64/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.3456 - val_loss: 0.4975\n",
      "Epoch 65/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.2729 - val_loss: 0.6122\n",
      "Epoch 66/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.3245 - val_loss: 0.4985\n",
      "Epoch 67/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.3029 - val_loss: 0.7068\n",
      "Epoch 68/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.3363 - val_loss: 0.4999\n",
      "Epoch 69/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.2605 - val_loss: 0.4656\n",
      "Epoch 70/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.2232 - val_loss: 0.4469\n",
      "Epoch 71/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.2119 - val_loss: 0.4385\n",
      "Epoch 72/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.2037 - val_loss: 0.4680\n",
      "Epoch 73/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.2219 - val_loss: 0.4317\n",
      "Epoch 74/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.2078 - val_loss: 0.4608\n",
      "Epoch 75/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.2214 - val_loss: 0.4320\n",
      "Epoch 76/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.1960 - val_loss: 0.4415\n",
      "Epoch 77/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.1983 - val_loss: 0.4230\n",
      "Epoch 78/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.1836 - val_loss: 0.4144\n",
      "Epoch 79/276\n",
      "902/902 [==============================] - 0s 266us/sample - loss: 0.2006 - val_loss: 0.4076\n",
      "Epoch 80/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.1741 - val_loss: 0.4017\n",
      "Epoch 81/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.1690 - val_loss: 0.4152\n",
      "Epoch 82/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.1655 - val_loss: 0.4074\n",
      "Epoch 83/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.1668 - val_loss: 0.4179\n",
      "Epoch 84/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.1626 - val_loss: 0.4038\n",
      "Epoch 85/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.1752 - val_loss: 0.3857\n",
      "Epoch 86/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.1553 - val_loss: 0.3842\n",
      "Epoch 87/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1516 - val_loss: 0.3986\n",
      "Epoch 88/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.1444 - val_loss: 0.3832\n",
      "Epoch 89/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1377 - val_loss: 0.3758\n",
      "Epoch 90/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.1345 - val_loss: 0.3785\n",
      "Epoch 91/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1322 - val_loss: 0.3963\n",
      "Epoch 92/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.1657 - val_loss: 0.4606\n",
      "Epoch 93/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.1666 - val_loss: 0.3806\n",
      "Epoch 94/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.1335 - val_loss: 0.4041\n",
      "Epoch 95/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.1556 - val_loss: 0.3971\n",
      "Epoch 96/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.1246 - val_loss: 0.3810\n",
      "Epoch 97/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.1253 - val_loss: 0.3850\n",
      "Epoch 98/276\n",
      "902/902 [==============================] - 0s 241us/sample - loss: 0.1156 - val_loss: 0.3675\n",
      "Epoch 99/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1119 - val_loss: 0.3810\n",
      "Epoch 100/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.1189 - val_loss: 0.3677\n",
      "Epoch 101/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.1101 - val_loss: 0.3714\n",
      "Epoch 102/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.1060 - val_loss: 0.3680\n",
      "Epoch 103/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.1108 - val_loss: 0.3698\n",
      "Epoch 104/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.1066 - val_loss: 0.4455\n",
      "Epoch 105/276\n",
      "902/902 [==============================] - 0s 272us/sample - loss: 0.1372 - val_loss: 0.4038\n",
      "Epoch 106/276\n",
      "902/902 [==============================] - 0s 268us/sample - loss: 0.1291 - val_loss: 0.3727\n",
      "Epoch 107/276\n",
      "902/902 [==============================] - 0s 272us/sample - loss: 0.1018 - val_loss: 0.4287\n",
      "Epoch 108/276\n",
      "902/902 [==============================] - 0s 267us/sample - loss: 0.1296 - val_loss: 0.3608\n",
      "Epoch 109/276\n",
      "902/902 [==============================] - 0s 267us/sample - loss: 0.0901 - val_loss: 0.3699\n",
      "Epoch 110/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0919 - val_loss: 0.3628\n",
      "Epoch 111/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0874 - val_loss: 0.4035\n",
      "Epoch 112/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.1136 - val_loss: 0.5226\n",
      "Epoch 113/276\n",
      "902/902 [==============================] - 0s 241us/sample - loss: 0.1414 - val_loss: 0.3931\n",
      "Epoch 114/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1055 - val_loss: 0.4360\n",
      "Epoch 115/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.1031 - val_loss: 0.3816\n",
      "Epoch 116/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0873 - val_loss: 0.3603\n",
      "Epoch 117/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 0.0845 - val_loss: 0.3858\n",
      "Epoch 118/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0809 - val_loss: 0.3603\n",
      "Epoch 119/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0766 - val_loss: 0.3439\n",
      "Epoch 120/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0852 - val_loss: 0.3659\n",
      "Epoch 121/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0808 - val_loss: 0.3559\n",
      "Epoch 122/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0801 - val_loss: 0.3752\n",
      "Epoch 123/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0871 - val_loss: 0.3571\n",
      "Epoch 124/276\n",
      "902/902 [==============================] - 0s 242us/sample - loss: 0.0764 - val_loss: 0.5163\n",
      "Epoch 125/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.1342 - val_loss: 0.3656\n",
      "Epoch 126/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 0.0874 - val_loss: 0.3613\n",
      "Epoch 127/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0902 - val_loss: 0.4035\n",
      "Epoch 128/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0807 - val_loss: 0.3517\n",
      "Epoch 129/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0753 - val_loss: 0.3574\n",
      "Epoch 130/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0673 - val_loss: 0.4246\n",
      "Epoch 131/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0767 - val_loss: 0.3525\n",
      "Epoch 132/276\n",
      "902/902 [==============================] - 0s 240us/sample - loss: 0.0620 - val_loss: 0.3531\n",
      "Epoch 133/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0601 - val_loss: 0.3543\n",
      "Epoch 134/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.0573 - val_loss: 0.3466\n",
      "Epoch 135/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.0567 - val_loss: 0.3513\n",
      "Epoch 136/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0559 - val_loss: 0.3608\n",
      "Epoch 137/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0552 - val_loss: 0.3485\n",
      "Epoch 138/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0526 - val_loss: 0.3699\n",
      "Epoch 139/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0543 - val_loss: 0.3683\n",
      "Epoch 140/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0549 - val_loss: 0.3612\n",
      "Epoch 141/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0566 - val_loss: 0.3442\n",
      "Epoch 142/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0517 - val_loss: 0.3434\n",
      "Epoch 143/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0511 - val_loss: 0.3402\n",
      "Epoch 144/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0560 - val_loss: 0.3428\n",
      "Epoch 145/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0499 - val_loss: 0.3464\n",
      "Epoch 146/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0483 - val_loss: 0.3451\n",
      "Epoch 147/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0509 - val_loss: 0.3687\n",
      "Epoch 148/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0497 - val_loss: 0.3648\n",
      "Epoch 149/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0592 - val_loss: 0.3550\n",
      "Epoch 150/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.0658 - val_loss: 0.3484\n",
      "Epoch 151/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.0577 - val_loss: 0.4361\n",
      "Epoch 152/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.1218 - val_loss: 0.3645\n",
      "Epoch 153/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.1070 - val_loss: 0.3810\n",
      "Epoch 154/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0583 - val_loss: 0.3482\n",
      "Epoch 155/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0518 - val_loss: 0.3432\n",
      "Epoch 156/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0581 - val_loss: 0.3658\n",
      "Epoch 157/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.0539 - val_loss: 0.3566\n",
      "Epoch 158/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0598 - val_loss: 0.3718\n",
      "Epoch 159/276\n",
      "902/902 [==============================] - 0s 241us/sample - loss: 0.0567 - val_loss: 0.3462\n",
      "Epoch 160/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0507 - val_loss: 0.4437\n",
      "Epoch 161/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0637 - val_loss: 0.3567\n",
      "Epoch 162/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0564 - val_loss: 0.3453\n",
      "Epoch 163/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0432 - val_loss: 0.3604\n",
      "Epoch 164/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0588 - val_loss: 0.3403\n",
      "Epoch 165/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0489 - val_loss: 0.3455\n",
      "Epoch 166/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0387 - val_loss: 0.3386\n",
      "Epoch 167/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0362 - val_loss: 0.3380\n",
      "Epoch 168/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0376 - val_loss: 0.3559\n",
      "Epoch 169/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0386 - val_loss: 0.4675\n",
      "Epoch 170/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0717 - val_loss: 0.3770\n",
      "Epoch 171/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0513 - val_loss: 0.3394\n",
      "Epoch 172/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0424 - val_loss: 0.3405\n",
      "Epoch 173/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0371 - val_loss: 0.3948\n",
      "Epoch 174/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0445 - val_loss: 0.3582\n",
      "Epoch 175/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0488 - val_loss: 0.3493\n",
      "Epoch 176/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0501 - val_loss: 0.3516\n",
      "Epoch 177/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0397 - val_loss: 0.3563\n",
      "Epoch 178/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0579 - val_loss: 0.3450\n",
      "Epoch 179/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0535 - val_loss: 0.3679\n",
      "Epoch 180/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0475 - val_loss: 0.3455\n",
      "Epoch 181/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0329 - val_loss: 0.3437\n",
      "Epoch 182/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0317 - val_loss: 0.3370\n",
      "Epoch 183/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0313 - val_loss: 0.3408\n",
      "Epoch 184/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0345 - val_loss: 0.3496\n",
      "Epoch 185/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0394 - val_loss: 0.3376\n",
      "Epoch 186/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0469 - val_loss: 0.4047\n",
      "Epoch 187/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0472 - val_loss: 0.3835\n",
      "Epoch 188/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0404 - val_loss: 0.3347\n",
      "Epoch 189/276\n",
      "902/902 [==============================] - 0s 266us/sample - loss: 0.0393 - val_loss: 0.3447\n",
      "Epoch 190/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.0404 - val_loss: 0.3393\n",
      "Epoch 191/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0311 - val_loss: 0.3716\n",
      "Epoch 192/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0400 - val_loss: 0.3447\n",
      "Epoch 193/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0349 - val_loss: 0.3535\n",
      "Epoch 194/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0416 - val_loss: 0.4100\n",
      "Epoch 195/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0525 - val_loss: 0.3478\n",
      "Epoch 196/276\n",
      "902/902 [==============================] - 0s 270us/sample - loss: 0.0280 - val_loss: 0.3876\n",
      "Epoch 197/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0390 - val_loss: 0.3468\n",
      "Epoch 198/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0292 - val_loss: 0.3717\n",
      "Epoch 199/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0331 - val_loss: 0.3433\n",
      "Epoch 200/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0347 - val_loss: 0.3529\n",
      "Epoch 201/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0345 - val_loss: 0.3379\n",
      "Epoch 202/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0331 - val_loss: 0.3510\n",
      "Epoch 203/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0366 - val_loss: 0.4035\n",
      "Epoch 204/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0402 - val_loss: 0.3447\n",
      "Epoch 205/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.0279 - val_loss: 0.3446\n",
      "Epoch 206/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0309 - val_loss: 0.4312\n",
      "Epoch 207/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0562 - val_loss: 0.4092\n",
      "Epoch 208/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0520 - val_loss: 0.3335\n",
      "Epoch 209/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0429 - val_loss: 0.3627\n",
      "Epoch 210/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0465 - val_loss: 0.3389\n",
      "Epoch 211/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0254 - val_loss: 0.3383\n",
      "Epoch 212/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0258 - val_loss: 0.3552\n",
      "Epoch 213/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0242 - val_loss: 0.3366\n",
      "Epoch 214/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0276 - val_loss: 0.3340\n",
      "Epoch 215/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0267 - val_loss: 0.3632\n",
      "Epoch 216/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0299 - val_loss: 0.3320\n",
      "Epoch 217/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0249 - val_loss: 0.3341\n",
      "Epoch 218/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.0238 - val_loss: 0.3305\n",
      "Epoch 219/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0232 - val_loss: 0.3637\n",
      "Epoch 220/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0249 - val_loss: 0.3420\n",
      "Epoch 221/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0228 - val_loss: 0.3743\n",
      "Epoch 222/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0284 - val_loss: 0.3814\n",
      "Epoch 223/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0281 - val_loss: 0.3491\n",
      "Epoch 224/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0389 - val_loss: 0.3527\n",
      "Epoch 225/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0295 - val_loss: 0.3397\n",
      "Epoch 226/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.0316 - val_loss: 0.3696\n",
      "Epoch 227/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.0319 - val_loss: 0.3398\n",
      "Epoch 228/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0246 - val_loss: 0.3277\n",
      "Epoch 229/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0295 - val_loss: 0.3362\n",
      "Epoch 230/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0222 - val_loss: 0.3504\n",
      "Epoch 231/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0222 - val_loss: 0.4481\n",
      "Epoch 232/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0519 - val_loss: 0.4852\n",
      "Epoch 233/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0991 - val_loss: 0.3536\n",
      "Epoch 234/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0686 - val_loss: 0.3452\n",
      "Epoch 235/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0375 - val_loss: 0.3312\n",
      "Epoch 236/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0300 - val_loss: 0.4526\n",
      "Epoch 237/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0623 - val_loss: 0.3455\n",
      "Epoch 238/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0347 - val_loss: 0.3358\n",
      "Epoch 239/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.0271 - val_loss: 0.3576\n",
      "Epoch 240/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0347 - val_loss: 0.3728\n",
      "Epoch 241/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0386 - val_loss: 0.3605\n",
      "Epoch 242/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0302 - val_loss: 0.3360\n",
      "Epoch 243/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0319 - val_loss: 0.3564\n",
      "Epoch 244/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0387 - val_loss: 0.3382\n",
      "Epoch 245/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0293 - val_loss: 0.3382\n",
      "Epoch 246/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.0231 - val_loss: 0.3305\n",
      "Epoch 247/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0271 - val_loss: 0.3551\n",
      "Epoch 248/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0263 - val_loss: 0.3419\n",
      "Epoch 249/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0255 - val_loss: 0.3297\n",
      "Epoch 250/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0191 - val_loss: 0.3490\n",
      "Epoch 251/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.0198 - val_loss: 0.3281\n",
      "Epoch 252/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0174 - val_loss: 0.3378\n",
      "Epoch 253/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0270 - val_loss: 0.3330\n",
      "Epoch 254/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0202 - val_loss: 0.3549\n",
      "Epoch 255/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0220 - val_loss: 0.3573\n",
      "Epoch 256/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0203 - val_loss: 0.3519\n",
      "Epoch 257/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0231 - val_loss: 0.3501\n",
      "Epoch 258/276\n",
      "902/902 [==============================] - 0s 279us/sample - loss: 0.0203 - val_loss: 0.3541\n",
      "Epoch 259/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0218 - val_loss: 0.3396\n",
      "Epoch 260/276\n",
      "902/902 [==============================] - 0s 270us/sample - loss: 0.0180 - val_loss: 0.3562\n",
      "Epoch 261/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0193 - val_loss: 0.3252\n",
      "Epoch 262/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0203 - val_loss: 0.3239\n",
      "Epoch 263/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0168 - val_loss: 0.3368\n",
      "Epoch 264/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0160 - val_loss: 0.3477\n",
      "Epoch 265/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0208 - val_loss: 0.3318\n",
      "Epoch 266/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0204 - val_loss: 0.3301\n",
      "Epoch 267/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0253 - val_loss: 0.4152\n",
      "Epoch 268/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.0764 - val_loss: 0.3344\n",
      "Epoch 269/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0243 - val_loss: 0.3417\n",
      "Epoch 270/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0317 - val_loss: 0.3630\n",
      "Epoch 271/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0231 - val_loss: 0.3420\n",
      "Epoch 272/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0182 - val_loss: 0.3452\n",
      "Epoch 273/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0169 - val_loss: 0.3877\n",
      "Epoch 274/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.0282 - val_loss: 0.3602\n",
      "Epoch 275/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0193 - val_loss: 0.3455\n",
      "Epoch 276/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0156 - val_loss: 0.3434\n",
      "902 113 113\n",
      "Train on 902 samples, validate on 113 samples\n",
      "Epoch 1/276\n",
      "902/902 [==============================] - 1s 1ms/sample - loss: 13.2838 - val_loss: 7.8131\n",
      "Epoch 2/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 8.6886 - val_loss: 4.3436\n",
      "Epoch 3/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 4.8114 - val_loss: 3.6097\n",
      "Epoch 4/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 4.0908 - val_loss: 3.4321\n",
      "Epoch 5/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 3.8421 - val_loss: 3.0548\n",
      "Epoch 6/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 3.6617 - val_loss: 3.3377\n",
      "Epoch 7/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 3.6012 - val_loss: 3.0036\n",
      "Epoch 8/276\n",
      "902/902 [==============================] - 0s 240us/sample - loss: 3.3619 - val_loss: 2.7157\n",
      "Epoch 9/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 3.1388 - val_loss: 2.7064\n",
      "Epoch 10/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 3.0396 - val_loss: 2.4143\n",
      "Epoch 11/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 2.8271 - val_loss: 2.5907\n",
      "Epoch 12/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 2.7289 - val_loss: 2.2142\n",
      "Epoch 13/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 2.5399 - val_loss: 2.2655\n",
      "Epoch 14/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 2.4150 - val_loss: 1.9820\n",
      "Epoch 15/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 2.2771 - val_loss: 2.0789\n",
      "Epoch 16/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 2.2011 - val_loss: 1.8013\n",
      "Epoch 17/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 2.1863 - val_loss: 2.2866\n",
      "Epoch 18/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 2.1091 - val_loss: 1.6670\n",
      "Epoch 19/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 1.8062 - val_loss: 1.8770\n",
      "Epoch 20/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 1.7452 - val_loss: 1.5704\n",
      "Epoch 21/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 1.6780 - val_loss: 1.4876\n",
      "Epoch 22/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 1.4952 - val_loss: 1.4237\n",
      "Epoch 23/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 1.3652 - val_loss: 1.3017\n",
      "Epoch 24/276\n",
      "902/902 [==============================] - 0s 267us/sample - loss: 1.2939 - val_loss: 1.3035\n",
      "Epoch 25/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 1.1884 - val_loss: 1.1790\n",
      "Epoch 26/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 1.1102 - val_loss: 1.1211\n",
      "Epoch 27/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 1.0515 - val_loss: 1.0844\n",
      "Epoch 28/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.9746 - val_loss: 1.0537\n",
      "Epoch 29/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.9195 - val_loss: 1.0403\n",
      "Epoch 30/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.8764 - val_loss: 0.9882\n",
      "Epoch 31/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.8368 - val_loss: 0.9766\n",
      "Epoch 32/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.8207 - val_loss: 1.1052\n",
      "Epoch 33/276\n",
      "902/902 [==============================] - 0s 267us/sample - loss: 0.8420 - val_loss: 0.9034\n",
      "Epoch 34/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.7537 - val_loss: 0.8644\n",
      "Epoch 35/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.6735 - val_loss: 0.8476\n",
      "Epoch 36/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.6423 - val_loss: 0.8494\n",
      "Epoch 37/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.6414 - val_loss: 0.8101\n",
      "Epoch 38/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.6302 - val_loss: 0.8183\n",
      "Epoch 39/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.6452 - val_loss: 0.8912\n",
      "Epoch 40/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.6199 - val_loss: 0.8367\n",
      "Epoch 41/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.5898 - val_loss: 0.7418\n",
      "Epoch 42/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.5011 - val_loss: 0.7031\n",
      "Epoch 43/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.4740 - val_loss: 0.6712\n",
      "Epoch 44/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.4704 - val_loss: 0.7821\n",
      "Epoch 45/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.5096 - val_loss: 0.6905\n",
      "Epoch 46/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.4424 - val_loss: 0.6237\n",
      "Epoch 47/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.4254 - val_loss: 0.6440\n",
      "Epoch 48/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.4182 - val_loss: 0.6151\n",
      "Epoch 49/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.4244 - val_loss: 0.5924\n",
      "Epoch 50/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 0.3647 - val_loss: 0.5864\n",
      "Epoch 51/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.3469 - val_loss: 0.6695\n",
      "Epoch 52/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.3799 - val_loss: 0.6702\n",
      "Epoch 53/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.3818 - val_loss: 0.5901\n",
      "Epoch 54/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.4125 - val_loss: 0.7244\n",
      "Epoch 55/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.3953 - val_loss: 0.5600\n",
      "Epoch 56/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.3230 - val_loss: 0.5662\n",
      "Epoch 57/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.3071 - val_loss: 0.5378\n",
      "Epoch 58/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.2897 - val_loss: 0.5572\n",
      "Epoch 59/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.3207 - val_loss: 0.9656\n",
      "Epoch 60/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.4679 - val_loss: 0.5572\n",
      "Epoch 61/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.2963 - val_loss: 0.5246\n",
      "Epoch 62/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.2935 - val_loss: 0.6061\n",
      "Epoch 63/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.3053 - val_loss: 0.5563\n",
      "Epoch 64/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.2628 - val_loss: 0.5117\n",
      "Epoch 65/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.2318 - val_loss: 0.5004\n",
      "Epoch 66/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.2725 - val_loss: 0.7835\n",
      "Epoch 67/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.3381 - val_loss: 0.5712\n",
      "Epoch 68/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.2536 - val_loss: 0.5085\n",
      "Epoch 69/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.2260 - val_loss: 0.5780\n",
      "Epoch 70/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.2292 - val_loss: 0.4975\n",
      "Epoch 71/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.2230 - val_loss: 0.5048\n",
      "Epoch 72/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.2011 - val_loss: 0.4890\n",
      "Epoch 73/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.1902 - val_loss: 0.4860\n",
      "Epoch 74/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1863 - val_loss: 0.4791\n",
      "Epoch 75/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.1858 - val_loss: 0.4975\n",
      "Epoch 76/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.1833 - val_loss: 0.4670\n",
      "Epoch 77/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.1688 - val_loss: 0.4577\n",
      "Epoch 78/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.1667 - val_loss: 0.5436\n",
      "Epoch 79/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.2081 - val_loss: 0.4767\n",
      "Epoch 80/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1646 - val_loss: 0.4558\n",
      "Epoch 81/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.1641 - val_loss: 0.4566\n",
      "Epoch 82/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.1454 - val_loss: 0.4535\n",
      "Epoch 83/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1439 - val_loss: 0.4764\n",
      "Epoch 84/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.1526 - val_loss: 0.4646\n",
      "Epoch 85/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.1434 - val_loss: 0.4532\n",
      "Epoch 86/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.1432 - val_loss: 0.4865\n",
      "Epoch 87/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1611 - val_loss: 0.4832\n",
      "Epoch 88/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.1870 - val_loss: 0.4554\n",
      "Epoch 89/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.1671 - val_loss: 0.4489\n",
      "Epoch 90/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.1400 - val_loss: 0.4413\n",
      "Epoch 91/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.1491 - val_loss: 0.4660\n",
      "Epoch 92/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1363 - val_loss: 0.4572\n",
      "Epoch 93/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1374 - val_loss: 0.4682\n",
      "Epoch 94/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.1420 - val_loss: 0.4331\n",
      "Epoch 95/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1166 - val_loss: 0.4382\n",
      "Epoch 96/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.1172 - val_loss: 0.4287\n",
      "Epoch 97/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.1110 - val_loss: 0.4700\n",
      "Epoch 98/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.1279 - val_loss: 0.4706\n",
      "Epoch 99/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.1269 - val_loss: 0.4718\n",
      "Epoch 100/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.1569 - val_loss: 0.4275\n",
      "Epoch 101/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.1217 - val_loss: 0.5187\n",
      "Epoch 102/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.1773 - val_loss: 0.4751\n",
      "Epoch 103/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.1385 - val_loss: 0.4945\n",
      "Epoch 104/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.1367 - val_loss: 0.5565\n",
      "Epoch 105/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.1741 - val_loss: 0.4778\n",
      "Epoch 106/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.1325 - val_loss: 0.4479\n",
      "Epoch 107/276\n",
      "902/902 [==============================] - 0s 240us/sample - loss: 0.1030 - val_loss: 0.4460\n",
      "Epoch 108/276\n",
      "902/902 [==============================] - 0s 241us/sample - loss: 0.1029 - val_loss: 0.4494\n",
      "Epoch 109/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.1271 - val_loss: 0.4290\n",
      "Epoch 110/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.1049 - val_loss: 0.4387\n",
      "Epoch 111/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0927 - val_loss: 0.4461\n",
      "Epoch 112/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.1093 - val_loss: 0.4300\n",
      "Epoch 113/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0845 - val_loss: 0.4349\n",
      "Epoch 114/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0839 - val_loss: 0.4312\n",
      "Epoch 115/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0946 - val_loss: 0.4188\n",
      "Epoch 116/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0895 - val_loss: 0.4049\n",
      "Epoch 117/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0971 - val_loss: 0.4049\n",
      "Epoch 118/276\n",
      "902/902 [==============================] - 0s 242us/sample - loss: 0.0860 - val_loss: 0.4467\n",
      "Epoch 119/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0862 - val_loss: 0.4387\n",
      "Epoch 120/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0838 - val_loss: 0.4508\n",
      "Epoch 121/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0911 - val_loss: 0.4206\n",
      "Epoch 122/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0832 - val_loss: 0.4124\n",
      "Epoch 123/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0750 - val_loss: 0.4150\n",
      "Epoch 124/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0688 - val_loss: 0.4284\n",
      "Epoch 125/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0762 - val_loss: 0.4575\n",
      "Epoch 126/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0829 - val_loss: 0.4000\n",
      "Epoch 127/276\n",
      "902/902 [==============================] - 0s 273us/sample - loss: 0.0640 - val_loss: 0.4159\n",
      "Epoch 128/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.0739 - val_loss: 0.4011\n",
      "Epoch 129/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0657 - val_loss: 0.4036\n",
      "Epoch 130/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.0680 - val_loss: 0.4117\n",
      "Epoch 131/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0650 - val_loss: 0.3944\n",
      "Epoch 132/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0564 - val_loss: 0.3933\n",
      "Epoch 133/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0573 - val_loss: 0.4021\n",
      "Epoch 134/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.0619 - val_loss: 0.4146\n",
      "Epoch 135/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0782 - val_loss: 0.4120\n",
      "Epoch 136/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0720 - val_loss: 0.3872\n",
      "Epoch 137/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0601 - val_loss: 0.4007\n",
      "Epoch 138/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.0727 - val_loss: 0.4135\n",
      "Epoch 139/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0763 - val_loss: 0.5394\n",
      "Epoch 140/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.1317 - val_loss: 0.4378\n",
      "Epoch 141/276\n",
      "902/902 [==============================] - 0s 239us/sample - loss: 0.0777 - val_loss: 0.5688\n",
      "Epoch 142/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.1299 - val_loss: 0.5593\n",
      "Epoch 143/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.1600 - val_loss: 0.5498\n",
      "Epoch 144/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.1213 - val_loss: 0.4138\n",
      "Epoch 145/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0644 - val_loss: 0.3956\n",
      "Epoch 146/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0554 - val_loss: 0.3928\n",
      "Epoch 147/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0553 - val_loss: 0.3909\n",
      "Epoch 148/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0496 - val_loss: 0.3909\n",
      "Epoch 149/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0455 - val_loss: 0.3905\n",
      "Epoch 150/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0448 - val_loss: 0.3995\n",
      "Epoch 151/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0450 - val_loss: 0.3923\n",
      "Epoch 152/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0459 - val_loss: 0.4019\n",
      "Epoch 153/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0522 - val_loss: 0.4530\n",
      "Epoch 154/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.1059 - val_loss: 0.4553\n",
      "Epoch 155/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0955 - val_loss: 0.4436\n",
      "Epoch 156/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0748 - val_loss: 0.3927\n",
      "Epoch 157/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0469 - val_loss: 0.3843\n",
      "Epoch 158/276\n",
      "902/902 [==============================] - 0s 273us/sample - loss: 0.0452 - val_loss: 0.4398\n",
      "Epoch 159/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0664 - val_loss: 0.3801\n",
      "Epoch 160/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0450 - val_loss: 0.3862\n",
      "Epoch 161/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0481 - val_loss: 0.3922\n",
      "Epoch 162/276\n",
      "902/902 [==============================] - 0s 276us/sample - loss: 0.0429 - val_loss: 0.3841\n",
      "Epoch 163/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0465 - val_loss: 0.3887\n",
      "Epoch 164/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0503 - val_loss: 0.3788\n",
      "Epoch 165/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0417 - val_loss: 0.3773\n",
      "Epoch 166/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0400 - val_loss: 0.4094\n",
      "Epoch 167/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.0498 - val_loss: 0.3821\n",
      "Epoch 168/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0451 - val_loss: 0.3806\n",
      "Epoch 169/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0392 - val_loss: 0.3812\n",
      "Epoch 170/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0399 - val_loss: 0.4167\n",
      "Epoch 171/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0481 - val_loss: 0.3952\n",
      "Epoch 172/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0464 - val_loss: 0.3802\n",
      "Epoch 173/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.0396 - val_loss: 0.3809\n",
      "Epoch 174/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0375 - val_loss: 0.3820\n",
      "Epoch 175/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.0351 - val_loss: 0.3856\n",
      "Epoch 176/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.0417 - val_loss: 0.3948\n",
      "Epoch 177/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0398 - val_loss: 0.3834\n",
      "Epoch 178/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0349 - val_loss: 0.4075\n",
      "Epoch 179/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0521 - val_loss: 0.4179\n",
      "Epoch 180/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0506 - val_loss: 0.3849\n",
      "Epoch 181/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0490 - val_loss: 0.4009\n",
      "Epoch 182/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0553 - val_loss: 0.4007\n",
      "Epoch 183/276\n",
      "902/902 [==============================] - 0s 259us/sample - loss: 0.0420 - val_loss: 0.3807\n",
      "Epoch 184/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0445 - val_loss: 0.4062\n",
      "Epoch 185/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0448 - val_loss: 0.3807\n",
      "Epoch 186/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0353 - val_loss: 0.3830\n",
      "Epoch 187/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0334 - val_loss: 0.3978\n",
      "Epoch 188/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0437 - val_loss: 0.3922\n",
      "Epoch 189/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0382 - val_loss: 0.3847\n",
      "Epoch 190/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0387 - val_loss: 0.3881\n",
      "Epoch 191/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0325 - val_loss: 0.3877\n",
      "Epoch 192/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0300 - val_loss: 0.4255\n",
      "Epoch 193/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0581 - val_loss: 0.4036\n",
      "Epoch 194/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0384 - val_loss: 0.4000\n",
      "Epoch 195/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0446 - val_loss: 0.3949\n",
      "Epoch 196/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0348 - val_loss: 0.4212\n",
      "Epoch 197/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0428 - val_loss: 0.4132\n",
      "Epoch 198/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0409 - val_loss: 0.3961\n",
      "Epoch 199/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0344 - val_loss: 0.3828\n",
      "Epoch 200/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0322 - val_loss: 0.3857\n",
      "Epoch 201/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0415 - val_loss: 0.4100\n",
      "Epoch 202/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0437 - val_loss: 0.3761\n",
      "Epoch 203/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0280 - val_loss: 0.3803\n",
      "Epoch 204/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0270 - val_loss: 0.3919\n",
      "Epoch 205/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0365 - val_loss: 0.4038\n",
      "Epoch 206/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.0388 - val_loss: 0.3964\n",
      "Epoch 207/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0353 - val_loss: 0.3998\n",
      "Epoch 208/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0337 - val_loss: 0.3968\n",
      "Epoch 209/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0392 - val_loss: 0.3925\n",
      "Epoch 210/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0460 - val_loss: 0.3728\n",
      "Epoch 211/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0297 - val_loss: 0.3800\n",
      "Epoch 212/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0285 - val_loss: 0.4096\n",
      "Epoch 213/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0308 - val_loss: 0.3779\n",
      "Epoch 214/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0289 - val_loss: 0.3781\n",
      "Epoch 215/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0289 - val_loss: 0.4148\n",
      "Epoch 216/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0394 - val_loss: 0.3820\n",
      "Epoch 217/276\n",
      "902/902 [==============================] - 0s 260us/sample - loss: 0.0323 - val_loss: 0.3969\n",
      "Epoch 218/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0351 - val_loss: 0.3734\n",
      "Epoch 219/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.0304 - val_loss: 0.3817\n",
      "Epoch 220/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0273 - val_loss: 0.3733\n",
      "Epoch 221/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0268 - val_loss: 0.3862\n",
      "Epoch 222/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.0316 - val_loss: 0.3718\n",
      "Epoch 223/276\n",
      "902/902 [==============================] - 0s 262us/sample - loss: 0.0265 - val_loss: 0.3810\n",
      "Epoch 224/276\n",
      "902/902 [==============================] - 0s 255us/sample - loss: 0.0249 - val_loss: 0.3826\n",
      "Epoch 225/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0270 - val_loss: 0.3791\n",
      "Epoch 226/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0229 - val_loss: 0.3798\n",
      "Epoch 227/276\n",
      "902/902 [==============================] - 0s 244us/sample - loss: 0.0240 - val_loss: 0.3889\n",
      "Epoch 228/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0365 - val_loss: 0.3726\n",
      "Epoch 229/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0244 - val_loss: 0.3776\n",
      "Epoch 230/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0221 - val_loss: 0.3726\n",
      "Epoch 231/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0238 - val_loss: 0.3925\n",
      "Epoch 232/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0248 - val_loss: 0.3855\n",
      "Epoch 233/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0391 - val_loss: 0.3836\n",
      "Epoch 234/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0466 - val_loss: 0.3817\n",
      "Epoch 235/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0311 - val_loss: 0.3818\n",
      "Epoch 236/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0233 - val_loss: 0.4113\n",
      "Epoch 237/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0363 - val_loss: 0.3822\n",
      "Epoch 238/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0329 - val_loss: 0.4051\n",
      "Epoch 239/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0470 - val_loss: 0.3744\n",
      "Epoch 240/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0382 - val_loss: 0.4702\n",
      "Epoch 241/276\n",
      "902/902 [==============================] - 0s 264us/sample - loss: 0.0668 - val_loss: 0.3701\n",
      "Epoch 242/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0518 - val_loss: 0.4560\n",
      "Epoch 243/276\n",
      "902/902 [==============================] - 0s 256us/sample - loss: 0.0564 - val_loss: 0.3835\n",
      "Epoch 244/276\n",
      "902/902 [==============================] - 0s 243us/sample - loss: 0.0350 - val_loss: 0.3897\n",
      "Epoch 245/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0311 - val_loss: 0.3797\n",
      "Epoch 246/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0239 - val_loss: 0.4191\n",
      "Epoch 247/276\n",
      "902/902 [==============================] - 0s 261us/sample - loss: 0.0425 - val_loss: 0.3815\n",
      "Epoch 248/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0371 - val_loss: 0.4228\n",
      "Epoch 249/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0375 - val_loss: 0.3882\n",
      "Epoch 250/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0325 - val_loss: 0.3756\n",
      "Epoch 251/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0246 - val_loss: 0.3988\n",
      "Epoch 252/276\n",
      "902/902 [==============================] - 0s 245us/sample - loss: 0.0360 - val_loss: 0.3778\n",
      "Epoch 253/276\n",
      "902/902 [==============================] - 0s 239us/sample - loss: 0.0288 - val_loss: 0.4026\n",
      "Epoch 254/276\n",
      "902/902 [==============================] - 0s 242us/sample - loss: 0.0390 - val_loss: 0.4176\n",
      "Epoch 255/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0446 - val_loss: 0.3950\n",
      "Epoch 256/276\n",
      "902/902 [==============================] - 0s 263us/sample - loss: 0.0384 - val_loss: 0.3734\n",
      "Epoch 257/276\n",
      "902/902 [==============================] - 0s 250us/sample - loss: 0.0274 - val_loss: 0.3704\n",
      "Epoch 258/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0208 - val_loss: 0.3772\n",
      "Epoch 259/276\n",
      "902/902 [==============================] - 0s 246us/sample - loss: 0.0206 - val_loss: 0.3695\n",
      "Epoch 260/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0202 - val_loss: 0.3844\n",
      "Epoch 261/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0295 - val_loss: 0.3715\n",
      "Epoch 262/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0230 - val_loss: 0.3739\n",
      "Epoch 263/276\n",
      "902/902 [==============================] - 0s 253us/sample - loss: 0.0248 - val_loss: 0.3741\n",
      "Epoch 264/276\n",
      "902/902 [==============================] - 0s 257us/sample - loss: 0.0272 - val_loss: 0.4060\n",
      "Epoch 265/276\n",
      "902/902 [==============================] - 0s 258us/sample - loss: 0.0305 - val_loss: 0.3981\n",
      "Epoch 266/276\n",
      "902/902 [==============================] - 0s 249us/sample - loss: 0.0368 - val_loss: 0.4079\n",
      "Epoch 267/276\n",
      "902/902 [==============================] - 0s 240us/sample - loss: 0.0460 - val_loss: 0.3782\n",
      "Epoch 268/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0336 - val_loss: 0.3731\n",
      "Epoch 269/276\n",
      "902/902 [==============================] - 0s 254us/sample - loss: 0.0292 - val_loss: 0.3812\n",
      "Epoch 270/276\n",
      "902/902 [==============================] - 0s 251us/sample - loss: 0.0277 - val_loss: 0.3681\n",
      "Epoch 271/276\n",
      "902/902 [==============================] - 0s 252us/sample - loss: 0.0234 - val_loss: 0.3677\n",
      "Epoch 272/276\n",
      "902/902 [==============================] - 0s 265us/sample - loss: 0.0204 - val_loss: 0.3755\n",
      "Epoch 273/276\n",
      "902/902 [==============================] - 0s 248us/sample - loss: 0.0197 - val_loss: 0.3695\n",
      "Epoch 274/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0170 - val_loss: 0.3733\n",
      "Epoch 275/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0188 - val_loss: 0.3671\n",
      "Epoch 276/276\n",
      "902/902 [==============================] - 0s 247us/sample - loss: 0.0192 - val_loss: 0.3985\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    \n",
    "    train_idx = [i for i in train_idx if i < len(df)]\n",
    "    valid_idx = [i for i in valid_idx if i < len(df)]    \n",
    "    test_idx = [i for i in test_idx if i < len(df)]\n",
    "    \n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.Reg_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                   (validX, validY), \n",
    "                                                                   patience = patience, \n",
    "                                                                   criteria = monitor)\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "    performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "\n",
    "    train_rmses, train_r2s = performance.evaluate(trainX, trainY)            \n",
    "    valid_rmses, valid_r2s = performance.evaluate(validX, validY)            \n",
    "    test_rmses, test_r2s = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                 'task_name':task_name,            \n",
    "                 'train_rmse':np.nanmean(train_rmses), \n",
    "                 'valid_rmse':np.nanmean(valid_rmses),                      \n",
    "                 'test_rmse':np.nanmean(test_rmses), \n",
    "\n",
    "                 'train_r2':np.nanmean(train_r2s), \n",
    "                 'valid_r2':np.nanmean(valid_r2s),                      \n",
    "                 'test_r2':np.nanmean(test_r2s), \n",
    "\n",
    "                 '# trainable params': trainable_params,\n",
    "                 'best_epoch': best_epoch,\n",
    "                 'batch_size':batch_size,\n",
    "                 'lr': lr,\n",
    "                 'weight_decay':weight_decay\n",
    "                }\n",
    "    results.append(final_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f12a89c2b38>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3hUVfrA8e+ZZFJJQkgCCQmQ0AVCDUgTFRvYRbAruu5ad3XV9be6q7ur6+pa1lXX3ntBdq2ggvTeOySQkIQU0nufcn5/nEmDQBIYnBDez/PkyZQ7956ZZN577nua0lojhBDi5GfxdAGEEEK4hwR0IYToJCSgCyFEJyEBXQghOgkJ6EII0Ul4e+rA4eHhOjY21lOHF0KIk9KmTZsKtNYRLT3nsYAeGxvLxo0bPXV4IYQ4KSml0o/0nKRchBCik5CALoQQnYQEdCGE6CQ8lkMXQnRuNpuNzMxMampqPF2Uk5Kfnx8xMTFYrdY2v0YCuhDihMjMzCQoKIjY2FiUUp4uzklFa01hYSGZmZnExcW1+XWSchFCnBA1NTWEhYVJMD8GSinCwsLafXUjAV0IccJIMD92x/LZeSyg55XXeurQQgjRKXksoOdLQBdCCLfyWECXhTWEEL8krTVOp9PTxTihPBfQkaAuhDix0tLSGDRoEDfddBPDhg3Dy8uLBx98kKFDh3Luueeyfv16zjrrLPr27cu3334LwK5duxg3bhwjR45k+PDh7Nu3D4CPP/644fHbb78dh8PhybfWIo92W7Q5ND7e0mgiRGf32He72J1d5tZ9DukZzF8vGdrqdvv27eODDz5g/PjxKKWYOnUqzz77LFdccQWPPPIICxcuZPfu3cyePZtLL72U119/nXvvvZfrr7+euro6HA4He/bs4YsvvmDVqlVYrVbuuusuPvnkE2666Sa3vqfj5eGA7sTHWzraCCFOnD59+jB+/HgAfHx8mDZtGgDx8fH4+vpitVqJj48nLS0NgAkTJvCPf/yDzMxMZsyYwYABA1i0aBGbNm1i7NixAFRXV9O9e3ePvJ+j8XhAF0J0fm2pSZ8ogYGBDbetVmtDd0CLxYKvr2/DbbvdDsB1113H6aefzrx587jwwgt544030Foze/ZsnnrqqV/+DbSDR6vHdRLQhRAdzP79++nbty/33HMPl112Gdu3b+ecc85h7ty55OXlAVBUVER6+hFnsfUYj+fQhRCiI5kzZw4fffQRVquVyMhI/vSnP9GtWzeeeOIJzj//fJxOJ1arlVdeeYU+ffp4urjNKE/1NPGNGqCTdmwlNjyw9Y2FECedPXv2cNppp3m6GCe1lj5DpdQmrXVCS9t7NOUiOXQhhHAfyaELIUQn4bGAfppKlxy6EEK4kccCuhdOSbkIIYQbeSygK8Bml4AuhBDu4sEcusbm6sgvhBDi+Hm0UdRRJ1PoCiGEu3g2oNsloAshOoYuXbp4ugjHrdWArpTyU0qtV0ptU0rtUko91sI2vkqpL5RSyUqpdUqp2LYc3G6TgC6EOLnYO3CquC1D/2uBqVrrCqWUFViplPpBa722yTa3AsVa6/5KqWuAp4GrW9uxw1Z3TIUWQpxkfngIcna4d5+R8TD9n0d8+qGHHqJXr17cfffdAPztb3/D29ubJUuWUFxcjM1m44knnuCyyy5r9VBLly7l0UcfJTQ0lMTERBYsWMC0adMYP348q1evZuzYsdxyyy389a9/JS8vj08++YRx48axbNky7r33XsCsEbp8+XKCgoJ49tlnmTNnDrW1tVxxxRU89thh9eRj0moNXRsVrrtW18+hHcgvAz5w3Z4LnKPasMKpU1IuQogT5Oqrr2bOnDkN9+fMmcPs2bP56quv2Lx5M0uWLOGBBx5o80I7mzdv5sUXX2Tv3r0AJCcn88ADD5CYmEhiYiKffvopK1eu5LnnnuPJJ58E4LnnnuOVV15h69atrFixAn9/fxYsWMC+fftYv349W7duZdOmTSxfvtwt77lNk3MppbyATUB/4BWt9bpDNokGMgC01nalVCkQBhQcsp/bgNsAxkRZcNqlhi7EKeEoNekTZdSoUeTl5ZGdnU1+fj6hoaFERkZy3333sXz5ciwWC1lZWeTm5hIZGdnq/saNG0dcXFzD/bi4OOLj4wEYOnQo55xzDkqpZnOrT5o0ifvvv5/rr7+eGTNmEBMTw4IFC1iwYAGjRo0CoKKign379jFlypTjfs9tCuhaawcwUinVFfhKKTVMa72zvQfTWr8JvAmQ0NNLOySHLoQ4gWbNmsXcuXPJycnh6quv5pNPPiE/P59NmzZhtVqJjY2lpqamTftqOq860DCXOhx5bvWHHnqIiy66iPnz5zNp0iR++ukntNY8/PDD3H777W56l43a1ctFa10CLAGmHfJUFtALQCnlDYQAha3tT2roQogT6eqrr+bzzz9n7ty5zJo1i9LSUrp3747VamXJkiUnfE7zlJQU4uPj+eMf/8jYsWNJTEzkggsu4N1336WiwmSys7KyGuZZP16t1tCVUhGATWtdopTyB87DNHo29S0wG1gDzAQW6zYkprQEdCHECTR06FDKy8uJjo4mKiqK66+/nksuuYT4+HgSEhIYPHjwCT3+Cy+8wJIlS7BYLAwdOpTp06fj6+vLnj17mDBhAmC6S3788cduWdKu1fnQlVLDMQ2eXpga/Ryt9eNKqceBjVrrb5VSfsBHwCigCLhGa73/aPtN6OmlH3zxY66ede1xvwkhRMcj86Efv/bOh95qDV1rvR0TqA99/C9NbtcAs9pbWKdDcuhCCOEuHl2CDrvNo4cXQoimduzYwY033tjsMV9fX9atO7RjX8fk2YDukBy6EJ2Z1po2DEnpMOLj49m6dauniwHQ5v7xTXl0LhctAV2ITsvPz4/CwsJjCkynOq01hYWF+Pn5tet1knIRQpwQMTExZGZmkp+f7+minJT8/PyIiYlp12s8G9CdEtCF6KysVmuzkZXixPNoykVy6EII4T4eDejKITV0IYRwF8/W0CXlIoQQbuPhGrqkXIQQwl08GtAtUkMXQgi38VhA1yiUBHQhhHAbD9bQldTQhRDCjTxXQ1cKi5aALoQQ7uLRlIuX1NCFEMJtPJtykRq6EEK4jYdr6HZPHV4IITodz9XQlcJLauhCCOE2nq2hS0AXQgi38WgO3UtLykUIIdzFoykXb22Tye+FEMJNPFpDtyoHtXan54oghBCdiEcHFvlgl4AuhBBu0mpAV0r1UkotUUrtVkrtUkrd28I2ZymlSpVSW10/f2n90AordmrtjmMruRBCiGbasgSdHXhAa71ZKRUEbFJKLdRa7z5kuxVa64vbfGRlAnqd1NCFEMItWq2ha60Paq03u26XA3uA6OM+slJ4Izl0IYRwl3bl0JVSscAoYF0LT09QSm1TSv2glBp6hNffppTaqJTaaLPZTQ7dJgFdCCHcoc0BXSnVBfgv8HutddkhT28G+mitRwD/Ab5uaR9a6ze11gla6wQfH1+sSnLoQgjhLm0K6EopKyaYf6K1/t+hz2uty7TWFa7b8wGrUiq8lZ1KDl0IIdyoLb1cFPAOsEdr/fwRtol0bYdSapxrv4Wt7NjVy0UCuhBCuENberlMAm4Ediiltroe+xPQG0Br/TowE7hTKWUHqoFrdCtDQJX0QxdCCLdqNaBrrVcCqpVtXgZebteRlQU/ZaPWJvO5CCGEO3hspKhS5tD2uhpPFUEIIToVD07OVR/QqzxWBCGE6Ew8XkN31lR6qghCCNGpeC6gW8yhHXXVniqCEEJ0Kh6voWub5NCFEMIdPJ5Dd0oNXQgh3MKjKxYBaJs0igohhDt4vIaOTWroQgjhDp4P6HbJoQshhDtIQBdCiE7C4zl0ZZeUixBCuIPHa+heUkMXQgi38HhAVw4J6EII4Q4eD+hSQxdCCPfwXEAH6rBicUpAF0IId/BoQLdZfPF21HqyCEII0Wl4NKDblS/eUkMXQgi38HAN3Q+rs86TRRBCiE7DszV0L1+8tdTQhRDCHTwa0B0WX3y01NCFEMIdPBvQvfywOmsorbZ5shhCCNEptBrQlVK9lFJLlFK7lVK7lFL3trCNUkq9pJRKVkptV0qNbsvBnV6++FLHhKcWkV0iUwAIIcTxaEsN3Q48oLUeAowH7lZKDTlkm+nAANfPbcBrbTl4eGhXogLA5nDyn8XJ7Si2EEKIQ7Ua0LXWB7XWm123y4E9QPQhm10GfKiNtUBXpVRUa/vu0iWIHv6aa8f15suNGRRWSJ90IYQ4Vu3KoSulYoFRwLpDnooGMprcz+TwoH84bz+w1zBrTC/sTs3SpPz2FEcIIUQTbQ7oSqkuwH+B32uty47lYEqp25RSG5VSG/Pz88HqD7YqhkYFERHky+KkvGPZrRBCCNoY0JVSVkww/0Rr/b8WNskCejW5H+N6rBmt9Zta6wStdUJERISpodeUYnlxGOcOCGH53nzKaqTHixBCHIu29HJRwDvAHq3180fY7FvgJldvl/FAqdb6YKtHdy1yQVkW1/erpbrOwazX1lBd52hr+YUQQri0pYY+CbgRmKqU2ur6uVApdYdS6g7XNvOB/UAy8BZwV5uOfvqdcNbDAAzzyuDl60aRlFvOwj257X0fQghxyvNubQOt9UpAtbKNBu5u99GDo+CMP8CK5yFvF+efew09gn35bls2l47o2e7dCSHEqcyjI0UB8PKGiIGQuxuLRXHx8J4sS8qnpEqmBBBCiPbwfEAH6D4UcncBMHNMDHUOJx+uSfdwoYQQ4uTSMQJ65DCoyIHSLE6LCuacwd15d1UqlbV2T5dMCCFOGh0joA+cbn5v/xzSVnLX2f0pqbLx6boDni2XEEKcRDpGQA/vD1EjYNHj8P5FjAksZELfMN5csZ8am3RhFEKItugYAR1g+DWNtw9u5Xfn9Ce/vJYPVqd5rEhCCHEy6TgB/fQ74K51YLFCzg4m9gvn7EERvLw4maJK6fEihBCt6TgB3WKB7oMhYjDk7gTgwQsGU15rZ96O1gedCiHEqa7jBPR6kcMgZwcAp0UFERsWwMLdMnJUCCFa0/ECeo9hUJEL5bkopThvSA/WpBRQLpN2CSHEUXW8gB43xfze/Q0AFwyNxObQ/LAjx4OFEkKIjq/jBfSo4aYL4+YPQGvG9AllcGQQ765KxUwZI4QQoiUdL6ADjJ5tGkZ/+hNKa26dHEdiTjlr9hd6umRCCNFhddCAfhOMuRnWvgr7l3DJiJ4E+njx3bZsT5dMCCE6rI4Z0L2scP4ToLzgwBr8rF6cO6QHP+7MweZwerp0QgjRIXXMgA7gGwSR8XBgLQAXxUdRXGVjTYqkXYQQoiUdN6AD9J4AmRvBXseUgRF08fXm++2SdhFCiJZ08IA+HuzVkL4KP6sX5w3pwU+7cqmzS9pFCCEO1bED+oDzISgKFj8BWnPx8ChKq22sSinwdMmEEKLD6dgB3ScApj4CWRth/1ImDwgnyM+bedtlbhchhDhUxw7oAEMuN71d0lbi6+3F+UMi+WlXjqRdhBDiEB0/oPt2MaNHXb1dLh4eRXmNnZXJ+R4umBBCdCytBnSl1LtKqTyl1M4jPH+WUqpUKbXV9fMXt5ey90STdrHXMal/OMF+3nwvaRchhGimLTX094FprWyzQms90vXz+PEX6xC9x4O9BjLW4eNt4YKhkSzclSvL0wkhRBOtBnSt9XKg6Bcoy5H1PRO69ID5D4KtmouGR1Fea2eBzJMuhBAN3JVDn6CU2qaU+kEpNfRIGymlblNKbVRKbczPb0cO3C8ELn0Z8vfA9jlM6h/OoB5B/PmrHaQWVLqj/EIIcdJzR0DfDPTRWo8A/gN8faQNtdZvaq0TtNYJERER7TvKgPMgqCekLMbqZeHt2QnYHZq3V+w/rsILIURncdwBXWtdprWucN2eD1iVUuHHXbJDKQX9psL+peB00KtbAGcNimDh7lycTpknXQghjjugK6UilVLKdXuca58nZgatfmdDTQlkbQbg/KE9yCuvZWtmyQk5nBBCnEza0m3xM2ANMEgplamUulUpdYdS6g7XJjOBnUqpbcBLwDX6RC0t1G8qKAvs/RGAqYN74GVRLNojjaNCCOHd2gZa62tbef5l4GW3lehoArpBn0mw5zs451FC/K3ER4ewbr9nO+EIIURH0PFHih7qtEuhIAnyk6CukvFx3diWWUJ1nfRJF0Kc2k7CgH6x+b35Q3iyJzPrvsbm0Gw+UOzZcgkhhIedfAE9uCfEjIW1rwEQl78Yb4vi5cXJlNXYPFw4IYTwnJMvoAOcdglok2LxCgzjyRnxrE8r4qWf93m4YEII4Tknb0BHmdulGVyV0Itxsd1YlyqNo0KIU9fJGdC79YXfLILRs6EkA4Bb/JbizNlBVZ3dw4UTQgjPODkDOkD0GAjrD7WlUJHHealPc71awLaMUk+XTAghPOLkDegAXXuZ3/uXorSTKFXIxjRJuwghTk0nd0AP6W1+Jy8CIM6nhO+3H+REDVQVQoiO7OQO6PU19OSfAeipikjKLeeJuatZvSfDgwUTQohf3skd0AMjICgKqgoA8LGXE2at48odd1L89f95uHBCCPHLOrkDulIw8vpmD316RRiDLQcIq06THi9CiFPKyR3QAUbfaH57+wEwqHIDFjTdKWJTums6gJIMKE73UAGFEOKXcfIH9NBYuOF/MPt7cz95MQA9VDFrkk0qhu/uga/v8kz5hBDiF3LyB3SA/udAz5GAgvSVAASqWhZuTcHucJraeckBz5ZRCCFOsM4R0AG8rNCj+frUzrJs5u/MwV56EEf5QZDujEKITqzzBHSAi/7V7O7IrtW8v2QH3o4qvJw2qJYpdoUQnVfnCui9x8O1n8M1nwIwc6A3JbmNqRZ76UH3HcspC2oIITqWzhXQAQZNh7gzAUjoVsMA/4qGpwpyDsDO/8KyZ47vGLm74fFusO/n49uPEEK4UecL6AC+XcA3GGtlLn+aEtrwcGFuBmz5BNYc5xKoy542vzPWHt9+hBDCjTpnQAcI6QXZW+hjbZx9saIgC0ozoab02PPpNWWw51tz2+rvhoIKIYR7dN6AnnALZK6HrZ+BNYBqfLGVHMRe7JrjZd2bsPCv7d9vwT7QTnO7ttx95RVCiOPUakBXSr2rlMpTSu08wvNKKfWSUipZKbVdKTXa/cU8BqNnQ9fekL8HLN6UeHXDtzgJb0cVAHr5M7DqBShOa99+XfPGAKa2LoQQHURbaujvA9OO8vx0YIDr5zbgteMvlht4+8DM98xt32B8uvZkkH1vw9PK6ZrnZfc37M0t541lKW3bb1Vh4+1aCehCiI6j1YCutV4OHG3ViMuAD7WxFuiqlIpyVwGPS0wC/GYJXD+HsNh4glVVs6c1CnZ9xafrDvDUD4mUVNW1vs9KVw09NE5SLkKIDsUdOfRooOnk45muxw6jlLpNKbVRKbUxPz/fDYdug+jRZgRpv6mNBdQRACzxmojO3kp2vgnSaYVVZjRpXVWLuwJMysXLB4J7SspFCNGh/KKNolrrN7XWCVrrhIiIiF/y0A190wE26YGU6EC+qB6LQuOVnwhAemGl6cHy3ACoLml5P5WF6IBwsqqtOCSgCyE6EHcE9CygV5P7Ma7HOhb/rg03a6c8wvM9niLVuy8AoRUmt55WUAXZW6CuAopTW95PVQElKph1B+1UlMr6pUKIjsMdAf1b4CZXb5fxQKnW2o1j7N3o7g3w60Vcde5EHr9rNkNOG0aF9mMgZnqA9KLKxnnTS49wTqosIKPGn3Ltj6+j8hcquBBCtM67tQ2UUp8BZwHhSqlM4K+AFUBr/TowH7gQSAaqgFtOVGGPW8TAZndnJvQhcU9vbvZewAFLDNsKZ4G3K6CXtRzQ7RUFpFb3pAJ/fOwVJueu1IkuuRBCtKrVgK61vraV5zVwt9tK9Aua2C+ML5zRJFj28hfLu1xcMBZ862vomS2/qKqAIj2Ich2ABQfYqsEn4JcrdL3acvjyFrjwGejW95c/vhCiw+m8I0XbwGJReJ/1B+b5XwLAyJr1DQOHHCUtBHR7Ld62Cgp1MBW4hv17qi963h5IXgjpqz1zfCFEh3NKB3SAWedO5qIHP8Th25WZXssbHq/Kb2ENUtegoiKCCQjuZh7zVF/0+v7wTQc6CSFOaad8QAfAYsErbjIjLWa0aIozquUcekUuAOWWEAK6uHrNHK3rYuoKyN975Oc3fwTf3nNsZa509eOvLDj6dkKIU4YE9HrDr2q4uUUNIag2B1a9BA574za5uwAo7tIPr4AQ89iRUi5aw5ybYMkTRz7m3h9hy0dHH8h0JPUBvUq6TgohDAno9YZcRs0dG0ma9im2yFHmsYWPQtK8xm2yt1KlArAFx2FtLaCXpEN1UWM3yJZU5JqZG3O2t7+89akWSbkIIVwkoDfhFzmAQeMvwjbkSu6tu4syawTOzR+B02lWKcrewl4VR0SIPz6BZuEMe9URRpRmbzW/Sw60/Dw0pHDI3tL+wjbU0CXlIoQwJKC3YOaEgfiNvpYPqydC8iL0Z1fDaxMgayNb7LH0CPbD2rUndm2hruAII0rrg3R1EdRWHP681lCR13zb9pBGUSHEISSgtyDAx5unZw7HZ+KdbHX2Re1bAH4mxbLd3pvIYD+CAgNI05E48xJb3snBrY23SzMOf762HOw15nbW5vYXsj6gV0pAP+kc3CZ/N3FCSEA/it9MH8/7g99iYs1LfHHGT+Sc/md+dI6le7AvIQFWknU03oVJh7+wPAcOrIOoEeZ+S2mX+tp5SC8zb0zTxte2qE+11JaCw9a+1wrP+uBSWPm8p0shOiEJ6EehlOKZWSMYPHgIf/xuP7MTx+PwDuD0uDC6+lvZp6PxKUsHe23zFy57Bpw2mOZaTLrFgO7Kn/caB077EacaaJHWJofuumo4rp4uS56EL28+9teL9rHVQE2JOekL4WYS0FvhZ/Xi9RvGMKl/GEm55dw4vg+RIX50DfBhnzMai3ZAYZPVjuoqYcvHMPI66D0evP1Mj5dD1Qf0mHHmd3uWwqspMSeB8EHm/vE0jG77HFIWH/vrRftUu06+0pgtTgAJ6G3g423h1evG8OAFg7jnnAEARIX4kWXtA8DCL19l837XBJP7l4GjFobOYFtmKbagGLOw9KHqe6n0Gmt+tyegu/Kvu2yuhaGOtWG0JMOcbGpKZfWlX0r930py6OIEkIDeRiEBVu4+uz8h/lbA1NwHD0tgq7Mf5xV8RLdPp1P28pnw+bXg0wV7rwnMfm89a51DzIjRuqrmefKKXFBeEDkcLN6H1+JzdsCnV8OGt02KpankhQC8leFaGKryGFd/Sl/VePtI0wUL96pPj0nvJHECSEA/DjPGxXF53eP8ngfwtZVSme/KlccksO1gFSVVNhbaRoCtEv3iCPTHMxqDc1k2OjCCGqfF1TCa1nznmz8yI0nnPQCpjXPM4HTAutcpDR/Dj86xOJWX6SN/LNJWNt4uO8LskvW09tyo1LKDsPlDzxzb3ZqmXA49UQtxnCSgH4fRvUN5+srh/O7u+5np/w4Ta1/iScttOKc9w7IkU2v+uqw/2tsPVZmHSl0Gu782qY5dX7PbZziXv7IKQvscHtDTVkKv8eAbDFs/bXw8eREUp5EYez01+JIf0B+yNh7bG8jcCBGDze3WauhJP8BzA5uPfK0ugTmzoSz72I7fVuvfhG9/d+KP80uor5k76iTNJdxOAvpxUEpx9dje9Ivowor/O5t/XTWKN6vOYnVpGPN2mJx6md1KXvxt/Ms+iz26D/qrO+HDSwHNG9YbSMwppyKwD+TshLWvg73O9DHP2wUDzoNhM2D3N41rnG77FPy7sStoMgAH/E8z/didTvPahX+BlCWNhazIb3mumNoKKEiCwRcBqnH+97SVsOjxw7ffv9T03Gmapqk/QSX90HzbkgPu7UqZs8P8bqkt4kRJ+gGW/tP9+60qbnJb0i7CvSSgu4nFojh7UHcsCm54Zx3phVX8alIcAP+omsF/7FdwU+3/cbD7GRAQBrPeZ2t5MABLu99gesT8+Ed4fjC8O83sNG4KJNwK9mpY/R/TkJY4H+JnUVBtLteTvAeZ+WQK9sK8+2DVi/DxDNj5P9Od8vVJ8P19Jsg2TZnkbDfzyMSMgyDX7JJOh9l2xb8O72pZfxWwf6lJfzjscNA1B019wAXTHe8/Y2D9W+77cOvnuin8BQP6hrdh+XPtHx/QmqZBXAL6iaM1FB1hFHdHcXCb2/+/JKC7UWigDy9eM4o7z+rHnDsmcOdZ/QD4dls2PYJ90V168FTQn+DXP+MYMI2DpdUALDnoCzd9A9fPhYHToGsvGHwx9BwFUcNh2JWw5mV4eypoB4y+iaLKOgC2K1fXxU9mmu6Sk+6F6DEw737Y8I5pfN35X3h9Mnx4manJQ+N0Az1HQUi0qaHv+sqcGKB5V0ZbTWPw3v6FSX/s+l9joG0a0JPmm3RCyiL3fKjluY1dPAuS2/Yahw22z2l5auP8vc1PbBnrW556IXeXuSIp2t/+Mh9NdZNj//fXppzu1FF7z1QVwasTYO1rv8zxdv4XXhppFoLpiHb+F96YAhvfha/vavx+HScJ6G52yYie/HHaYEb3DiW8i0/D47MnxnLuaT1YkphHrd1BXnkNNofGy6JYl1po1iUdcB5c/irc+BVc8wl4mR41nPd36H8u+HU1z0UOawjoSbYecNHzZhqBsx6Gcx+DS182KZWfHoaAcBOYakpNAF72tPlSbXwPgqMhqIdplD24DeY/CJHx5vHkn00wXf8W/Ge02UdkfOMb3fR+4z9h7i4TRDM2mCsDgANrj177qKs0y/fV2/A2LHi0eUNhdTGs/Le5bfFuuYZekQ9zb23Mr9tq4INL4H+/aXxtaZYpS3G6+RL98H/mca3NMn6fXtM8LVVZCOWubqj5R5jaoSwbfv6bOX7KkrY3cFYVgb+Z2I3iVPdeyexfBs/2g8R5rW/rLg5783EYR3JgDeTthh8f+mXKt+WjxuMejdMJ391r/l/BrAB2ont81VXBgr+Y2xvfga2fwIrn3LLrVtcUFcdOKcXNE2Pxs3px55n9WJKUx+cbMvjnD4nU2k1NeULfMFYmF1BUWcIPZ2AAACAASURBVEe3QJ+WdxQSbQI8sD+/ghmPL8DhNAGkuMoGY281P/W6D4bZ35n0y/BZphYa1t/UCpa58sLd+sGUP5jbk++DjHUm6M98H1a9YL4Qe74zz1sDwRoA5z8BC/8KMQkmAAP0iIfcHfDyWBOgAEJjTSPv38PMFcPk+2HuLdBnIkx5EJJ+hG/uguCecMYfTDpn9cuAhrB+MOZmEyD/dxvsW2D2GXdmyzn0DW/BzrnmBOHtA4HdG7/E+xZCYDgseAROvxMKk036KnG+OYFVFTb27vnoCnPSCo2D0y5p3H9+k6kdClPMFcj4u8xnu+51E5DrKuDqj83r6qrMSbP7EIifCRav5uWtKoTwgebzBnN1YKsGq3/Lf/v22POt+QznPQCxk81I4rSVJk2W8CvzeR+JrQY+vw56joRz/tL2Y65+CZb8A363yfzdjyRjPaDAN8ic9AdfZBqFfbq4f5H10ixzcgPI2mTe+5FkbjCVk+J08zf88HKIHg23/HDiFn9P/L7x/66+wpA437SdBYYf164loJ9gf7t0aMPtif3CCfTx4r1VaQ2PTRsWycrkAnZklXLmwIhW97d8bz4lVY0NjiVVdS1v2GeC+QGTsgEYc4tJqfgEQmgfDhRWUZVTxuCo4XD3OvMFC+4JZzxgaukB3aDnaBPA7TUm6Ny+zKQyitNNLf6M+0wN2eINl7xoAlb/c01NGEzg2/COCXopi80l8M7/mRNMzg74crbZLnK4+XJ/dy8se9ZcnRSnwsTfmTRU6nLz+vkPmmNpJ3j5mBSQ8oK9TRpmY8bCaZea+ex/2mGubNa9btJVA6ebbd+YAri+sIMuNGUJjTUN0Dvnmsf9QkzDb/Qo6D3R1PqzNpkG6h1zzWdUVWS22/iuCejr3zQnRDCNzvXBsbbCfO7VRaZ89QHdaTO9jeLOMG0YYK4sKvJg+tPNg0pVkTkp9RoHmZtM2uuMB8xJJ3wg7F1gRg8XJMG6N6HHEBOkwdRAfYPNib//Oc3/V7Q27Tcpi0y5zvhD2xY+19r0wHLaYcsnMPXPR942c6MJlOGDzEmxYB+8eTaMvwOmPnL49iUZpvYaPtCMui47CLYqc8IHU2HI3eVq1G/CYTf/QxYvCBtgOgzkJ8EXN8DZf4ahlzfffs+35vf+paYS46g1FYLkn80Vc1P1V3HHuyj89jkQHAPxV5rvh2+ImZNpwztw1h/NNrXlpiJlaV8SpU0BXSk1DXgR8ALe1lr/85DnbwaeBeqvVV7WWr/drpKcAvysXtwwvg/rUovYmmF6rZw/tAePfL2TFXvzCfbzZlTv0KPuY/OB5vOvl9XYcThN6qZVXt7mS+5yycsrKa22se8f07H6BpnaE0C3ODj74eavbVqD9AuGG+aaAGTxMgEqOLp5bfSWH8xVwL6fzGXs4ItM3/rE+TDkUrj8ddgxxwTHsbeaf15blWlwzd5iatwxCXDO30y5/UNNamPb564DKFPbdtTB5a+ZYN9vKqx8Ac75KwRFmoDeazxc+Cy8cYa5mpj1PrwQb04G5Qchchhc+1ljuQ+shXcvMLe7D4G0FeZHWcxJJGJw4+XxtZ+bk9eK52HpkzD//2DHl6YcXXqYsjgdZvqHVS+YydrKc8C/GwybaU5qy/4JH1wM4243A8Ys3q4rEW0Ch19XE9TDBpgUWnEajLjOnHhsleYKwdFkLqGLXzA9dNa+Yo7bfai5Ulj0mHk+baVJnUUMMjOChsSYz3bT+9D3LBPYPr/O/D2HzTDtF4Up5mQUHG0+h9yd5gRdsM+kwayBJhiGDzAN9P6hpoacsx2GXG4ez94Mo2+CXqebnlofz4C6cnNlFjvZfNaFKSYYB4SZYF1bCharuf/dveZ/4vZl5rP9+Epzcjv7zzD+TvP3VBbTyyt5IVz8b5MyXPY0LP67qcjMvQXWvmpOiCG9TLvR9i/M55Gzw6SDfINNLXne/aYiFNTTnAB/+D/TZVhZTHmnPmpGeWtt/gftNeZ/uarQ1LqHXGb+35c+aa5ug3uaK4WAMPO/Ouke034FcNrFJoCvfB7C+0PXPqZNLHI4XPeFqUBVFZjPf9P7R/2KK91K7k8p5QXsBc4DMoENwLVa691NtrkZSNBa//aoO2siISFBb9x4jP2nT3Jaa+Ieng9A2j8v4oxnFpNRVI1FweIHziI2PPCIr63fFiDYz5uyGjubHz3vyOmao4h9yOQyX79hDNOGRR7DO/EwW41ptGxykmpmz3fQe4L5gib9AD2GQtfepgePb7A50QSGmy94UyUZJmBlrIdN75mrhJyd4O0Lk34P2z83PSimPmKuJKqL4ZvfmoFggd3NlzAkxlx9pK82NdjeE8xrrP5w8fMm6IO5xM/ZYQJhYISphfsEmBNH0y6iYE4EvU43J8neE03Q2PYZjPuNCVgZ6+GqD03Qf+d8E0Su/sicyNa+YoL7d/eaE3J+kvksqgrNsUfdABe/aE52FbmmnHX18/groEmcUBbw8jVXcHUVcNmr5urF1qQdwifIVAyarsZ11YcmdfbSKBMAJ/3eBFztaNwmqKfZT3BP0xb06SxTRp8upsJQU+YqizIpvEM/IzDpten/NCev9101+FE3mhNB2gpXN19b43u4+AVTziX/MFdwZzxgTupNy2UNNFcTYAJ1ZZ5Jz5XnmIrFoZSXeX3cFPM5Z20xXZEBQnrDLfPMSejFEabdrNd4c9VYP8ePT5A54Xn5mBNGk7+Deqxsk9Y64fCDti2gTwD+prW+wHX/YQCt9VNNtrkZCejtsiQxj5yyGq4d15uLXlrBruwylIIZo2J4+sp49uVV8MayFO44qx+DI033xvzyWsb+4+eGfQyODCIxp5zFD5xJ34gu7S7DxKcWkV1aw9TB3Xn35rFue2+nLKfz8Etke60J+F16NKRPLvj3cmYlxPDrM/qabbR2tQH4mhOU1qZWW5xmAk5tuckLR8aDbxcz3sC7lRO4rQasfkd+vqbM1By1w1x51F+B5e4GtCsIbTY18+gx5n0UJJly9jrdlFFZTGD2CTDtAIUp4N/VnBB6xJsyFqWathlwTXNhMVctymI+j4wNJojlJ5na65BLzVWKxWpeX5BsgnDvCSZw7v7GBLnoMdD/PEhfafLgYPbrGwxjf22u6gB2f2t6f138b9MWBSYtU11kjtf0qnLfQlPGoB7mCsMv2KQWK3Jh0HTzmYBJn6173bTDdO1t3rPF25xwtdMMFExZbK6qRt1gjuF0mraiygLod3bj513fSK6U+Wxzd5uKwaDp5u+essh0bAh1jVXp1hc1+objCugzgWla61+77t8InN40eLsC+lNAPqY2f5/W+rBVHZRStwG3AfTu3XtMevpR1ts8hWzNKGFDahEZxVV8uCad6K7+dA/2ZcuBEny8Lfx835n0Dgvgy40ZPDh3OxcNj2Le9oMMiw5mZ1YZ/7trIqNbSdW0ZMhffqSqzkFEkC8b/nzuCXhn4lBaa/r9aT4zx8TwzMwRni6OOAkppY4Y0N3VbfE7IFZrPRxYCHzQ0kZa6ze11gla64SIiNYbAE8VI3t15TdT+vLIRUN4/qoRFFXWseVACTdPjMXp1Ly/Og2AORsz6BseyDNXDueG8b25/7yBAJQ2aSRtq+o6B1V1Dny8LBRU1GJzON35lsQR1NqdODWU17h5wJIQtK1RNAvo1eR+DI2NnwBorZuOZngbeOb4i3bq8fG2MGN0DFYvCx+tSeeB8wdSXFXHnI0ZRIb4siGtmIemDybQ15snLo8nq8Tk7rZklHD24O7tOlZhpWlIOy0qiG2ZpRRU1BIV4oauc0B5jY1AH28sbWmoPcVU1JpAXlYjq0wJ92tLDX0DMEApFaeU8gGuAb5tuoFSKqrJ3UuBDjo86+RwyYiezLljAkF+Vn43tT9Bft48OT+RwZFBXJ3QeG6N7urP9GGRvLk8hcziFuZrOYr6gUlDepr8fE5pjVvKXl3nYOI/F/O/LTIdb0uqak1DW1m11NCF+7Ua0LXWduC3wE+YQD1Ha71LKfW4UupS12b3KKV2KaW2AfcAN5+oAp9q+ncPYskfzuK7305m3j1nEHpIb5ZHLjY9PP4xz5xD69tE1u4vpKDikKXxmihsCOhmGbvcsrYF9OLKOp74fjc1NkeLz2eVVFNeY2fPwRaG3QupoYsTqk390LXW84H5hzz2lya3HwYePvR1wj38rF7Ex4S0+Fx0V39+e3Z/nluwlxvfWcfWAyVM7B/GT7tyOT2uG49dNpQeQX6HnQiKKlwBPap9NfTvt2fz9spUpgyMYEoLA6Hq91M/T41orrLOFdCrJaAL95O5XDqB30zpy1UJMeSX1zI4KoifduUS4OPFutQipr2wgts/2kRBRS2VtY2X+fU59P7du2D1UuSUmftOp6a67vDa986sUq59cy2LEvMASC2opKzGxr2fb2lWu89x3c4ucU8Kp57T2TkWg6hsqKHbaa2Hmei8iivrmPnaahJz3HslKwG9E/D19uKZmSP48fdTmHP7BJ6bNYLvfjeZKQMjGNMnlPVpRUx4ahFXvbGGzOIqqursFFbW4eNlIdjPm+5BfuSW1eB0ap5bkMSZzy6hzt6818u7q1JZs7+Qpa6FO1ILKlmdXMA3W7NZsKtxBfscV83cXTl5gNeWptD3T/OPmOY5mVS6cugOp6a6E7wfcWw+23CAjenFfLruQOsbt4PM5dLJKKWYOSYGgA9/NQ67w8msN9bg1LAto4TJTy8hIsiXQB8vugX6oJQiPMiXr7ZksTWjhLyyGirrHKxPLWLyADNRUFWdnR935jQ7TlphJV18zb/PjqzShsfra+h55TXYHU68vY6/zvD0j2YCo8ziKvp3Dzrqtg6n5vmFSdwwvs9x9dpJzisno6i63b2HWlOfcgHTMBrgI1/BU9GGVDONcmTIUQZ/HQOpoXdy3l4W/nfnRL65exJPXhHPPVP7Y7Uo0gqr6OLXPJhkFVdTWefA6qX4eU9uw+Nfbsykqs7BjNFmpF3f8EDSCiobAvmOrMbLxvqauVNDbvmRG2Xbqr5rJtAw5cHRJOaU8cqSFO74aFPDY0sS88grb98Vw8uLk7lvztZ2vaYtmqa9pGH01FRnd7Ihzaxc5e7eTlI9OAUo15Dz6043Q5dvPaMvC3bl0CfMzBnz+KVD2ZtbTlx4IIk55SxNymPupky2HCjGx9vC7uwyJvcP57mZI/jVpDh+3JnDa8tSzNS9wN7ccmpsDvysXhwsrcHX20Kt3cnBkmqiux5f3/YfXEv5AWS0oWtm/QllW6Y52VTU2vnVBxu4bUpfIrr4MqJXV8bGdmt1P9klNZRU2aiuc+Dv49Xq9m1V1aR9QhpGT02JOWUNvZ1K3fw/IAH9FBTib2VWk/7sI3p1ZUSvrgAkxHajX0QXqm0OrF4WiqtsBPtb+eeV8VgsimHRISTmlONwakqrbUzsF8bqlEIGP/ojY2ND2ZVdRkKfUDamF5Pthjz6mpRCYsMCyCmrIaOo9YDetEafXVJNXnktWsPenHLeSDKrDyX+fRp+1qMH6ez6toCyGuKOMllae1VIDf2Ul1nc+D/q7v8BCejiMBP6hTGhX9gRn0/oE0pksB91DicPnD+Q6nl76BHkx8Z0kxcc0jOYjenFvLI4mQ2pRcxKMDn9qBB/ugX6YFGNVw1H43Bq1qcVcVF8FBvTi9uUcslq8mVZvje/YbTq2v2NS7+9vzqNO87sd8R9OJ26oefOwdLqIwb01SkF3PXJZr7/3WRiQhvnyC6tsjH7vfU8NSOe01zdQus1Tbl8vj6D7kF+DItuuUuq6JzqBwH2iwh0+1WaBHTRbrHhgaz9U+MiCV/dNQmA9MJK7vlsCxfGR3FaVDAfr03nv5sz+WJDBnUOJ726mfRL724B/GpSHEWVdSTEdiMuPNA12rWaxy4d2hDs9xwso7zGzul9u5FbVsOBNtTQM4uriQ0LoLjKxrbMUoJc7QRNe5Ss3FfQENCf+H43y/flc1VCr4bZDwsqa7E5TJfCo/XW+XhtOiVVNr7Zms3dZ/dveHzN/kK2ZpSwfG9+CwHd0ZCSWrDbtFO8eVOL8yx1Kg6n5j+L93H12F5um2LiaBJzyvjdp1v4/LbxhHXxPeHHa4+s4mqCfL2JCQ2g+EgL1BwjCejCbfqEBfLNbycDML5vGNeO601hRS1/+HIbkSF+fLs1G38fL9akFLIq2Uz/Y/VSnD80knnbTa58bGw3Lh4eRXJeBf/8wfRuOT0ujM3pJSxJyicpp5xBkaanS43NQUZRFXd9spnzh/bgt2cPILOkmpjQAKJDNTuySog45Mt86YieLN+Xj9aaWruTj9amY3M4+ffCvVx/eh/8fbw42KQP/cEjBPTSahs/7zF98r/ffrBZQK9fvGR/fuVhr6ustRMV4kdaoTk57curOGybY6W15kBRVUPbyKHW7S8kPibEIz1rknLKeeHnfbyzMpUdf7vghB9v+d589uVVsCu7rMUBcJ6UVVJNdKg/If5W0gormbf9IJMHhBPibz3ufUsvF3FChXXx5b1bxvHUjOEsvP9MFj1wFh//+nTevimBn+8/k0uG92T53nxG9e7K4MggfvfZFsY/tYgZr61mW2YJ90ztT8+u/g2LflzwwnIWJ+by2tIUhv31J27/aBPphVW8ujSF699ey56DZUR39Sc+uitJOeXsyi5rWPyjZ4gf4+K6UVJlI7O4mjX7C6m1O7n9zH5U1jn4aksWaQWVpBU2BuIj1dB/3p1Lnd3J5SN7sudgGfvzGwPz1gzTgyG1oIWAXmdvNmo3vbDyuPrXV9c5GrqUrt1fxJnPLmVDWtFh2+WV1XDNW2v5aI1npqzOKTOpsPIaO6uTC9r9+tSCSnY26R7bmqQc8/domq8+UFjVIQZzZRZXE+MK6BlFVdz96WY+Xuuev4vU0MUvpqerx8vEfo0L4T5/9ciGL1leeS3zth9k6d58iivreO2G0Q256VkJMXQLtPLm8lR+8+EmHE5NsJ83+wsqefCCQfQND+TOTzYDENXVjwHdg7A5NHnltVwzthefb8igX/cuxLvy1fd+voXNB0qweinumTqAb7dm86evdjQrb/cgXw6W1pBaUMmWA8VM7h/OpvTihnVgwwJ9uPfcgXy9NZtVKYX0jeiCw6nZ4ephs7/g8Np3Za2dQB9v+rrypwUVdaTkVzC057Hl0eduyuDRb3Yx/54z2J5prgx+2JFzWE+exJxytIZd2Ucemeh0alQb2zfaq+nI4SVJeUzs377FkB/9eidZJdUs+cNZbdp+b2450Jiv3nygmBmvruala0dx6YjGxbLtDifTX1zBb6f257KR0SzcnUu/iED6RnShxubA26LcMpaintaarOJqxvcNo4uvN/UDoOv/dsdLaujC45RSKKXoEezHrybH8eGvxvHdIQ2NwX5WrhgVw2vXj+bK0dE8dulQlj14Nk/NiOfWyXFMj4/i4emDAegTFsDwJnPfPDR9MD5eFgZ0D2pI19SvzTp1cHf8fbz411Uj+OO0wfxx2uCG150WFczK5Hyuf2st98/ZxkX/Wcmdn2xmY3oxq5ILmNAvjNiwAKJC/FibYlJI2zNLqKxzMDgyiIKKumbd0goqaqmotRPo68XiB87i09+MB2BfbvPAX2NzUFV35P7Ju7JLefy73dgdzoaxAJsPFJPkCmKLEnMbTpIOp2ZpUl5DIE/KKW/YzyNf72CRa7yB1pqZr6/mka93tvLXMkqrbGxMK2LlvgJmvLrqsKuMshobf/9+N8WuSeAOllbjZVGMiAlpSEm1ldOp2ZZR0uarGYdTsy/PvM/6Xk/fbcsG4J2Vqc22zSiuZl9eBcuS8rE5nNz9yWae/SkJh1Nz+SurePSbtn0ebVVWbae81k50V3+C/Rvr0zuz3DMFgNTQxUklNjyw2Uo/147r3XD7til9mdAvjNOigrF6WXhm5nAm9w+na4APH//6dOLCA/GzenHblL6E+FuZOSaGQNdo1/F9wxjf1/TsWbg7h6o6BzGh/izb66Soqo6hPYPZlV2Gv9WLR7/eSV55LZP6h6OUYkLfMJbtNXn5d1el0cXXm9um9OX+OdsY8dgC/nrJEIbHhHDtm+uoczgZ5qqNx4YF4m1R7MouJbWgEh9vC1YvxStLUogK8WPePWe0uPj3e6vSmLspk37dA9ntmtVyy4ES9uVWYFGQXljF3twK+oQFcNO761mfWoSvt6m7peRXUGd3UlhZy8drD5BeWMU5p/Vgzf5CNh8o4UBRNU9cPqzVWvqrS5N5a8V+zj2tB5sPlJCYU85IV9dXgB935vDOylT251fw3i3jOFhaQ48gX0b3CeWz9QfaNYo4tbCSclfvoNSCysMamg+VUVRFjc1MXZFZXI3TqflhRw4BPl5syyhhW0ZJQzfdFFcbxp6ccvPZOJys2V/IDzsPkphT3mxkrzvUj6WIDvWnvEmXxaySagorao+7AVcCuug0lFIMj2kMKlc16Ws/Lq4xBfGnC0876n6+vGMiWmtyy2sZ3TuUcXHdCPa3sje3nBX7Cnhp0T6UgsmutMH4fmH8b0sWl7y8kt3ZZfzmjL7NlgR87LvdzfZf3xfdx9vCmD6hvLWiea0xPjqEHVmlfLUli5ljYnA4NX//fjdLk/K46+z+LNtr5tP598K9DSMNNx8o5mBpNZeO6Mn8HTl8ui6dUb1DWZ9aRIi/ldJqG0qB3alJya9oSElsSCui1u7g/VVpgLmK2JdXwcAeLU+xsGhPLsv35rMjqxSnpqGnzo6s0mYBvX765CVJ+ezILOVgSQ1RXf0Z2asr761KM3MA7c5h9sTYZim4ljRNRyTnVbQa0Hdmm6uWAd27kFlcxcb0YnLKavj7ZUN5/PvdzN9xsCGg16fFkvPK2e5KlZVU2fjtp1sAMzq5sKKWD9ek421R/O6cAUc9dmtSXG0t/SK6kOo6tlJmidYdWaWcNej4ppqQgC7EIUytWBHd1Z8rXfPigOmBMyKmKxP6hhEaaKVXN5MSmj4skt3ZZSTnVTBrTC/uOLMfoYE+zLl9AoMig1ialEdKXgXj4sK44Z11BDfpzfDW7AT+vXAvMaEBhAX6oBRcMrwnl7+6ij9/tYPVKQWk5FeyLaOE2LAA/m/udgBumtCHD10NnIN6BDWkWyb2C8diUXy5KZPdrgbiG8b34ekfExkfF8aa/YV8si6dfNe0DDU2J99tO8jCPblcPrInX2/NZnVyAQN7BJFVUk2NzUE/1wLkWmue+iGR5DxzJdDUrkMaLLdnljKgexfKamzcN2crlbV2xvQJbTjRPfDlNsB04zxSQF+VXMBrS1NY6WpEVaoxIB7NFxsy6BHsy7RhkfxncTLvrUolyM+bK8fEsGB3Lgt35/Kw66Re3xPJ5tB8v/0g3haF3ZXYvnVyHO+sTOWz9Qd4afE+/Ly9uPWMuOPqJZSUU463RREXHtgw4+m42G5sSi9mxb6CZgH9p1059IvoQv/ubV8AXgK6EO3g4205bNBVkJ+Vv1069LBt668KLhsZ3fDYj78/o6FxGEzbwF8vOfy1b96YwLM/JbFsbz7Bft48N2sEl47oyaw31rA9s4TfTu1PabXpA//H6YO49YONaG0GdQ2LDuG7bdlsSCvmjjP7cfHwKP69cC9XjI6muKqOj9eaGf7io0PYlV3KY9/twqIUf5w+mC0ZJby2LAVfqxcv/GyuAF64ZiRTBkSwKb2YZFeKwqnB3+pFtc1BkJ93swna7A4nu7JLuXZcb84e1J2b3l0PQFSIH726BfDa9aOpqLWzM6uUj9amc8A1r1BogJU6hxOrxcKWjGJueW8DEUG+9Aj2ZVSvUHYdLG04/itLklmSmMdnt43H2iR1k5xXwYp9Bdx/3sCGaSd+2JnDzRNjCfDx5vwhPXj0m118ty2bqYO7k5JfQbdAH4oq61i+N59h0cH0i+jC4MhgbprQh3dWpvLcgr34eFmotjlYnJjHxcMbG1XB9IT6yzc7Sc6r4N2bxzb0yGrJ3twK4sID8fG2EOxnTuzDokMI6+LDfzdn8uAFg1i4O7dhTqK48EB++v0UfLzblp5SnurGk5CQoDdu3OiRYwtxsiqsqG3oW11SVcfSpHwuG9mTGpuTPTllDTXgnVmlvLsqlQcvGERUiD+5ZTVEdPHFYlHszCrl79/v5uaJsezNreDlJfu4eHhP/n31SLZmlPDnr3awK7sMHy8LUV39SC+sahjdGxpgpW9EF9anFnHfuQN5b3Uql43oycfrDnDGgHBiwwJJLahk2d58Xrh6JJePimbGq6vYfKCE3587gN+fO7DhvaTkV3DOv5Y13B8X243UwkpiwwJILTCzeX5116SGbp6/en8Dy/bmc1pUEHsOmukn/jhtMGFdfAj2sxLg48XbK1PZmFbEsgfPpsbm4M5PNlFV5+Dd2SbQ5pXVMP3FFRRW1jXUxmeMimbp3nyKKuuYNSaGZ2c1ttFc/J8V7Mwq44NfjeMPX26jzu7kzIERXH96bywWxdjYbvz6g42sSi7A12ohwOrFF7dPoFe3AEqrbPzzx0TWpBTwr6tGMKZPN858dgnDeobwyvWjyS2r4fQnF/HkFfH0CQvg+rfXcWF8JPN3mG6ofSMC2Z9fyX3nDuTecxtTPUqpTVrrFkejSUAX4hRXVFlHgI9Xw/w2Tqfmh505BPl5M7pPKGtSCtmeWYICrhrbi7zyWlbsLeCec/pTa3eyN7ecv367i+o6B/sLKukV6k8XPytv35RARJAv2zJKuOyVVcy5fUKztgyAd1emUl5jR6N5bWkKoQE+5JbXNATzpumGHZmlfLM1i4V7zBgAf6sX+1vo6/+3S4Zw86S4I77fqjo7m9NLmLcjm8/WZ/DkFfFM7h/OvB0HOW9I92ZTNBdW1OLtZSHE38rXW7L4fvtBliTl4XClZS4Y2oOfduVy7zkDuGBoJNe+tZYuvt68eM1IXvh5H+tSC+ka4ENNnYO3Zidw7Vtrufec5yFIuAAABn9JREFUxhPb2v2FjOrdFavFwu0fb2Lh7lxOiwrmletG0ScskPvnbOWbrdncf95ASqttbD5QzNd3T5aALoTwHIdTt9hjp6mc0hqC/LzZcqCE0EDrEfvmO5wam8PJpvRivtqSxc0TYwEzB39qQRU3T4xt9Vj1CipqCQ3wafP2YCaM219QwcGSGt5csR8vpVj10FS6BfqwM6uUX3+wsWFdgGeuHM4ZA8O5+o21ZJdUY3dqXr1+NBfGRx22X6dT8+22bMbGdWtIF9XaHfz20y0s3J2Ll0UxLrYbn98+QQK6EEK4W365GVvQdAK3gopaFu3JpXe3wIb2lqySal5dkky3QB/uPKtfuxtW9+WW4+/jRUxogKRchBCiszhaQJeRokII0Um0KaArpaYppZKUUslKqYdaeN5XKfWF6/l1SqlYdxdUCCHE0bUa0JVSXsArwHRgCHCtUmrIIZvdChRrrfsD/waedndBhRBCHF1baujjgGSt9X6tdR3wOXDZIdtcBnzguj0XOEediCnbhBBCHFFbAno0kNHkfqbrsRa30VrbgVLgsDXMlFK3KaU2KqU25ufnH1uJhRBCtOgXbRTVWr+ptU7QWidERHSsVUSEEOJk15aAngX0anI/xvVYi9sopbyBEKDQHQUUQgjRNm0J6BuAAUqpOKWUD3AN8O0h23wLzHbdngks1h1hrSchhDiFtGlgkVLqQuAFwAv4//bOJjTOKgrDz0tNG7GFWiqlC5FEBOlC2iBSoXQhqJhNdZeVLgTBH9CFi0pBWsGFgi4EsSgGtIh/qNiNYJWAK1uiJm1qqU21m1KbhVTrRkSPi3uCk2S+yTRt5s69nAeGuXO/IXnf78ycmXvv3PONm9mLkl4AJs3ssKRB4BCwA/gNGDOzn5f5m5eB01droI/YDFz5xRL7k5q8QF1+avICdfnplZdbzKztnHW2naKSJpt2O5VITX5q8gJ1+anJC9Tlpx+8xE7RIAiCSoiEHgRBUAk5E/qbGf/3alCTn5q8QF1+avICdfnJ7iXbHHoQBEFwbYkplyAIgkqIhB4EQVAJWRL6cuV4+x1J5ySdkDQladL7Nkk6IumM39+YW2cTksYlzUmaaelrq1+J1zxWxyWN5FO+lAYv+yWd9/hM+T6K+WPPuZfTku7Po7oZSTdLmpD0o6STkp72/uLi08FLkfGRNCjpmKRp93PA+4e8bPislxFf6/29LytuZj29kTYnnQWGgbXANLCt1zqu0sM5YPOivpeBvd7eC7yUW2cH/buBEWBmOf3AKPAFIGAncDS3/i687AeebfPcbf56WwcM+etwTW4PizRuBUa8vQH4yXUXF58OXoqMj5/j9d4eAI76Of+ItJkS4CDwuLefAA56ewz4cLU15viG3k053hJpLSH8DvBgRi0dMbNvSDt6W2nSvwd41xLfAhslLb3CbSYavDSxB/jAzP4ys1+AWdLrsW8wswtm9r23LwOnSNVMi4tPBy9N9HV8/Bz/6Q8H/GbAPaSy4bA0Nj0tK54joXdTjrffMeBLSd9Jesz7tpjZBW//CmzJI23FNOkvNV5P+RTEeMv0V1FefIi+g/RNsOj4LPIChcZH0hpJU8AccIQ0irhkqWw4LNTcVVnxa0ksiq6MXWY2QrqK05OSdrcetDTGKvb3oKXrB94AbgW2AxeAV/LKuXIkrQc+AZ4xsz9aj5UWnzZeio2Pmf1jZttJVWfvAm7PLGkBORJ6N+V4+xozO+/3c8BnpMBenB/q+v1cPoUrokl/cfEys4v+xvsXeIv/h+1FeJE0QEqA75nZp95dZHzaeSk9PgBmdgmYAO4mTXNd54daNfe8rHiOhN5NOd6+RdINkjbMt4H7gBkWlhB+BPg8j8IV06T/MPCw/5piJ/B7y9C/L1k0h/wQKT6QvIz5rw+GgNuAY73W1wmfY30bOGVmr7YcKi4+TV5KjY+kmyRt9Pb1wL2kdYEJUtlwWBqb3pYVz7RaPEpa8T4L7Muh4Sq0D5NW4qeBk/P6SXNjXwNngK+ATbm1dvDwPmmo+zdpzu/RJv2klf3XPVYngDtz6+/CyyHXepz0ptra8vx97uU08EBu/W387CJNpxwHpvw2WmJ8OngpMj7AHcAPrnsGeN77h0kfPLPAx8A67x/0x7N+fHi1NcbW/yAIgkqIRdEgCIJKiIQeBEFQCZHQgyAIKiESehAEQSVEQg+CIKiESOhBEASVEAk9CIKgEv4DA9X3/9Gor+kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['rmse', 'val_rmse']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5747852737384287"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06382945181346295"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_rmse.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>valid_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>valid_r2</th>\n",
       "      <th>test_r2</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ESOL</td>\n",
       "      <td>0.146218</td>\n",
       "      <td>0.523291</td>\n",
       "      <td>0.647990</td>\n",
       "      <td>0.996040</td>\n",
       "      <td>0.932177</td>\n",
       "      <td>0.889020</td>\n",
       "      <td>803681</td>\n",
       "      <td>275</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ESOL</td>\n",
       "      <td>0.126394</td>\n",
       "      <td>0.585173</td>\n",
       "      <td>0.545599</td>\n",
       "      <td>0.996736</td>\n",
       "      <td>0.933716</td>\n",
       "      <td>0.935269</td>\n",
       "      <td>803681</td>\n",
       "      <td>275</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ESOL</td>\n",
       "      <td>0.207896</td>\n",
       "      <td>0.631244</td>\n",
       "      <td>0.530767</td>\n",
       "      <td>0.996399</td>\n",
       "      <td>0.902215</td>\n",
       "      <td>0.935536</td>\n",
       "      <td>803681</td>\n",
       "      <td>275</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_rmse  valid_rmse  test_rmse  train_r2  valid_r2   test_r2  \\\n",
       "0      ESOL    0.146218    0.523291   0.647990  0.996040  0.932177  0.889020   \n",
       "1      ESOL    0.126394    0.585173   0.545599  0.996736  0.933716  0.935269   \n",
       "2      ESOL    0.207896    0.631244   0.530767  0.996399  0.902215  0.935536   \n",
       "\n",
       "   # trainable params  best_epoch  batch_size      lr  weight_decay  \n",
       "0              803681         275         128  0.0001             0  \n",
       "1              803681         275         128  0.0001             0  \n",
       "2              803681         275         128  0.0001             0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
