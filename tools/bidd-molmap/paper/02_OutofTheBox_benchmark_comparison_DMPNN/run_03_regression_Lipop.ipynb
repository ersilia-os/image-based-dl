{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: Lipop number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'Lipop'\n",
    "\n",
    "from chembench import load_data\n",
    "\n",
    "df, induces = load_data(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 4200/4200 [12:31<00:00,  5.59it/s]\n",
      "100%|##########| 4200/4200 [01:31<00:00, 45.78it/s]\n"
     ]
    }
   ],
   "source": [
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').values\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256, 128, 32]\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "loss = 'mse'\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = 'linear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360 420 420\n",
      "WARNING:tensorflow:From /home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "epoch: 0001, loss: 2.3800 - val_loss: 1.4134; rmse: 1.1856 - rmse_val: 1.1889;  r2: 0.0421 - r2_val: 0.0428                                                                                                    \n",
      "epoch: 0002, loss: 1.4040 - val_loss: 1.3833; rmse: 1.1729 - rmse_val: 1.1761;  r2: 0.0773 - r2_val: 0.0762                                                                                                    \n",
      "epoch: 0003, loss: 1.3613 - val_loss: 1.3479; rmse: 1.1576 - rmse_val: 1.1610;  r2: 0.1077 - r2_val: 0.1032                                                                                                    \n",
      "epoch: 0004, loss: 1.3349 - val_loss: 1.3194; rmse: 1.1441 - rmse_val: 1.1486;  r2: 0.1338 - r2_val: 0.1261                                                                                                    \n",
      "epoch: 0005, loss: 1.3011 - val_loss: 1.2883; rmse: 1.1289 - rmse_val: 1.1350;  r2: 0.1641 - r2_val: 0.1528                                                                                                    \n",
      "epoch: 0006, loss: 1.2581 - val_loss: 1.2485; rmse: 1.1093 - rmse_val: 1.1174;  r2: 0.1975 - r2_val: 0.1823                                                                                                    \n",
      "epoch: 0007, loss: 1.2171 - val_loss: 1.2391; rmse: 1.1041 - rmse_val: 1.1132;  r2: 0.2509 - r2_val: 0.2327                                                                                                    \n",
      "epoch: 0008, loss: 1.1554 - val_loss: 1.1279; rmse: 1.0485 - rmse_val: 1.0620;  r2: 0.2932 - r2_val: 0.2732                                                                                                    \n",
      "epoch: 0009, loss: 1.0639 - val_loss: 1.0393; rmse: 1.0101 - rmse_val: 1.0194;  r2: 0.3615 - r2_val: 0.3503                                                                                                    \n",
      "epoch: 0010, loss: 0.9639 - val_loss: 0.8792; rmse: 0.9346 - rmse_val: 0.9377;  r2: 0.4274 - r2_val: 0.4325                                                                                                    \n",
      "epoch: 0011, loss: 0.8514 - val_loss: 0.7870; rmse: 0.8803 - rmse_val: 0.8871;  r2: 0.4835 - r2_val: 0.4835                                                                                                    \n",
      "epoch: 0012, loss: 0.7589 - val_loss: 0.7340; rmse: 0.8518 - rmse_val: 0.8568;  r2: 0.5320 - r2_val: 0.5303                                                                                                    \n",
      "epoch: 0013, loss: 0.6940 - val_loss: 0.7648; rmse: 0.8539 - rmse_val: 0.8745;  r2: 0.5677 - r2_val: 0.5527                                                                                                    \n",
      "epoch: 0014, loss: 0.6467 - val_loss: 0.7390; rmse: 0.8326 - rmse_val: 0.8596;  r2: 0.6072 - r2_val: 0.5787                                                                                                    \n",
      "epoch: 0015, loss: 0.6098 - val_loss: 0.6141; rmse: 0.7435 - rmse_val: 0.7836;  r2: 0.6325 - r2_val: 0.5928                                                                                                    \n",
      "epoch: 0016, loss: 0.5551 - val_loss: 0.6312; rmse: 0.7450 - rmse_val: 0.7945;  r2: 0.6587 - r2_val: 0.6101                                                                                                    \n",
      "epoch: 0017, loss: 0.5347 - val_loss: 0.5841; rmse: 0.7052 - rmse_val: 0.7642;  r2: 0.6806 - r2_val: 0.6225                                                                                                    \n",
      "epoch: 0018, loss: 0.4770 - val_loss: 0.5420; rmse: 0.6671 - rmse_val: 0.7362;  r2: 0.7002 - r2_val: 0.6348                                                                                                    \n",
      "epoch: 0019, loss: 0.4634 - val_loss: 0.6136; rmse: 0.7084 - rmse_val: 0.7833;  r2: 0.7197 - r2_val: 0.6451                                                                                                    \n",
      "epoch: 0020, loss: 0.4269 - val_loss: 0.5485; rmse: 0.6508 - rmse_val: 0.7406;  r2: 0.7392 - r2_val: 0.6534                                                                                                    \n",
      "epoch: 0021, loss: 0.4350 - val_loss: 0.5694; rmse: 0.6606 - rmse_val: 0.7546;  r2: 0.7512 - r2_val: 0.6617                                                                                                    \n",
      "epoch: 0022, loss: 0.3922 - val_loss: 0.5165; rmse: 0.6098 - rmse_val: 0.7187;  r2: 0.7670 - r2_val: 0.6682                                                                                                    \n",
      "epoch: 0023, loss: 0.3727 - val_loss: 0.5144; rmse: 0.6057 - rmse_val: 0.7172;  r2: 0.7783 - r2_val: 0.6772                                                                                                    \n",
      "epoch: 0024, loss: 0.3447 - val_loss: 0.4644; rmse: 0.5568 - rmse_val: 0.6815;  r2: 0.7889 - r2_val: 0.6834                                                                                                    \n",
      "epoch: 0025, loss: 0.3320 - val_loss: 0.5120; rmse: 0.5817 - rmse_val: 0.7155;  r2: 0.8021 - r2_val: 0.6835                                                                                                    \n",
      "epoch: 0026, loss: 0.3003 - val_loss: 0.4632; rmse: 0.5333 - rmse_val: 0.6806;  r2: 0.8128 - r2_val: 0.6905                                                                                                    \n",
      "epoch: 0027, loss: 0.2932 - val_loss: 0.5098; rmse: 0.5726 - rmse_val: 0.7140;  r2: 0.8242 - r2_val: 0.7000                                                                                                    \n",
      "epoch: 0028, loss: 0.2933 - val_loss: 0.5221; rmse: 0.5795 - rmse_val: 0.7226;  r2: 0.8287 - r2_val: 0.7009                                                                                                    \n",
      "epoch: 0029, loss: 0.2814 - val_loss: 0.4345; rmse: 0.4876 - rmse_val: 0.6592;  r2: 0.8382 - r2_val: 0.7029                                                                                                    \n",
      "epoch: 0030, loss: 0.2685 - val_loss: 0.4529; rmse: 0.5006 - rmse_val: 0.6730;  r2: 0.8480 - r2_val: 0.7102                                                                                                    \n",
      "epoch: 0031, loss: 0.2436 - val_loss: 0.4232; rmse: 0.4632 - rmse_val: 0.6506;  r2: 0.8552 - r2_val: 0.7115                                                                                                    \n",
      "epoch: 0032, loss: 0.2351 - val_loss: 0.4201; rmse: 0.4523 - rmse_val: 0.6482;  r2: 0.8651 - r2_val: 0.7171                                                                                                    \n",
      "epoch: 0033, loss: 0.2220 - val_loss: 0.4176; rmse: 0.4423 - rmse_val: 0.6462;  r2: 0.8683 - r2_val: 0.7159                                                                                                    \n",
      "epoch: 0034, loss: 0.2178 - val_loss: 0.5009; rmse: 0.5222 - rmse_val: 0.7078;  r2: 0.8746 - r2_val: 0.7173                                                                                                    \n",
      "epoch: 0035, loss: 0.2063 - val_loss: 0.5045; rmse: 0.5168 - rmse_val: 0.7103;  r2: 0.8798 - r2_val: 0.7167                                                                                                    \n",
      "epoch: 0036, loss: 0.2196 - val_loss: 0.4119; rmse: 0.4175 - rmse_val: 0.6418;  r2: 0.8873 - r2_val: 0.7233                                                                                                    \n",
      "epoch: 0037, loss: 0.1766 - val_loss: 0.4034; rmse: 0.3981 - rmse_val: 0.6352;  r2: 0.8954 - r2_val: 0.7264                                                                                                    \n",
      "epoch: 0038, loss: 0.2030 - val_loss: 0.3975; rmse: 0.3921 - rmse_val: 0.6304;  r2: 0.8988 - r2_val: 0.7308                                                                                                    \n",
      "epoch: 0039, loss: 0.1579 - val_loss: 0.3949; rmse: 0.3785 - rmse_val: 0.6284;  r2: 0.9035 - r2_val: 0.7305                                                                                                    \n",
      "epoch: 0040, loss: 0.1542 - val_loss: 0.4091; rmse: 0.3859 - rmse_val: 0.6396;  r2: 0.9098 - r2_val: 0.7312                                                                                                    \n",
      "epoch: 0041, loss: 0.1525 - val_loss: 0.4578; rmse: 0.4321 - rmse_val: 0.6766;  r2: 0.9132 - r2_val: 0.7287                                                                                                    \n",
      "epoch: 0042, loss: 0.1448 - val_loss: 0.4383; rmse: 0.4066 - rmse_val: 0.6620;  r2: 0.9178 - r2_val: 0.7316                                                                                                    \n",
      "epoch: 0043, loss: 0.1445 - val_loss: 0.3979; rmse: 0.3511 - rmse_val: 0.6308;  r2: 0.9216 - r2_val: 0.7332                                                                                                    \n",
      "epoch: 0044, loss: 0.1398 - val_loss: 0.3985; rmse: 0.3383 - rmse_val: 0.6313;  r2: 0.9267 - r2_val: 0.7303                                                                                                    \n",
      "epoch: 0045, loss: 0.1209 - val_loss: 0.4303; rmse: 0.3772 - rmse_val: 0.6560;  r2: 0.9281 - r2_val: 0.7307                                                                                                    \n",
      "epoch: 0046, loss: 0.1144 - val_loss: 0.6420; rmse: 0.5972 - rmse_val: 0.8013;  r2: 0.9313 - r2_val: 0.7369                                                                                                    \n",
      "epoch: 0047, loss: 0.1678 - val_loss: 0.4240; rmse: 0.3631 - rmse_val: 0.6512;  r2: 0.9331 - r2_val: 0.7300                                                                                                    \n",
      "epoch: 0048, loss: 0.1182 - val_loss: 0.3877; rmse: 0.3009 - rmse_val: 0.6226;  r2: 0.9394 - r2_val: 0.7341                                                                                                    \n",
      "epoch: 0049, loss: 0.1009 - val_loss: 0.3879; rmse: 0.2963 - rmse_val: 0.6228;  r2: 0.9429 - r2_val: 0.7360                                                                                                    \n",
      "epoch: 0050, loss: 0.0910 - val_loss: 0.3888; rmse: 0.2809 - rmse_val: 0.6235;  r2: 0.9465 - r2_val: 0.7328                                                                                                    \n",
      "epoch: 0051, loss: 0.0996 - val_loss: 0.4309; rmse: 0.3469 - rmse_val: 0.6564;  r2: 0.9476 - r2_val: 0.7331                                                                                                    \n",
      "epoch: 0052, loss: 0.0953 - val_loss: 0.4130; rmse: 0.3124 - rmse_val: 0.6427;  r2: 0.9507 - r2_val: 0.7343                                                                                                    \n",
      "epoch: 0053, loss: 0.0885 - val_loss: 0.4116; rmse: 0.2966 - rmse_val: 0.6416;  r2: 0.9542 - r2_val: 0.7317                                                                                                    \n",
      "epoch: 0054, loss: 0.0783 - val_loss: 0.3949; rmse: 0.2697 - rmse_val: 0.6284;  r2: 0.9554 - r2_val: 0.7333                                                                                                    \n",
      "epoch: 0055, loss: 0.0819 - val_loss: 0.3919; rmse: 0.2512 - rmse_val: 0.6260;  r2: 0.9592 - r2_val: 0.7324                                                                                                    \n",
      "epoch: 0056, loss: 0.0741 - val_loss: 0.4437; rmse: 0.3431 - rmse_val: 0.6661;  r2: 0.9600 - r2_val: 0.7362                                                                                                    \n",
      "epoch: 0057, loss: 0.0700 - val_loss: 0.4873; rmse: 0.3876 - rmse_val: 0.6980;  r2: 0.9598 - r2_val: 0.7292                                                                                                    \n",
      "epoch: 0058, loss: 0.0760 - val_loss: 0.3977; rmse: 0.2543 - rmse_val: 0.6307;  r2: 0.9641 - r2_val: 0.7356                                                                                                    \n",
      "epoch: 0059, loss: 0.0571 - val_loss: 0.3965; rmse: 0.2319 - rmse_val: 0.6297;  r2: 0.9660 - r2_val: 0.7292                                                                                                    \n",
      "epoch: 0060, loss: 0.0581 - val_loss: 0.3873; rmse: 0.2143 - rmse_val: 0.6223;  r2: 0.9692 - r2_val: 0.7337                                                                                                    \n",
      "epoch: 0061, loss: 0.0497 - val_loss: 0.3892; rmse: 0.2066 - rmse_val: 0.6238;  r2: 0.9708 - r2_val: 0.7322                                                                                                    \n",
      "epoch: 0062, loss: 0.0498 - val_loss: 0.4049; rmse: 0.2352 - rmse_val: 0.6363;  r2: 0.9720 - r2_val: 0.7316                                                                                                    \n",
      "epoch: 0063, loss: 0.0522 - val_loss: 0.4153; rmse: 0.2588 - rmse_val: 0.6445;  r2: 0.9736 - r2_val: 0.7339                                                                                                    \n",
      "epoch: 0064, loss: 0.0479 - val_loss: 0.3954; rmse: 0.2055 - rmse_val: 0.6288;  r2: 0.9735 - r2_val: 0.7300                                                                                                    \n",
      "epoch: 0065, loss: 0.0497 - val_loss: 0.4372; rmse: 0.2940 - rmse_val: 0.6612;  r2: 0.9746 - r2_val: 0.7350                                                                                                    \n",
      "epoch: 0066, loss: 0.0953 - val_loss: 0.3930; rmse: 0.2005 - rmse_val: 0.6269;  r2: 0.9728 - r2_val: 0.7297                                                                                                    \n",
      "epoch: 0067, loss: 0.0496 - val_loss: 0.4042; rmse: 0.2225 - rmse_val: 0.6357;  r2: 0.9767 - r2_val: 0.7322                                                                                                    \n",
      "epoch: 0068, loss: 0.0405 - val_loss: 0.3975; rmse: 0.1988 - rmse_val: 0.6304;  r2: 0.9783 - r2_val: 0.7323                                                                                                    \n",
      "epoch: 0069, loss: 0.0376 - val_loss: 0.3952; rmse: 0.1903 - rmse_val: 0.6287;  r2: 0.9798 - r2_val: 0.7334                                                                                                    \n",
      "epoch: 0070, loss: 0.0388 - val_loss: 0.4481; rmse: 0.2890 - rmse_val: 0.6694;  r2: 0.9799 - r2_val: 0.7294                                                                                                    \n",
      "epoch: 0071, loss: 0.0576 - val_loss: 0.3910; rmse: 0.1752 - rmse_val: 0.6253;  r2: 0.9794 - r2_val: 0.7310                                                                                                    \n",
      "epoch: 0072, loss: 0.0493 - val_loss: 0.3895; rmse: 0.1671 - rmse_val: 0.6241;  r2: 0.9810 - r2_val: 0.7319                                                                                                    \n",
      "epoch: 0073, loss: 0.0352 - val_loss: 0.3986; rmse: 0.1781 - rmse_val: 0.6314;  r2: 0.9819 - r2_val: 0.7294                                                                                                    \n",
      "epoch: 0074, loss: 0.0305 - val_loss: 0.4105; rmse: 0.2159 - rmse_val: 0.6407;  r2: 0.9833 - r2_val: 0.7329                                                                                                    \n",
      "epoch: 0075, loss: 0.0411 - val_loss: 0.3893; rmse: 0.1684 - rmse_val: 0.6240;  r2: 0.9835 - r2_val: 0.7352                                                                                                    \n",
      "epoch: 0076, loss: 0.0416 - val_loss: 0.3922; rmse: 0.1617 - rmse_val: 0.6262;  r2: 0.9840 - r2_val: 0.7321                                                                                                    \n",
      "epoch: 0077, loss: 0.0284 - val_loss: 0.4037; rmse: 0.1841 - rmse_val: 0.6354;  r2: 0.9853 - r2_val: 0.7304                                                                                                    \n",
      "epoch: 0078, loss: 0.0336 - val_loss: 0.4030; rmse: 0.1709 - rmse_val: 0.6349;  r2: 0.9863 - r2_val: 0.7287                                                                                                    \n",
      "epoch: 0079, loss: 0.0387 - val_loss: 0.4005; rmse: 0.1726 - rmse_val: 0.6328;  r2: 0.9856 - r2_val: 0.7305                                                                                                    \n",
      "epoch: 0080, loss: 0.0331 - val_loss: 0.3956; rmse: 0.1616 - rmse_val: 0.6289;  r2: 0.9868 - r2_val: 0.7325                                                                                                    \n",
      "epoch: 0081, loss: 0.0278 - val_loss: 0.4233; rmse: 0.2288 - rmse_val: 0.6506;  r2: 0.9871 - r2_val: 0.7321                                                                                                    \n",
      "epoch: 0082, loss: 0.0299 - val_loss: 0.3968; rmse: 0.1457 - rmse_val: 0.6299;  r2: 0.9875 - r2_val: 0.7290                                                                                                    \n",
      "epoch: 0083, loss: 0.0212 - val_loss: 0.4134; rmse: 0.1934 - rmse_val: 0.6430;  r2: 0.9885 - r2_val: 0.7309                                                                                                    \n",
      "epoch: 0084, loss: 0.0280 - val_loss: 0.4484; rmse: 0.2735 - rmse_val: 0.6696;  r2: 0.9882 - r2_val: 0.7315                                                                                                    \n",
      "epoch: 0085, loss: 0.0293 - val_loss: 0.3918; rmse: 0.1290 - rmse_val: 0.6259;  r2: 0.9888 - r2_val: 0.7305                                                                                                    \n",
      "epoch: 0086, loss: 0.0194 - val_loss: 0.3936; rmse: 0.1234 - rmse_val: 0.6274;  r2: 0.9901 - r2_val: 0.7295                                                                                                    \n",
      "epoch: 0087, loss: 0.0175 - val_loss: 0.4009; rmse: 0.1353 - rmse_val: 0.6332;  r2: 0.9905 - r2_val: 0.7271                                                                                                    \n",
      "epoch: 0088, loss: 0.0184 - val_loss: 0.4174; rmse: 0.1843 - rmse_val: 0.6461;  r2: 0.9903 - r2_val: 0.7257                                                                                                    \n",
      "epoch: 0089, loss: 0.0237 - val_loss: 0.3991; rmse: 0.1200 - rmse_val: 0.6317;  r2: 0.9908 - r2_val: 0.7260                                                                                                    \n",
      "epoch: 0090, loss: 0.0168 - val_loss: 0.3997; rmse: 0.1178 - rmse_val: 0.6322;  r2: 0.9916 - r2_val: 0.7262                                                                                                    \n",
      "epoch: 0091, loss: 0.0147 - val_loss: 0.3929; rmse: 0.1086 - rmse_val: 0.6268;  r2: 0.9920 - r2_val: 0.7298                                                                                                    \n",
      "epoch: 0092, loss: 0.0149 - val_loss: 0.3986; rmse: 0.1083 - rmse_val: 0.6313;  r2: 0.9923 - r2_val: 0.7261                                                                                                    \n",
      "epoch: 0093, loss: 0.0236 - val_loss: 0.4180; rmse: 0.1714 - rmse_val: 0.6465;  r2: 0.9917 - r2_val: 0.7246                                                                                                    \n",
      "epoch: 0094, loss: 0.0157 - val_loss: 0.4224; rmse: 0.1912 - rmse_val: 0.6499;  r2: 0.9922 - r2_val: 0.7265                                                                                                    \n",
      "epoch: 0095, loss: 0.0245 - val_loss: 0.4155; rmse: 0.1829 - rmse_val: 0.6446;  r2: 0.9918 - r2_val: 0.7288                                                                                                    \n",
      "epoch: 0096, loss: 0.0194 - val_loss: 0.4281; rmse: 0.2008 - rmse_val: 0.6543;  r2: 0.9920 - r2_val: 0.7259                                                                                                    \n",
      "epoch: 0097, loss: 0.0242 - val_loss: 0.4098; rmse: 0.1492 - rmse_val: 0.6401;  r2: 0.9924 - r2_val: 0.7265                                                                                                    \n",
      "epoch: 0098, loss: 0.0169 - val_loss: 0.4059; rmse: 0.1473 - rmse_val: 0.6371;  r2: 0.9929 - r2_val: 0.7286                                                                                                    \n",
      "epoch: 0099, loss: 0.0215 - val_loss: 0.4099; rmse: 0.1678 - rmse_val: 0.6402;  r2: 0.9928 - r2_val: 0.7298                                                                                                    \n",
      "epoch: 0100, loss: 0.0224 - val_loss: 0.4081; rmse: 0.1756 - rmse_val: 0.6388;  r2: 0.9918 - r2_val: 0.7320                                                                                                    \n",
      "epoch: 0101, loss: 0.0166 - val_loss: 0.4212; rmse: 0.1878 - rmse_val: 0.6490;  r2: 0.9933 - r2_val: 0.7279                                                                                                    \n",
      "epoch: 0102, loss: 0.0298 - val_loss: 0.3950; rmse: 0.1042 - rmse_val: 0.6285;  r2: 0.9927 - r2_val: 0.7283                                                                                                    \n",
      "epoch: 0103, loss: 0.0221 - val_loss: 0.4054; rmse: 0.1400 - rmse_val: 0.6367;  r2: 0.9926 - r2_val: 0.7272                                                                                                    \n",
      "epoch: 0104, loss: 0.0132 - val_loss: 0.4152; rmse: 0.1634 - rmse_val: 0.6444;  r2: 0.9939 - r2_val: 0.7266                                                                                                    \n",
      "epoch: 0105, loss: 0.0131 - val_loss: 0.3985; rmse: 0.0935 - rmse_val: 0.6313;  r2: 0.9940 - r2_val: 0.7259                                                                                                    \n",
      "epoch: 0106, loss: 0.0109 - val_loss: 0.4126; rmse: 0.1338 - rmse_val: 0.6423;  r2: 0.9943 - r2_val: 0.7233                                                                                                    \n",
      "epoch: 0107, loss: 0.0153 - val_loss: 0.4022; rmse: 0.0930 - rmse_val: 0.6342;  r2: 0.9944 - r2_val: 0.7234                                                                                                    \n",
      "epoch: 0108, loss: 0.0181 - val_loss: 0.4011; rmse: 0.0964 - rmse_val: 0.6333;  r2: 0.9939 - r2_val: 0.7244                                                                                                    \n",
      "epoch: 0109, loss: 0.0127 - val_loss: 0.4012; rmse: 0.0910 - rmse_val: 0.6334;  r2: 0.9944 - r2_val: 0.7240                                                                                                    \n",
      "epoch: 0110, loss: 0.0191 - val_loss: 0.4022; rmse: 0.1093 - rmse_val: 0.6342;  r2: 0.9942 - r2_val: 0.7255                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00110: early stopping\n",
      "3360 420 420\n",
      "Train on 3360 samples, validate on 420 samples\n",
      "Epoch 1/60\n",
      "3360/3360 [==============================] - 2s 614us/sample - loss: 2.3370 - val_loss: 1.4689\n",
      "Epoch 2/60\n",
      "3360/3360 [==============================] - 1s 253us/sample - loss: 1.3756 - val_loss: 1.3921\n",
      "Epoch 3/60\n",
      "3360/3360 [==============================] - 1s 252us/sample - loss: 1.3134 - val_loss: 1.3719\n",
      "Epoch 4/60\n",
      "3360/3360 [==============================] - 1s 248us/sample - loss: 1.2625 - val_loss: 1.3425\n",
      "Epoch 5/60\n",
      "3360/3360 [==============================] - 1s 251us/sample - loss: 1.1990 - val_loss: 1.3147\n",
      "Epoch 6/60\n",
      "3360/3360 [==============================] - 1s 254us/sample - loss: 1.1289 - val_loss: 1.1677\n",
      "Epoch 7/60\n",
      "3360/3360 [==============================] - 1s 253us/sample - loss: 1.0352 - val_loss: 1.0563\n",
      "Epoch 8/60\n",
      "3360/3360 [==============================] - 1s 254us/sample - loss: 0.9375 - val_loss: 0.9821\n",
      "Epoch 9/60\n",
      "3360/3360 [==============================] - 1s 247us/sample - loss: 0.8356 - val_loss: 0.8753\n",
      "Epoch 10/60\n",
      "3360/3360 [==============================] - 1s 250us/sample - loss: 0.7783 - val_loss: 0.8039\n",
      "Epoch 11/60\n",
      "3360/3360 [==============================] - 1s 245us/sample - loss: 0.7026 - val_loss: 0.7711\n",
      "Epoch 12/60\n",
      "3360/3360 [==============================] - 1s 253us/sample - loss: 0.6277 - val_loss: 0.7247\n",
      "Epoch 13/60\n",
      "3360/3360 [==============================] - 1s 256us/sample - loss: 0.5711 - val_loss: 0.6980\n",
      "Epoch 14/60\n",
      "3360/3360 [==============================] - 1s 250us/sample - loss: 0.5428 - val_loss: 0.6811\n",
      "Epoch 15/60\n",
      "3360/3360 [==============================] - 1s 240us/sample - loss: 0.5075 - val_loss: 0.6881\n",
      "Epoch 16/60\n",
      "3360/3360 [==============================] - 1s 250us/sample - loss: 0.4929 - val_loss: 0.6365\n",
      "Epoch 17/60\n",
      "3360/3360 [==============================] - 1s 249us/sample - loss: 0.4471 - val_loss: 0.6390\n",
      "Epoch 18/60\n",
      "3360/3360 [==============================] - 1s 252us/sample - loss: 0.4101 - val_loss: 0.6074\n",
      "Epoch 19/60\n",
      "3360/3360 [==============================] - 1s 249us/sample - loss: 0.3750 - val_loss: 0.6872\n",
      "Epoch 20/60\n",
      "3360/3360 [==============================] - 1s 252us/sample - loss: 0.3786 - val_loss: 0.6089\n",
      "Epoch 21/60\n",
      "3360/3360 [==============================] - 1s 237us/sample - loss: 0.3402 - val_loss: 0.6984\n",
      "Epoch 22/60\n",
      "3360/3360 [==============================] - 1s 233us/sample - loss: 0.3545 - val_loss: 0.6384\n",
      "Epoch 23/60\n",
      "3360/3360 [==============================] - 1s 247us/sample - loss: 0.3231 - val_loss: 0.5780\n",
      "Epoch 24/60\n",
      "3360/3360 [==============================] - 1s 241us/sample - loss: 0.2880 - val_loss: 0.6083\n",
      "Epoch 25/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.2866 - val_loss: 0.5701\n",
      "Epoch 26/60\n",
      "3360/3360 [==============================] - 1s 249us/sample - loss: 0.2743 - val_loss: 0.5598\n",
      "Epoch 27/60\n",
      "3360/3360 [==============================] - 1s 248us/sample - loss: 0.2450 - val_loss: 0.5606\n",
      "Epoch 28/60\n",
      "3360/3360 [==============================] - 1s 255us/sample - loss: 0.2277 - val_loss: 0.6047\n",
      "Epoch 29/60\n",
      "3360/3360 [==============================] - 1s 253us/sample - loss: 0.2178 - val_loss: 0.5437\n",
      "Epoch 30/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.2276 - val_loss: 0.5431\n",
      "Epoch 31/60\n",
      "3360/3360 [==============================] - 1s 246us/sample - loss: 0.2540 - val_loss: 0.6530\n",
      "Epoch 32/60\n",
      "3360/3360 [==============================] - 1s 240us/sample - loss: 0.2454 - val_loss: 0.6151\n",
      "Epoch 33/60\n",
      "3360/3360 [==============================] - 1s 242us/sample - loss: 0.2156 - val_loss: 0.5778\n",
      "Epoch 34/60\n",
      "3360/3360 [==============================] - 1s 242us/sample - loss: 0.1966 - val_loss: 0.5693\n",
      "Epoch 35/60\n",
      "3360/3360 [==============================] - 1s 246us/sample - loss: 0.1679 - val_loss: 0.5417\n",
      "Epoch 36/60\n",
      "3360/3360 [==============================] - 1s 248us/sample - loss: 0.1688 - val_loss: 0.5401\n",
      "Epoch 37/60\n",
      "3360/3360 [==============================] - 1s 242us/sample - loss: 0.1508 - val_loss: 0.5512\n",
      "Epoch 38/60\n",
      "3360/3360 [==============================] - 1s 246us/sample - loss: 0.1467 - val_loss: 0.5835\n",
      "Epoch 39/60\n",
      "3360/3360 [==============================] - 1s 245us/sample - loss: 0.1647 - val_loss: 0.5248\n",
      "Epoch 40/60\n",
      "3360/3360 [==============================] - 1s 248us/sample - loss: 0.1291 - val_loss: 0.5259\n",
      "Epoch 41/60\n",
      "3360/3360 [==============================] - 1s 239us/sample - loss: 0.1366 - val_loss: 0.5148\n",
      "Epoch 42/60\n",
      "3360/3360 [==============================] - 1s 249us/sample - loss: 0.1181 - val_loss: 0.5127\n",
      "Epoch 43/60\n",
      "3360/3360 [==============================] - 1s 249us/sample - loss: 0.1158 - val_loss: 0.5650\n",
      "Epoch 44/60\n",
      "3360/3360 [==============================] - 1s 255us/sample - loss: 0.1156 - val_loss: 0.6693\n",
      "Epoch 45/60\n",
      "3360/3360 [==============================] - 1s 257us/sample - loss: 0.1239 - val_loss: 0.5114\n",
      "Epoch 46/60\n",
      "3360/3360 [==============================] - 1s 253us/sample - loss: 0.1082 - val_loss: 0.5030\n",
      "Epoch 47/60\n",
      "3360/3360 [==============================] - 1s 266us/sample - loss: 0.0934 - val_loss: 0.4977\n",
      "Epoch 48/60\n",
      "3360/3360 [==============================] - 1s 261us/sample - loss: 0.0861 - val_loss: 0.5288\n",
      "Epoch 49/60\n",
      "3360/3360 [==============================] - 1s 248us/sample - loss: 0.0947 - val_loss: 0.5134\n",
      "Epoch 50/60\n",
      "3360/3360 [==============================] - 1s 256us/sample - loss: 0.0858 - val_loss: 0.5096\n",
      "Epoch 51/60\n",
      "3360/3360 [==============================] - 1s 261us/sample - loss: 0.0725 - val_loss: 0.5106\n",
      "Epoch 52/60\n",
      "3360/3360 [==============================] - 1s 251us/sample - loss: 0.0788 - val_loss: 0.5321\n",
      "Epoch 53/60\n",
      "3360/3360 [==============================] - 1s 253us/sample - loss: 0.0876 - val_loss: 0.5987\n",
      "Epoch 54/60\n",
      "3360/3360 [==============================] - 1s 250us/sample - loss: 0.1077 - val_loss: 0.5087\n",
      "Epoch 55/60\n",
      "3360/3360 [==============================] - 1s 261us/sample - loss: 0.0724 - val_loss: 0.4959\n",
      "Epoch 56/60\n",
      "3360/3360 [==============================] - 1s 269us/sample - loss: 0.0618 - val_loss: 0.5143\n",
      "Epoch 57/60\n",
      "3360/3360 [==============================] - 1s 261us/sample - loss: 0.0623 - val_loss: 0.5018\n",
      "Epoch 58/60\n",
      "3360/3360 [==============================] - 1s 259us/sample - loss: 0.0572 - val_loss: 0.5087\n",
      "Epoch 59/60\n",
      "3360/3360 [==============================] - 1s 257us/sample - loss: 0.0620 - val_loss: 0.4980\n",
      "Epoch 60/60\n",
      "3360/3360 [==============================] - 1s 260us/sample - loss: 0.0645 - val_loss: 0.4954\n",
      "3360 420 420\n",
      "Train on 3360 samples, validate on 420 samples\n",
      "Epoch 1/60\n",
      "3360/3360 [==============================] - 2s 704us/sample - loss: 2.1580 - val_loss: 1.3538\n",
      "Epoch 2/60\n",
      "3360/3360 [==============================] - 1s 251us/sample - loss: 1.4102 - val_loss: 1.3031\n",
      "Epoch 3/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 1.3818 - val_loss: 1.2642\n",
      "Epoch 4/60\n",
      "3360/3360 [==============================] - 1s 246us/sample - loss: 1.3381 - val_loss: 1.2391\n",
      "Epoch 5/60\n",
      "3360/3360 [==============================] - 1s 241us/sample - loss: 1.2985 - val_loss: 1.2086\n",
      "Epoch 6/60\n",
      "3360/3360 [==============================] - 1s 245us/sample - loss: 1.2552 - val_loss: 1.1451\n",
      "Epoch 7/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 1.1795 - val_loss: 1.0702\n",
      "Epoch 8/60\n",
      "3360/3360 [==============================] - 1s 236us/sample - loss: 1.0950 - val_loss: 0.9711\n",
      "Epoch 9/60\n",
      "3360/3360 [==============================] - 1s 241us/sample - loss: 0.9957 - val_loss: 0.8787\n",
      "Epoch 10/60\n",
      "3360/3360 [==============================] - 1s 233us/sample - loss: 0.8765 - val_loss: 0.7893\n",
      "Epoch 11/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 0.8541 - val_loss: 0.7840\n",
      "Epoch 12/60\n",
      "3360/3360 [==============================] - 1s 242us/sample - loss: 0.7537 - val_loss: 0.7164\n",
      "Epoch 13/60\n",
      "3360/3360 [==============================] - 1s 239us/sample - loss: 0.7262 - val_loss: 0.6832\n",
      "Epoch 14/60\n",
      "3360/3360 [==============================] - 1s 243us/sample - loss: 0.6741 - val_loss: 0.7019\n",
      "Epoch 15/60\n",
      "3360/3360 [==============================] - 1s 254us/sample - loss: 0.6452 - val_loss: 0.6460\n",
      "Epoch 16/60\n",
      "3360/3360 [==============================] - 1s 247us/sample - loss: 0.5987 - val_loss: 0.6132\n",
      "Epoch 17/60\n",
      "3360/3360 [==============================] - 1s 243us/sample - loss: 0.5485 - val_loss: 0.6137\n",
      "Epoch 18/60\n",
      "3360/3360 [==============================] - 1s 243us/sample - loss: 0.5311 - val_loss: 0.5740\n",
      "Epoch 19/60\n",
      "3360/3360 [==============================] - 1s 239us/sample - loss: 0.4989 - val_loss: 0.5630\n",
      "Epoch 20/60\n",
      "3360/3360 [==============================] - 1s 252us/sample - loss: 0.4663 - val_loss: 0.5412\n",
      "Epoch 21/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.4342 - val_loss: 0.5313\n",
      "Epoch 22/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.4153 - val_loss: 0.5778\n",
      "Epoch 23/60\n",
      "3360/3360 [==============================] - 1s 246us/sample - loss: 0.4247 - val_loss: 0.6014\n",
      "Epoch 24/60\n",
      "3360/3360 [==============================] - 1s 254us/sample - loss: 0.4176 - val_loss: 0.5993\n",
      "Epoch 25/60\n",
      "3360/3360 [==============================] - 1s 239us/sample - loss: 0.3936 - val_loss: 0.5213\n",
      "Epoch 26/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.3330 - val_loss: 0.4780\n",
      "Epoch 27/60\n",
      "3360/3360 [==============================] - 1s 252us/sample - loss: 0.3241 - val_loss: 0.4638\n",
      "Epoch 28/60\n",
      "3360/3360 [==============================] - 1s 236us/sample - loss: 0.3140 - val_loss: 0.4641\n",
      "Epoch 29/60\n",
      "3360/3360 [==============================] - 1s 237us/sample - loss: 0.3020 - val_loss: 0.4800\n",
      "Epoch 30/60\n",
      "3360/3360 [==============================] - 1s 236us/sample - loss: 0.3180 - val_loss: 0.4505\n",
      "Epoch 31/60\n",
      "3360/3360 [==============================] - 1s 229us/sample - loss: 0.2602 - val_loss: 0.4460\n",
      "Epoch 32/60\n",
      "3360/3360 [==============================] - 1s 228us/sample - loss: 0.2508 - val_loss: 0.4908\n",
      "Epoch 33/60\n",
      "3360/3360 [==============================] - 1s 235us/sample - loss: 0.2500 - val_loss: 0.4361\n",
      "Epoch 34/60\n",
      "3360/3360 [==============================] - 1s 234us/sample - loss: 0.2353 - val_loss: 0.4466\n",
      "Epoch 35/60\n",
      "3360/3360 [==============================] - 1s 233us/sample - loss: 0.2369 - val_loss: 0.5149\n",
      "Epoch 36/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 0.2450 - val_loss: 0.4269\n",
      "Epoch 37/60\n",
      "3360/3360 [==============================] - 1s 237us/sample - loss: 0.1955 - val_loss: 0.4660\n",
      "Epoch 38/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 0.2050 - val_loss: 0.4440\n",
      "Epoch 39/60\n",
      "3360/3360 [==============================] - 1s 235us/sample - loss: 0.1908 - val_loss: 0.4622\n",
      "Epoch 40/60\n",
      "3360/3360 [==============================] - 1s 243us/sample - loss: 0.1756 - val_loss: 0.4861\n",
      "Epoch 41/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 0.1958 - val_loss: 0.4440\n",
      "Epoch 42/60\n",
      "3360/3360 [==============================] - 1s 256us/sample - loss: 0.1514 - val_loss: 0.4117\n",
      "Epoch 43/60\n",
      "3360/3360 [==============================] - 1s 232us/sample - loss: 0.1563 - val_loss: 0.4055\n",
      "Epoch 44/60\n",
      "3360/3360 [==============================] - 1s 237us/sample - loss: 0.1435 - val_loss: 0.4103\n",
      "Epoch 45/60\n",
      "3360/3360 [==============================] - 1s 231us/sample - loss: 0.1417 - val_loss: 0.4418\n",
      "Epoch 46/60\n",
      "3360/3360 [==============================] - 1s 234us/sample - loss: 0.1525 - val_loss: 0.5190\n",
      "Epoch 47/60\n",
      "3360/3360 [==============================] - 1s 240us/sample - loss: 0.1326 - val_loss: 0.4102\n",
      "Epoch 48/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 0.1330 - val_loss: 0.4426\n",
      "Epoch 49/60\n",
      "3360/3360 [==============================] - 1s 235us/sample - loss: 0.1301 - val_loss: 0.4192\n",
      "Epoch 50/60\n",
      "3360/3360 [==============================] - 1s 243us/sample - loss: 0.1216 - val_loss: 0.3958\n",
      "Epoch 51/60\n",
      "3360/3360 [==============================] - 1s 240us/sample - loss: 0.1280 - val_loss: 0.4186\n",
      "Epoch 52/60\n",
      "3360/3360 [==============================] - 1s 243us/sample - loss: 0.1168 - val_loss: 0.4032\n",
      "Epoch 53/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.0932 - val_loss: 0.4150\n",
      "Epoch 54/60\n",
      "3360/3360 [==============================] - 1s 240us/sample - loss: 0.0981 - val_loss: 0.4071\n",
      "Epoch 55/60\n",
      "3360/3360 [==============================] - 1s 239us/sample - loss: 0.0836 - val_loss: 0.4008\n",
      "Epoch 56/60\n",
      "3360/3360 [==============================] - 1s 244us/sample - loss: 0.0888 - val_loss: 0.3920\n",
      "Epoch 57/60\n",
      "3360/3360 [==============================] - 1s 238us/sample - loss: 0.0854 - val_loss: 0.3990\n",
      "Epoch 58/60\n",
      "3360/3360 [==============================] - 1s 229us/sample - loss: 0.0756 - val_loss: 0.3903\n",
      "Epoch 59/60\n",
      "3360/3360 [==============================] - 1s 248us/sample - loss: 0.0729 - val_loss: 0.3989\n",
      "Epoch 60/60\n",
      "3360/3360 [==============================] - 1s 239us/sample - loss: 0.0742 - val_loss: 0.3888\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    \n",
    "    train_idx = [i for i in train_idx if i < len(df)]\n",
    "    valid_idx = [i for i in valid_idx if i < len(df)]    \n",
    "    test_idx = [i for i in test_idx if i < len(df)]\n",
    "    \n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.Reg_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                   (validX, validY), \n",
    "                                                                   patience = patience, \n",
    "                                                                   criteria = monitor)\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "    performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "\n",
    "    train_rmses, train_r2s = performance.evaluate(trainX, trainY)            \n",
    "    valid_rmses, valid_r2s = performance.evaluate(validX, validY)            \n",
    "    test_rmses, test_r2s = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                 'task_name':task_name,            \n",
    "                 'train_rmse':np.nanmean(train_rmses), \n",
    "                 'valid_rmse':np.nanmean(valid_rmses),                      \n",
    "                 'test_rmse':np.nanmean(test_rmses), \n",
    "\n",
    "                 'train_r2':np.nanmean(train_r2s), \n",
    "                 'valid_r2':np.nanmean(valid_r2s),                      \n",
    "                 'test_r2':np.nanmean(test_r2s), \n",
    "\n",
    "                 '# trainable params': trainable_params,\n",
    "                 'best_epoch': best_epoch,\n",
    "                 'batch_size':batch_size,\n",
    "                 'lr': lr,\n",
    "                 'weight_decay':weight_decay\n",
    "                }\n",
    "    results.append(final_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f84f52690b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['rmse', 'val_rmse']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6250199535582208"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04015739126596142"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_rmse.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>valid_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>valid_r2</th>\n",
       "      <th>test_r2</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Lipop</td>\n",
       "      <td>0.214278</td>\n",
       "      <td>0.622349</td>\n",
       "      <td>0.616750</td>\n",
       "      <td>0.969232</td>\n",
       "      <td>0.733749</td>\n",
       "      <td>0.738197</td>\n",
       "      <td>803681</td>\n",
       "      <td>59</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Lipop</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.703880</td>\n",
       "      <td>0.589642</td>\n",
       "      <td>0.971661</td>\n",
       "      <td>0.662340</td>\n",
       "      <td>0.753251</td>\n",
       "      <td>803681</td>\n",
       "      <td>59</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Lipop</td>\n",
       "      <td>0.247299</td>\n",
       "      <td>0.622935</td>\n",
       "      <td>0.668669</td>\n",
       "      <td>0.962202</td>\n",
       "      <td>0.706602</td>\n",
       "      <td>0.698356</td>\n",
       "      <td>803681</td>\n",
       "      <td>59</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_rmse  valid_rmse  test_rmse  train_r2  valid_r2   test_r2  \\\n",
       "0     Lipop    0.214278    0.622349   0.616750  0.969232  0.733749  0.738197   \n",
       "1     Lipop    0.206349    0.703880   0.589642  0.971661  0.662340  0.753251   \n",
       "2     Lipop    0.247299    0.622935   0.668669  0.962202  0.706602  0.698356   \n",
       "\n",
       "   # trainable params  best_epoch  batch_size      lr  weight_decay  \n",
       "0              803681          59         128  0.0001             0  \n",
       "1              803681          59         128  0.0001             0  \n",
       "2              803681          59         128  0.0001             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
