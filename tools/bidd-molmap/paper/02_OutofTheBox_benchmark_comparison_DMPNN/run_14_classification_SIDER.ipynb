{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [18:01:56] Enabling RDKit 2019.09.2 jupyter extensions\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: SIDER number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'SIDER'\n",
    "from chembench import load_data\n",
    "df, induces = load_data(task_name)\n",
    "\n",
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X1.astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256] #27 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_auc'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141 143 143\n",
      "WARNING:tensorflow:From /home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "epoch: 0001, loss: 0.6039 - val_loss: 0.6184; auc: 0.5618 - val_auc: 0.5312                                                                                                    \n",
      "epoch: 0002, loss: 0.6005 - val_loss: 0.6165; auc: 0.6021 - val_auc: 0.5636                                                                                                    \n",
      "epoch: 0003, loss: 0.5986 - val_loss: 0.6146; auc: 0.6344 - val_auc: 0.5952                                                                                                    \n",
      "epoch: 0004, loss: 0.5971 - val_loss: 0.6157; auc: 0.6527 - val_auc: 0.6061                                                                                                    \n",
      "epoch: 0005, loss: 0.5962 - val_loss: 0.6124; auc: 0.6637 - val_auc: 0.6173                                                                                                    \n",
      "epoch: 0006, loss: 0.5944 - val_loss: 0.6121; auc: 0.6709 - val_auc: 0.6251                                                                                                    \n",
      "epoch: 0007, loss: 0.5925 - val_loss: 0.6117; auc: 0.6757 - val_auc: 0.6289                                                                                                    \n",
      "epoch: 0008, loss: 0.5911 - val_loss: 0.6105; auc: 0.6781 - val_auc: 0.6304                                                                                                    \n",
      "epoch: 0009, loss: 0.5902 - val_loss: 0.6119; auc: 0.6802 - val_auc: 0.6335                                                                                                    \n",
      "epoch: 0010, loss: 0.5889 - val_loss: 0.6068; auc: 0.6783 - val_auc: 0.6324                                                                                                    \n",
      "epoch: 0011, loss: 0.5867 - val_loss: 0.6100; auc: 0.6784 - val_auc: 0.6330                                                                                                    \n",
      "epoch: 0012, loss: 0.5848 - val_loss: 0.6050; auc: 0.6774 - val_auc: 0.6352                                                                                                    \n",
      "epoch: 0013, loss: 0.5829 - val_loss: 0.6061; auc: 0.6788 - val_auc: 0.6372                                                                                                    \n",
      "epoch: 0014, loss: 0.5835 - val_loss: 0.6012; auc: 0.6754 - val_auc: 0.6375                                                                                                    \n",
      "epoch: 0015, loss: 0.5794 - val_loss: 0.6059; auc: 0.6779 - val_auc: 0.6368                                                                                                    \n",
      "epoch: 0016, loss: 0.5781 - val_loss: 0.6036; auc: 0.6763 - val_auc: 0.6378                                                                                                    \n",
      "epoch: 0017, loss: 0.5762 - val_loss: 0.5980; auc: 0.6760 - val_auc: 0.6385                                                                                                    \n",
      "epoch: 0018, loss: 0.5732 - val_loss: 0.6020; auc: 0.6783 - val_auc: 0.6399                                                                                                    \n",
      "epoch: 0019, loss: 0.5719 - val_loss: 0.6013; auc: 0.6768 - val_auc: 0.6390                                                                                                    \n",
      "epoch: 0020, loss: 0.5691 - val_loss: 0.6015; auc: 0.6766 - val_auc: 0.6396                                                                                                    \n",
      "epoch: 0021, loss: 0.5681 - val_loss: 0.6008; auc: 0.6778 - val_auc: 0.6405                                                                                                    \n",
      "epoch: 0022, loss: 0.5657 - val_loss: 0.5968; auc: 0.6791 - val_auc: 0.6420                                                                                                    \n",
      "epoch: 0023, loss: 0.5633 - val_loss: 0.5928; auc: 0.6789 - val_auc: 0.6434                                                                                                    \n",
      "epoch: 0024, loss: 0.5621 - val_loss: 0.5956; auc: 0.6821 - val_auc: 0.6437                                                                                                    \n",
      "epoch: 0025, loss: 0.5587 - val_loss: 0.5928; auc: 0.6829 - val_auc: 0.6465                                                                                                    \n",
      "epoch: 0026, loss: 0.5568 - val_loss: 0.5937; auc: 0.6831 - val_auc: 0.6457                                                                                                    \n",
      "epoch: 0027, loss: 0.5546 - val_loss: 0.5966; auc: 0.6864 - val_auc: 0.6479                                                                                                    \n",
      "epoch: 0028, loss: 0.5528 - val_loss: 0.5964; auc: 0.6873 - val_auc: 0.6495                                                                                                    \n",
      "epoch: 0029, loss: 0.5518 - val_loss: 0.5810; auc: 0.6862 - val_auc: 0.6515                                                                                                    \n",
      "epoch: 0030, loss: 0.5535 - val_loss: 0.6101; auc: 0.6920 - val_auc: 0.6523                                                                                                    \n",
      "epoch: 0031, loss: 0.5467 - val_loss: 0.5809; auc: 0.6907 - val_auc: 0.6557                                                                                                    \n",
      "epoch: 0032, loss: 0.5448 - val_loss: 0.6230; auc: 0.6954 - val_auc: 0.6562                                                                                                    \n",
      "epoch: 0033, loss: 0.5470 - val_loss: 0.5761; auc: 0.6909 - val_auc: 0.6578                                                                                                    \n",
      "epoch: 0034, loss: 0.5479 - val_loss: 0.6160; auc: 0.7009 - val_auc: 0.6634                                                                                                    \n",
      "epoch: 0035, loss: 0.5418 - val_loss: 0.5827; auc: 0.7014 - val_auc: 0.6657                                                                                                    \n",
      "epoch: 0036, loss: 0.5388 - val_loss: 0.6024; auc: 0.7008 - val_auc: 0.6623                                                                                                    \n",
      "epoch: 0037, loss: 0.5365 - val_loss: 0.5884; auc: 0.7054 - val_auc: 0.6693                                                                                                    \n",
      "epoch: 0038, loss: 0.5337 - val_loss: 0.5935; auc: 0.7057 - val_auc: 0.6681                                                                                                    \n",
      "epoch: 0039, loss: 0.5315 - val_loss: 0.5852; auc: 0.7061 - val_auc: 0.6694                                                                                                    \n",
      "epoch: 0040, loss: 0.5300 - val_loss: 0.5899; auc: 0.7089 - val_auc: 0.6708                                                                                                    \n",
      "epoch: 0041, loss: 0.5296 - val_loss: 0.5839; auc: 0.7124 - val_auc: 0.6783                                                                                                    \n",
      "epoch: 0042, loss: 0.5280 - val_loss: 0.5977; auc: 0.7138 - val_auc: 0.6759                                                                                                    \n",
      "epoch: 0043, loss: 0.5252 - val_loss: 0.5845; auc: 0.7158 - val_auc: 0.6813                                                                                                    \n",
      "epoch: 0044, loss: 0.5243 - val_loss: 0.5947; auc: 0.7178 - val_auc: 0.6814                                                                                                    \n",
      "epoch: 0045, loss: 0.5242 - val_loss: 0.6035; auc: 0.7226 - val_auc: 0.6846                                                                                                    \n",
      "epoch: 0046, loss: 0.5206 - val_loss: 0.6150; auc: 0.7218 - val_auc: 0.6818                                                                                                    \n",
      "epoch: 0047, loss: 0.5181 - val_loss: 0.5880; auc: 0.7254 - val_auc: 0.6855                                                                                                    \n",
      "epoch: 0048, loss: 0.5154 - val_loss: 0.6058; auc: 0.7257 - val_auc: 0.6855                                                                                                    \n",
      "epoch: 0049, loss: 0.5142 - val_loss: 0.5932; auc: 0.7274 - val_auc: 0.6871                                                                                                    \n",
      "epoch: 0050, loss: 0.5120 - val_loss: 0.5934; auc: 0.7303 - val_auc: 0.6882                                                                                                    \n",
      "epoch: 0051, loss: 0.5099 - val_loss: 0.6236; auc: 0.7333 - val_auc: 0.6868                                                                                                    \n",
      "epoch: 0052, loss: 0.5095 - val_loss: 0.6095; auc: 0.7349 - val_auc: 0.6895                                                                                                    \n",
      "epoch: 0053, loss: 0.5066 - val_loss: 0.6142; auc: 0.7376 - val_auc: 0.6918                                                                                                    \n",
      "epoch: 0054, loss: 0.5053 - val_loss: 0.5953; auc: 0.7352 - val_auc: 0.6897                                                                                                    \n",
      "epoch: 0055, loss: 0.5051 - val_loss: 0.5921; auc: 0.7399 - val_auc: 0.6940                                                                                                    \n",
      "epoch: 0056, loss: 0.5041 - val_loss: 0.6466; auc: 0.7445 - val_auc: 0.6923                                                                                                    \n",
      "epoch: 0057, loss: 0.5025 - val_loss: 0.6247; auc: 0.7463 - val_auc: 0.6944                                                                                                    \n",
      "epoch: 0058, loss: 0.4980 - val_loss: 0.6059; auc: 0.7460 - val_auc: 0.6948                                                                                                    \n",
      "epoch: 0059, loss: 0.4959 - val_loss: 0.6164; auc: 0.7466 - val_auc: 0.6949                                                                                                    \n",
      "epoch: 0060, loss: 0.4957 - val_loss: 0.6341; auc: 0.7519 - val_auc: 0.6958                                                                                                    \n",
      "epoch: 0061, loss: 0.4923 - val_loss: 0.6004; auc: 0.7525 - val_auc: 0.6988                                                                                                    \n",
      "epoch: 0062, loss: 0.4910 - val_loss: 0.5963; auc: 0.7525 - val_auc: 0.6977                                                                                                    \n",
      "epoch: 0063, loss: 0.4911 - val_loss: 0.5981; auc: 0.7540 - val_auc: 0.6994                                                                                                    \n",
      "epoch: 0064, loss: 0.4914 - val_loss: 0.6045; auc: 0.7543 - val_auc: 0.6986                                                                                                    \n",
      "epoch: 0065, loss: 0.4885 - val_loss: 0.5972; auc: 0.7595 - val_auc: 0.7006                                                                                                    \n",
      "epoch: 0066, loss: 0.4863 - val_loss: 0.6152; auc: 0.7603 - val_auc: 0.6993                                                                                                    \n",
      "epoch: 0067, loss: 0.4857 - val_loss: 0.6336; auc: 0.7645 - val_auc: 0.6998                                                                                                    \n",
      "epoch: 0068, loss: 0.4820 - val_loss: 0.6088; auc: 0.7655 - val_auc: 0.7014                                                                                                    \n",
      "epoch: 0069, loss: 0.4823 - val_loss: 0.6047; auc: 0.7663 - val_auc: 0.7010                                                                                                    \n",
      "epoch: 0070, loss: 0.4829 - val_loss: 0.6245; auc: 0.7699 - val_auc: 0.7027                                                                                                    \n",
      "epoch: 0071, loss: 0.4799 - val_loss: 0.6492; auc: 0.7745 - val_auc: 0.7019                                                                                                    \n",
      "epoch: 0072, loss: 0.4774 - val_loss: 0.6453; auc: 0.7756 - val_auc: 0.7017                                                                                                    \n",
      "epoch: 0073, loss: 0.4763 - val_loss: 0.6873; auc: 0.7797 - val_auc: 0.7012                                                                                                    \n",
      "epoch: 0074, loss: 0.4744 - val_loss: 0.6965; auc: 0.7816 - val_auc: 0.7017                                                                                                    \n",
      "epoch: 0075, loss: 0.4769 - val_loss: 0.6657; auc: 0.7821 - val_auc: 0.7012                                                                                                    \n",
      "epoch: 0076, loss: 0.4723 - val_loss: 0.6383; auc: 0.7827 - val_auc: 0.7036                                                                                                    \n",
      "epoch: 0077, loss: 0.4654 - val_loss: 0.6265; auc: 0.7809 - val_auc: 0.7039                                                                                                    \n",
      "epoch: 0078, loss: 0.4637 - val_loss: 0.6297; auc: 0.7865 - val_auc: 0.7051                                                                                                    \n",
      "epoch: 0079, loss: 0.4619 - val_loss: 0.6561; auc: 0.7855 - val_auc: 0.7034                                                                                                    \n",
      "epoch: 0080, loss: 0.4603 - val_loss: 0.6227; auc: 0.7900 - val_auc: 0.7052                                                                                                    \n",
      "epoch: 0081, loss: 0.4594 - val_loss: 0.6556; auc: 0.7885 - val_auc: 0.7041                                                                                                    \n",
      "epoch: 0082, loss: 0.4645 - val_loss: 0.6204; auc: 0.7867 - val_auc: 0.7046                                                                                                    \n",
      "epoch: 0083, loss: 0.4650 - val_loss: 0.6083; auc: 0.7894 - val_auc: 0.7055                                                                                                    \n",
      "epoch: 0084, loss: 0.4588 - val_loss: 0.6416; auc: 0.7934 - val_auc: 0.7051                                                                                                    \n",
      "epoch: 0085, loss: 0.4534 - val_loss: 0.6349; auc: 0.7960 - val_auc: 0.7058                                                                                                    \n",
      "epoch: 0086, loss: 0.4529 - val_loss: 0.6189; auc: 0.7954 - val_auc: 0.7067                                                                                                    \n",
      "epoch: 0087, loss: 0.4518 - val_loss: 0.6365; auc: 0.7982 - val_auc: 0.7056                                                                                                    \n",
      "epoch: 0088, loss: 0.4500 - val_loss: 0.6466; auc: 0.8015 - val_auc: 0.7062                                                                                                    \n",
      "epoch: 0089, loss: 0.4450 - val_loss: 0.6269; auc: 0.8017 - val_auc: 0.7085                                                                                                    \n",
      "epoch: 0090, loss: 0.4434 - val_loss: 0.6555; auc: 0.8050 - val_auc: 0.7059                                                                                                    \n",
      "epoch: 0091, loss: 0.4408 - val_loss: 0.6719; auc: 0.8090 - val_auc: 0.7076                                                                                                    \n",
      "epoch: 0092, loss: 0.4388 - val_loss: 0.6820; auc: 0.8110 - val_auc: 0.7073                                                                                                    \n",
      "epoch: 0093, loss: 0.4380 - val_loss: 0.6542; auc: 0.8117 - val_auc: 0.7080                                                                                                    \n",
      "epoch: 0094, loss: 0.4356 - val_loss: 0.6882; auc: 0.8141 - val_auc: 0.7080                                                                                                    \n",
      "epoch: 0095, loss: 0.4386 - val_loss: 0.6433; auc: 0.8130 - val_auc: 0.7095                                                                                                    \n",
      "epoch: 0096, loss: 0.4406 - val_loss: 0.6588; auc: 0.8109 - val_auc: 0.7064                                                                                                    \n",
      "epoch: 0097, loss: 0.4397 - val_loss: 0.6099; auc: 0.8123 - val_auc: 0.7089                                                                                                    \n",
      "epoch: 0098, loss: 0.4369 - val_loss: 0.6639; auc: 0.8167 - val_auc: 0.7076                                                                                                    \n",
      "epoch: 0099, loss: 0.4342 - val_loss: 0.6431; auc: 0.8194 - val_auc: 0.7092                                                                                                    \n",
      "epoch: 0100, loss: 0.4290 - val_loss: 0.6871; auc: 0.8229 - val_auc: 0.7100                                                                                                    \n",
      "epoch: 0101, loss: 0.4256 - val_loss: 0.6767; auc: 0.8246 - val_auc: 0.7091                                                                                                    \n",
      "epoch: 0102, loss: 0.4246 - val_loss: 0.6890; auc: 0.8270 - val_auc: 0.7104                                                                                                    \n",
      "epoch: 0103, loss: 0.4248 - val_loss: 0.7162; auc: 0.8298 - val_auc: 0.7090                                                                                                    \n",
      "epoch: 0104, loss: 0.4218 - val_loss: 0.6630; auc: 0.8263 - val_auc: 0.7104                                                                                                    \n",
      "epoch: 0105, loss: 0.4222 - val_loss: 0.6536; auc: 0.8266 - val_auc: 0.7089                                                                                                    \n",
      "epoch: 0106, loss: 0.4198 - val_loss: 0.6974; auc: 0.8321 - val_auc: 0.7090                                                                                                    \n",
      "epoch: 0107, loss: 0.4149 - val_loss: 0.7027; auc: 0.8348 - val_auc: 0.7102                                                                                                    \n",
      "epoch: 0108, loss: 0.4152 - val_loss: 0.7328; auc: 0.8374 - val_auc: 0.7095                                                                                                    \n",
      "epoch: 0109, loss: 0.4142 - val_loss: 0.7130; auc: 0.8375 - val_auc: 0.7099                                                                                                    \n",
      "epoch: 0110, loss: 0.4105 - val_loss: 0.7190; auc: 0.8384 - val_auc: 0.7080                                                                                                    \n",
      "epoch: 0111, loss: 0.4063 - val_loss: 0.7385; auc: 0.8412 - val_auc: 0.7084                                                                                                    \n",
      "epoch: 0112, loss: 0.4058 - val_loss: 0.6975; auc: 0.8408 - val_auc: 0.7096                                                                                                    \n",
      "epoch: 0113, loss: 0.4035 - val_loss: 0.7129; auc: 0.8412 - val_auc: 0.7096                                                                                                    \n",
      "epoch: 0114, loss: 0.4004 - val_loss: 0.7195; auc: 0.8447 - val_auc: 0.7087                                                                                                    \n",
      "epoch: 0115, loss: 0.4001 - val_loss: 0.7079; auc: 0.8437 - val_auc: 0.7094                                                                                                    \n",
      "epoch: 0116, loss: 0.4005 - val_loss: 0.6958; auc: 0.8442 - val_auc: 0.7089                                                                                                    \n",
      "epoch: 0117, loss: 0.3961 - val_loss: 0.7229; auc: 0.8485 - val_auc: 0.7088                                                                                                    \n",
      "epoch: 0118, loss: 0.3935 - val_loss: 0.6882; auc: 0.8473 - val_auc: 0.7097                                                                                                    \n",
      "epoch: 0119, loss: 0.3924 - val_loss: 0.7150; auc: 0.8482 - val_auc: 0.7088                                                                                                    \n",
      "epoch: 0120, loss: 0.3917 - val_loss: 0.6627; auc: 0.8478 - val_auc: 0.7094                                                                                                    \n",
      "epoch: 0121, loss: 0.3901 - val_loss: 0.7264; auc: 0.8512 - val_auc: 0.7092                                                                                                    \n",
      "epoch: 0122, loss: 0.3890 - val_loss: 0.6728; auc: 0.8509 - val_auc: 0.7100                                                                                                    \n",
      "epoch: 0123, loss: 0.3878 - val_loss: 0.7766; auc: 0.8577 - val_auc: 0.7060                                                                                                    \n",
      "epoch: 0124, loss: 0.3887 - val_loss: 0.6798; auc: 0.8548 - val_auc: 0.7097                                                                                                    \n",
      "epoch: 0125, loss: 0.3831 - val_loss: 0.7301; auc: 0.8561 - val_auc: 0.7087                                                                                                    \n",
      "epoch: 0126, loss: 0.3814 - val_loss: 0.7416; auc: 0.8604 - val_auc: 0.7073                                                                                                    \n",
      "epoch: 0127, loss: 0.3784 - val_loss: 0.6939; auc: 0.8570 - val_auc: 0.7095                                                                                                    \n",
      "epoch: 0128, loss: 0.3775 - val_loss: 0.7409; auc: 0.8630 - val_auc: 0.7079                                                                                                    \n",
      "epoch: 0129, loss: 0.3773 - val_loss: 0.6961; auc: 0.8586 - val_auc: 0.7095                                                                                                    \n",
      "epoch: 0130, loss: 0.3747 - val_loss: 0.7565; auc: 0.8653 - val_auc: 0.7062                                                                                                    \n",
      "epoch: 0131, loss: 0.3715 - val_loss: 0.7118; auc: 0.8618 - val_auc: 0.7086                                                                                                    \n",
      "epoch: 0132, loss: 0.3706 - val_loss: 0.7316; auc: 0.8659 - val_auc: 0.7087                                                                                                    \n",
      "epoch: 0133, loss: 0.3696 - val_loss: 0.7565; auc: 0.8665 - val_auc: 0.7078                                                                                                    \n",
      "epoch: 0134, loss: 0.3675 - val_loss: 0.6873; auc: 0.8659 - val_auc: 0.7088                                                                                                    \n",
      "epoch: 0135, loss: 0.3656 - val_loss: 0.7535; auc: 0.8688 - val_auc: 0.7079                                                                                                    \n",
      "epoch: 0136, loss: 0.3641 - val_loss: 0.7195; auc: 0.8693 - val_auc: 0.7081                                                                                                    \n",
      "epoch: 0137, loss: 0.3659 - val_loss: 0.7028; auc: 0.8677 - val_auc: 0.7088                                                                                                    \n",
      "epoch: 0138, loss: 0.3618 - val_loss: 0.7138; auc: 0.8706 - val_auc: 0.7088                                                                                                    \n",
      "epoch: 0139, loss: 0.3596 - val_loss: 0.7583; auc: 0.8725 - val_auc: 0.7075                                                                                                    \n",
      "epoch: 0140, loss: 0.3572 - val_loss: 0.7215; auc: 0.8725 - val_auc: 0.7081                                                                                                    \n",
      "epoch: 0141, loss: 0.3568 - val_loss: 0.7873; auc: 0.8764 - val_auc: 0.7052                                                                                                    \n",
      "epoch: 0142, loss: 0.3623 - val_loss: 0.6985; auc: 0.8699 - val_auc: 0.7109                                                                                                    \n",
      "epoch: 0143, loss: 0.3604 - val_loss: 0.7489; auc: 0.8768 - val_auc: 0.7066                                                                                                    \n",
      "epoch: 0144, loss: 0.3562 - val_loss: 0.7493; auc: 0.8778 - val_auc: 0.7049                                                                                                    \n",
      "epoch: 0145, loss: 0.3535 - val_loss: 0.7231; auc: 0.8757 - val_auc: 0.7073                                                                                                    \n",
      "epoch: 0146, loss: 0.3493 - val_loss: 0.7536; auc: 0.8790 - val_auc: 0.7066                                                                                                    \n",
      "epoch: 0147, loss: 0.3471 - val_loss: 0.7337; auc: 0.8790 - val_auc: 0.7069                                                                                                    \n",
      "epoch: 0148, loss: 0.3451 - val_loss: 0.7527; auc: 0.8804 - val_auc: 0.7063                                                                                                    \n",
      "epoch: 0149, loss: 0.3470 - val_loss: 0.7296; auc: 0.8795 - val_auc: 0.7067                                                                                                    \n",
      "epoch: 0150, loss: 0.3470 - val_loss: 0.8257; auc: 0.8846 - val_auc: 0.7043                                                                                                    \n",
      "epoch: 0151, loss: 0.3454 - val_loss: 0.7911; auc: 0.8833 - val_auc: 0.7063                                                                                                    \n",
      "epoch: 0152, loss: 0.3419 - val_loss: 0.7488; auc: 0.8821 - val_auc: 0.7067                                                                                                    \n",
      "epoch: 0153, loss: 0.3422 - val_loss: 0.7803; auc: 0.8853 - val_auc: 0.7038                                                                                                    \n",
      "epoch: 0154, loss: 0.3406 - val_loss: 0.7863; auc: 0.8857 - val_auc: 0.7061                                                                                                    \n",
      "epoch: 0155, loss: 0.3375 - val_loss: 0.7560; auc: 0.8857 - val_auc: 0.7063                                                                                                    \n",
      "epoch: 0156, loss: 0.3362 - val_loss: 0.7512; auc: 0.8850 - val_auc: 0.7073                                                                                                    \n",
      "epoch: 0157, loss: 0.3350 - val_loss: 0.8061; auc: 0.8888 - val_auc: 0.7052                                                                                                    \n",
      "epoch: 0158, loss: 0.3341 - val_loss: 0.8024; auc: 0.8900 - val_auc: 0.7042                                                                                                    \n",
      "epoch: 0159, loss: 0.3363 - val_loss: 0.7569; auc: 0.8878 - val_auc: 0.7055                                                                                                    \n",
      "epoch: 0160, loss: 0.3326 - val_loss: 0.7544; auc: 0.8884 - val_auc: 0.7052                                                                                                    \n",
      "epoch: 0161, loss: 0.3295 - val_loss: 0.8061; auc: 0.8908 - val_auc: 0.7049                                                                                                    \n",
      "epoch: 0162, loss: 0.3279 - val_loss: 0.7989; auc: 0.8917 - val_auc: 0.7038                                                                                                    \n",
      "epoch: 0163, loss: 0.3284 - val_loss: 0.7850; auc: 0.8918 - val_auc: 0.7053                                                                                                    \n",
      "epoch: 0164, loss: 0.3252 - val_loss: 0.7720; auc: 0.8909 - val_auc: 0.7049                                                                                                    \n",
      "epoch: 0165, loss: 0.3257 - val_loss: 0.7586; auc: 0.8917 - val_auc: 0.7061                                                                                                    \n",
      "epoch: 0166, loss: 0.3255 - val_loss: 0.8322; auc: 0.8941 - val_auc: 0.7035                                                                                                    \n",
      "epoch: 0167, loss: 0.3225 - val_loss: 0.7786; auc: 0.8936 - val_auc: 0.7041                                                                                                    \n",
      "epoch: 0168, loss: 0.3199 - val_loss: 0.8038; auc: 0.8953 - val_auc: 0.7049                                                                                                    \n",
      "epoch: 0169, loss: 0.3205 - val_loss: 0.8229; auc: 0.8966 - val_auc: 0.7023                                                                                                    \n",
      "epoch: 0170, loss: 0.3212 - val_loss: 0.8096; auc: 0.8954 - val_auc: 0.7040                                                                                                    \n",
      "epoch: 0171, loss: 0.3201 - val_loss: 0.7672; auc: 0.8933 - val_auc: 0.7053                                                                                                    \n",
      "epoch: 0172, loss: 0.3181 - val_loss: 0.8414; auc: 0.8980 - val_auc: 0.7025                                                                                                    \n",
      "epoch: 0173, loss: 0.3178 - val_loss: 0.7700; auc: 0.8958 - val_auc: 0.7037                                                                                                    \n",
      "epoch: 0174, loss: 0.3158 - val_loss: 0.7734; auc: 0.8967 - val_auc: 0.7049                                                                                                    \n",
      "epoch: 0175, loss: 0.3145 - val_loss: 0.7949; auc: 0.8977 - val_auc: 0.7047                                                                                                    \n",
      "epoch: 0176, loss: 0.3124 - val_loss: 0.8276; auc: 0.8997 - val_auc: 0.7036                                                                                                    \n",
      "epoch: 0177, loss: 0.3108 - val_loss: 0.8255; auc: 0.8992 - val_auc: 0.7049                                                                                                    \n",
      "epoch: 0178, loss: 0.3133 - val_loss: 0.7809; auc: 0.8997 - val_auc: 0.7040                                                                                                    \n",
      "epoch: 0179, loss: 0.3091 - val_loss: 0.8356; auc: 0.9009 - val_auc: 0.7026                                                                                                    \n",
      "epoch: 0180, loss: 0.3071 - val_loss: 0.8264; auc: 0.9016 - val_auc: 0.7040                                                                                                    \n",
      "epoch: 0181, loss: 0.3071 - val_loss: 0.8195; auc: 0.9024 - val_auc: 0.7034                                                                                                    \n",
      "epoch: 0182, loss: 0.3066 - val_loss: 0.8074; auc: 0.9014 - val_auc: 0.7034                                                                                                    \n",
      "epoch: 0183, loss: 0.3058 - val_loss: 0.8077; auc: 0.9014 - val_auc: 0.7039                                                                                                    \n",
      "epoch: 0184, loss: 0.3039 - val_loss: 0.8508; auc: 0.9043 - val_auc: 0.7018                                                                                                    \n",
      "epoch: 0185, loss: 0.3041 - val_loss: 0.8318; auc: 0.9038 - val_auc: 0.7031                                                                                                    \n",
      "epoch: 0186, loss: 0.3037 - val_loss: 0.8446; auc: 0.9047 - val_auc: 0.7020                                                                                                    \n",
      "epoch: 0187, loss: 0.3015 - val_loss: 0.8330; auc: 0.9041 - val_auc: 0.7046                                                                                                    \n",
      "epoch: 0188, loss: 0.3011 - val_loss: 0.8145; auc: 0.9043 - val_auc: 0.7047                                                                                                    \n",
      "epoch: 0189, loss: 0.3006 - val_loss: 0.8144; auc: 0.9043 - val_auc: 0.7040                                                                                                    \n",
      "epoch: 0190, loss: 0.3016 - val_loss: 0.7992; auc: 0.9043 - val_auc: 0.7033                                                                                                    \n",
      "epoch: 0191, loss: 0.2985 - val_loss: 0.8718; auc: 0.9065 - val_auc: 0.7017                                                                                                    \n",
      "epoch: 0192, loss: 0.2961 - val_loss: 0.8646; auc: 0.9076 - val_auc: 0.7019                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00192: early stopping\n",
      "1141 143 143\n",
      "Train on 1141 samples, validate on 143 samples\n",
      "Epoch 1/142\n",
      "1141/1141 [==============================] - 2s 1ms/sample - loss: 0.6101 - val_loss: 0.6171\n",
      "Epoch 2/142\n",
      "1141/1141 [==============================] - 0s 313us/sample - loss: 0.6059 - val_loss: 0.6131\n",
      "Epoch 3/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.6047 - val_loss: 0.6131\n",
      "Epoch 4/142\n",
      "1141/1141 [==============================] - 0s 342us/sample - loss: 0.6024 - val_loss: 0.6112\n",
      "Epoch 5/142\n",
      "1141/1141 [==============================] - 0s 397us/sample - loss: 0.6013 - val_loss: 0.6100\n",
      "Epoch 6/142\n",
      "1141/1141 [==============================] - 0s 343us/sample - loss: 0.5993 - val_loss: 0.6130\n",
      "Epoch 7/142\n",
      "1141/1141 [==============================] - 0s 404us/sample - loss: 0.5979 - val_loss: 0.6090\n",
      "Epoch 8/142\n",
      "1141/1141 [==============================] - 0s 400us/sample - loss: 0.5953 - val_loss: 0.6116\n",
      "Epoch 9/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.5938 - val_loss: 0.6106\n",
      "Epoch 10/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.5923 - val_loss: 0.6053\n",
      "Epoch 11/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.5900 - val_loss: 0.6070\n",
      "Epoch 12/142\n",
      "1141/1141 [==============================] - 0s 321us/sample - loss: 0.5880 - val_loss: 0.6061\n",
      "Epoch 13/142\n",
      "1141/1141 [==============================] - 0s 308us/sample - loss: 0.5857 - val_loss: 0.6021\n",
      "Epoch 14/142\n",
      "1141/1141 [==============================] - 0s 351us/sample - loss: 0.5836 - val_loss: 0.6063\n",
      "Epoch 15/142\n",
      "1141/1141 [==============================] - 0s 370us/sample - loss: 0.5806 - val_loss: 0.6074\n",
      "Epoch 16/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.5785 - val_loss: 0.6056\n",
      "Epoch 17/142\n",
      "1141/1141 [==============================] - 0s 331us/sample - loss: 0.5756 - val_loss: 0.6067\n",
      "Epoch 18/142\n",
      "1141/1141 [==============================] - 0s 357us/sample - loss: 0.5741 - val_loss: 0.6001\n",
      "Epoch 19/142\n",
      "1141/1141 [==============================] - 0s 344us/sample - loss: 0.5707 - val_loss: 0.6099\n",
      "Epoch 20/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.5682 - val_loss: 0.6023\n",
      "Epoch 21/142\n",
      "1141/1141 [==============================] - 0s 399us/sample - loss: 0.5660 - val_loss: 0.6131\n",
      "Epoch 22/142\n",
      "1141/1141 [==============================] - 0s 387us/sample - loss: 0.5644 - val_loss: 0.6040\n",
      "Epoch 23/142\n",
      "1141/1141 [==============================] - 0s 376us/sample - loss: 0.5612 - val_loss: 0.6082\n",
      "Epoch 24/142\n",
      "1141/1141 [==============================] - 0s 359us/sample - loss: 0.5584 - val_loss: 0.6059\n",
      "Epoch 25/142\n",
      "1141/1141 [==============================] - 0s 362us/sample - loss: 0.5578 - val_loss: 0.5989\n",
      "Epoch 26/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.5541 - val_loss: 0.6357\n",
      "Epoch 27/142\n",
      "1141/1141 [==============================] - 0s 371us/sample - loss: 0.5574 - val_loss: 0.5939\n",
      "Epoch 28/142\n",
      "1141/1141 [==============================] - 0s 368us/sample - loss: 0.5502 - val_loss: 0.6297\n",
      "Epoch 29/142\n",
      "1141/1141 [==============================] - 0s 377us/sample - loss: 0.5505 - val_loss: 0.5968\n",
      "Epoch 30/142\n",
      "1141/1141 [==============================] - 0s 372us/sample - loss: 0.5473 - val_loss: 0.6310\n",
      "Epoch 31/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.5472 - val_loss: 0.6212\n",
      "Epoch 32/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.5479 - val_loss: 0.6005\n",
      "Epoch 33/142\n",
      "1141/1141 [==============================] - 0s 380us/sample - loss: 0.5427 - val_loss: 0.6150\n",
      "Epoch 34/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.5395 - val_loss: 0.6298\n",
      "Epoch 35/142\n",
      "1141/1141 [==============================] - 0s 355us/sample - loss: 0.5394 - val_loss: 0.5963\n",
      "Epoch 36/142\n",
      "1141/1141 [==============================] - 0s 371us/sample - loss: 0.5380 - val_loss: 0.6280\n",
      "Epoch 37/142\n",
      "1141/1141 [==============================] - 0s 365us/sample - loss: 0.5347 - val_loss: 0.6514\n",
      "Epoch 38/142\n",
      "1141/1141 [==============================] - 0s 357us/sample - loss: 0.5338 - val_loss: 0.6090\n",
      "Epoch 39/142\n",
      "1141/1141 [==============================] - 0s 339us/sample - loss: 0.5324 - val_loss: 0.6020\n",
      "Epoch 40/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.5298 - val_loss: 0.6599\n",
      "Epoch 41/142\n",
      "1141/1141 [==============================] - 0s 379us/sample - loss: 0.5296 - val_loss: 0.6472\n",
      "Epoch 42/142\n",
      "1141/1141 [==============================] - 0s 339us/sample - loss: 0.5258 - val_loss: 0.6287\n",
      "Epoch 43/142\n",
      "1141/1141 [==============================] - 0s 367us/sample - loss: 0.5261 - val_loss: 0.6526\n",
      "Epoch 44/142\n",
      "1141/1141 [==============================] - 0s 406us/sample - loss: 0.5264 - val_loss: 0.6510\n",
      "Epoch 45/142\n",
      "1141/1141 [==============================] - 0s 352us/sample - loss: 0.5226 - val_loss: 0.6423\n",
      "Epoch 46/142\n",
      "1141/1141 [==============================] - 0s 366us/sample - loss: 0.5198 - val_loss: 0.6456\n",
      "Epoch 47/142\n",
      "1141/1141 [==============================] - 0s 369us/sample - loss: 0.5219 - val_loss: 0.6683\n",
      "Epoch 48/142\n",
      "1141/1141 [==============================] - 0s 336us/sample - loss: 0.5166 - val_loss: 0.6242\n",
      "Epoch 49/142\n",
      "1141/1141 [==============================] - 0s 356us/sample - loss: 0.5155 - val_loss: 0.6413\n",
      "Epoch 50/142\n",
      "1141/1141 [==============================] - 0s 328us/sample - loss: 0.5122 - val_loss: 0.6528\n",
      "Epoch 51/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.5127 - val_loss: 0.6741\n",
      "Epoch 52/142\n",
      "1141/1141 [==============================] - 0s 363us/sample - loss: 0.5128 - val_loss: 0.6416\n",
      "Epoch 53/142\n",
      "1141/1141 [==============================] - 0s 369us/sample - loss: 0.5096 - val_loss: 0.6660\n",
      "Epoch 54/142\n",
      "1141/1141 [==============================] - 0s 347us/sample - loss: 0.5084 - val_loss: 0.6965\n",
      "Epoch 55/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.5077 - val_loss: 0.6316\n",
      "Epoch 56/142\n",
      "1141/1141 [==============================] - 0s 353us/sample - loss: 0.5043 - val_loss: 0.6744\n",
      "Epoch 57/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.5029 - val_loss: 0.6667\n",
      "Epoch 58/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.5005 - val_loss: 0.6567\n",
      "Epoch 59/142\n",
      "1141/1141 [==============================] - 0s 365us/sample - loss: 0.5006 - val_loss: 0.6755\n",
      "Epoch 60/142\n",
      "1141/1141 [==============================] - 0s 363us/sample - loss: 0.5001 - val_loss: 0.6221\n",
      "Epoch 61/142\n",
      "1141/1141 [==============================] - 0s 328us/sample - loss: 0.5008 - val_loss: 0.6682\n",
      "Epoch 62/142\n",
      "1141/1141 [==============================] - 0s 356us/sample - loss: 0.4949 - val_loss: 0.6682\n",
      "Epoch 63/142\n",
      "1141/1141 [==============================] - 0s 342us/sample - loss: 0.4929 - val_loss: 0.7002\n",
      "Epoch 64/142\n",
      "1141/1141 [==============================] - 0s 399us/sample - loss: 0.4912 - val_loss: 0.6392\n",
      "Epoch 65/142\n",
      "1141/1141 [==============================] - 0s 383us/sample - loss: 0.4930 - val_loss: 0.6970\n",
      "Epoch 66/142\n",
      "1141/1141 [==============================] - 0s 385us/sample - loss: 0.4892 - val_loss: 0.7066\n",
      "Epoch 67/142\n",
      "1141/1141 [==============================] - 0s 373us/sample - loss: 0.4875 - val_loss: 0.6989\n",
      "Epoch 68/142\n",
      "1141/1141 [==============================] - 0s 396us/sample - loss: 0.4848 - val_loss: 0.7206\n",
      "Epoch 69/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.4828 - val_loss: 0.6945\n",
      "Epoch 70/142\n",
      "1141/1141 [==============================] - 0s 354us/sample - loss: 0.4835 - val_loss: 0.7129\n",
      "Epoch 71/142\n",
      "1141/1141 [==============================] - 0s 365us/sample - loss: 0.4805 - val_loss: 0.7017\n",
      "Epoch 72/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.4794 - val_loss: 0.7500\n",
      "Epoch 73/142\n",
      "1141/1141 [==============================] - 0s 363us/sample - loss: 0.4815 - val_loss: 0.6783\n",
      "Epoch 74/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.4805 - val_loss: 0.7014\n",
      "Epoch 75/142\n",
      "1141/1141 [==============================] - 0s 363us/sample - loss: 0.4795 - val_loss: 0.6977\n",
      "Epoch 76/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.4737 - val_loss: 0.7007\n",
      "Epoch 77/142\n",
      "1141/1141 [==============================] - 0s 370us/sample - loss: 0.4703 - val_loss: 0.6932\n",
      "Epoch 78/142\n",
      "1141/1141 [==============================] - 0s 367us/sample - loss: 0.4682 - val_loss: 0.7247\n",
      "Epoch 79/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.4683 - val_loss: 0.6671\n",
      "Epoch 80/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.4677 - val_loss: 0.6808\n",
      "Epoch 81/142\n",
      "1141/1141 [==============================] - 0s 369us/sample - loss: 0.4666 - val_loss: 0.6780\n",
      "Epoch 82/142\n",
      "1141/1141 [==============================] - 0s 357us/sample - loss: 0.4659 - val_loss: 0.7395\n",
      "Epoch 83/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.4629 - val_loss: 0.7214\n",
      "Epoch 84/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.4597 - val_loss: 0.6954\n",
      "Epoch 85/142\n",
      "1141/1141 [==============================] - 0s 384us/sample - loss: 0.4579 - val_loss: 0.6955\n",
      "Epoch 86/142\n",
      "1141/1141 [==============================] - 0s 365us/sample - loss: 0.4537 - val_loss: 0.7372\n",
      "Epoch 87/142\n",
      "1141/1141 [==============================] - 0s 363us/sample - loss: 0.4517 - val_loss: 0.7518\n",
      "Epoch 88/142\n",
      "1141/1141 [==============================] - 1s 473us/sample - loss: 0.4502 - val_loss: 0.7174\n",
      "Epoch 89/142\n",
      "1141/1141 [==============================] - 0s 433us/sample - loss: 0.4489 - val_loss: 0.7774\n",
      "Epoch 90/142\n",
      "1141/1141 [==============================] - 1s 447us/sample - loss: 0.4483 - val_loss: 0.7653\n",
      "Epoch 91/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.4464 - val_loss: 0.7481\n",
      "Epoch 92/142\n",
      "1141/1141 [==============================] - 0s 414us/sample - loss: 0.4442 - val_loss: 0.7400\n",
      "Epoch 93/142\n",
      "1141/1141 [==============================] - 0s 391us/sample - loss: 0.4414 - val_loss: 0.7016\n",
      "Epoch 94/142\n",
      "1141/1141 [==============================] - 0s 371us/sample - loss: 0.4448 - val_loss: 0.7504\n",
      "Epoch 95/142\n",
      "1141/1141 [==============================] - 0s 311us/sample - loss: 0.4422 - val_loss: 0.7180\n",
      "Epoch 96/142\n",
      "1141/1141 [==============================] - 0s 333us/sample - loss: 0.4403 - val_loss: 0.7652\n",
      "Epoch 97/142\n",
      "1141/1141 [==============================] - 0s 328us/sample - loss: 0.4356 - val_loss: 0.7668\n",
      "Epoch 98/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.4315 - val_loss: 0.7659\n",
      "Epoch 99/142\n",
      "1141/1141 [==============================] - 0s 325us/sample - loss: 0.4291 - val_loss: 0.7466\n",
      "Epoch 100/142\n",
      "1141/1141 [==============================] - 0s 317us/sample - loss: 0.4290 - val_loss: 0.7335\n",
      "Epoch 101/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.4272 - val_loss: 0.8105\n",
      "Epoch 102/142\n",
      "1141/1141 [==============================] - 0s 334us/sample - loss: 0.4285 - val_loss: 0.7989\n",
      "Epoch 103/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.4252 - val_loss: 0.7583\n",
      "Epoch 104/142\n",
      "1141/1141 [==============================] - 0s 305us/sample - loss: 0.4218 - val_loss: 0.7553\n",
      "Epoch 105/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.4204 - val_loss: 0.8064\n",
      "Epoch 106/142\n",
      "1141/1141 [==============================] - 0s 315us/sample - loss: 0.4190 - val_loss: 0.7625\n",
      "Epoch 107/142\n",
      "1141/1141 [==============================] - 0s 326us/sample - loss: 0.4180 - val_loss: 0.7631\n",
      "Epoch 108/142\n",
      "1141/1141 [==============================] - 0s 296us/sample - loss: 0.4136 - val_loss: 0.7842\n",
      "Epoch 109/142\n",
      "1141/1141 [==============================] - 0s 323us/sample - loss: 0.4120 - val_loss: 0.7817\n",
      "Epoch 110/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.4094 - val_loss: 0.8168\n",
      "Epoch 111/142\n",
      "1141/1141 [==============================] - 0s 389us/sample - loss: 0.4082 - val_loss: 0.7702\n",
      "Epoch 112/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.4051 - val_loss: 0.7991\n",
      "Epoch 113/142\n",
      "1141/1141 [==============================] - 0s 318us/sample - loss: 0.4029 - val_loss: 0.8040\n",
      "Epoch 114/142\n",
      "1141/1141 [==============================] - 0s 333us/sample - loss: 0.4010 - val_loss: 0.8454\n",
      "Epoch 115/142\n",
      "1141/1141 [==============================] - 0s 327us/sample - loss: 0.4002 - val_loss: 0.7847\n",
      "Epoch 116/142\n",
      "1141/1141 [==============================] - 0s 308us/sample - loss: 0.3975 - val_loss: 0.8144\n",
      "Epoch 117/142\n",
      "1141/1141 [==============================] - 0s 318us/sample - loss: 0.3969 - val_loss: 0.7778\n",
      "Epoch 118/142\n",
      "1141/1141 [==============================] - 0s 307us/sample - loss: 0.3960 - val_loss: 0.8449\n",
      "Epoch 119/142\n",
      "1141/1141 [==============================] - 0s 306us/sample - loss: 0.3940 - val_loss: 0.7736\n",
      "Epoch 120/142\n",
      "1141/1141 [==============================] - 0s 292us/sample - loss: 0.3962 - val_loss: 0.8314\n",
      "Epoch 121/142\n",
      "1141/1141 [==============================] - 0s 296us/sample - loss: 0.3926 - val_loss: 0.8484\n",
      "Epoch 122/142\n",
      "1141/1141 [==============================] - 0s 308us/sample - loss: 0.3886 - val_loss: 0.8158\n",
      "Epoch 123/142\n",
      "1141/1141 [==============================] - 0s 321us/sample - loss: 0.3881 - val_loss: 0.7615\n",
      "Epoch 124/142\n",
      "1141/1141 [==============================] - 0s 317us/sample - loss: 0.3852 - val_loss: 0.8612\n",
      "Epoch 125/142\n",
      "1141/1141 [==============================] - 0s 308us/sample - loss: 0.3868 - val_loss: 0.8036\n",
      "Epoch 126/142\n",
      "1141/1141 [==============================] - 0s 323us/sample - loss: 0.3840 - val_loss: 0.8554\n",
      "Epoch 127/142\n",
      "1141/1141 [==============================] - 0s 311us/sample - loss: 0.3811 - val_loss: 0.8499\n",
      "Epoch 128/142\n",
      "1141/1141 [==============================] - 0s 297us/sample - loss: 0.3784 - val_loss: 0.8023\n",
      "Epoch 129/142\n",
      "1141/1141 [==============================] - 0s 307us/sample - loss: 0.3783 - val_loss: 0.8551\n",
      "Epoch 130/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.3803 - val_loss: 0.7963\n",
      "Epoch 131/142\n",
      "1141/1141 [==============================] - 0s 358us/sample - loss: 0.3798 - val_loss: 0.9053\n",
      "Epoch 132/142\n",
      "1141/1141 [==============================] - 0s 399us/sample - loss: 0.3803 - val_loss: 0.8205\n",
      "Epoch 133/142\n",
      "1141/1141 [==============================] - 0s 340us/sample - loss: 0.3744 - val_loss: 0.8444\n",
      "Epoch 134/142\n",
      "1141/1141 [==============================] - 0s 321us/sample - loss: 0.3681 - val_loss: 0.8641\n",
      "Epoch 135/142\n",
      "1141/1141 [==============================] - 0s 310us/sample - loss: 0.3672 - val_loss: 0.8256\n",
      "Epoch 136/142\n",
      "1141/1141 [==============================] - 0s 329us/sample - loss: 0.3637 - val_loss: 0.8965\n",
      "Epoch 137/142\n",
      "1141/1141 [==============================] - 0s 330us/sample - loss: 0.3645 - val_loss: 0.8587\n",
      "Epoch 138/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.3630 - val_loss: 0.8754\n",
      "Epoch 139/142\n",
      "1141/1141 [==============================] - 0s 353us/sample - loss: 0.3616 - val_loss: 0.8287\n",
      "Epoch 140/142\n",
      "1141/1141 [==============================] - 0s 400us/sample - loss: 0.3585 - val_loss: 0.8740\n",
      "Epoch 141/142\n",
      "1141/1141 [==============================] - 0s 347us/sample - loss: 0.3570 - val_loss: 0.8969\n",
      "Epoch 142/142\n",
      "1141/1141 [==============================] - 0s 316us/sample - loss: 0.3578 - val_loss: 0.8353\n",
      "1141 143 143\n",
      "Train on 1141 samples, validate on 143 samples\n",
      "Epoch 1/142\n",
      "1141/1141 [==============================] - 2s 2ms/sample - loss: 0.5999 - val_loss: 0.5836\n",
      "Epoch 2/142\n",
      "1141/1141 [==============================] - 0s 297us/sample - loss: 0.5973 - val_loss: 0.5824\n",
      "Epoch 3/142\n",
      "1141/1141 [==============================] - 0s 356us/sample - loss: 0.5954 - val_loss: 0.5824\n",
      "Epoch 4/142\n",
      "1141/1141 [==============================] - 0s 372us/sample - loss: 0.5941 - val_loss: 0.5822\n",
      "Epoch 5/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.5922 - val_loss: 0.5812\n",
      "Epoch 6/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.5906 - val_loss: 0.5809\n",
      "Epoch 7/142\n",
      "1141/1141 [==============================] - 0s 348us/sample - loss: 0.5891 - val_loss: 0.5803\n",
      "Epoch 8/142\n",
      "1141/1141 [==============================] - 0s 365us/sample - loss: 0.5877 - val_loss: 0.5799\n",
      "Epoch 9/142\n",
      "1141/1141 [==============================] - 0s 358us/sample - loss: 0.5861 - val_loss: 0.5793\n",
      "Epoch 10/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.5846 - val_loss: 0.5790\n",
      "Epoch 11/142\n",
      "1141/1141 [==============================] - 0s 375us/sample - loss: 0.5826 - val_loss: 0.5792\n",
      "Epoch 12/142\n",
      "1141/1141 [==============================] - 0s 356us/sample - loss: 0.5806 - val_loss: 0.5782\n",
      "Epoch 13/142\n",
      "1141/1141 [==============================] - 0s 352us/sample - loss: 0.5786 - val_loss: 0.5783\n",
      "Epoch 14/142\n",
      "1141/1141 [==============================] - 0s 370us/sample - loss: 0.5768 - val_loss: 0.5775\n",
      "Epoch 15/142\n",
      "1141/1141 [==============================] - 0s 356us/sample - loss: 0.5745 - val_loss: 0.5772\n",
      "Epoch 16/142\n",
      "1141/1141 [==============================] - 0s 356us/sample - loss: 0.5731 - val_loss: 0.5768\n",
      "Epoch 17/142\n",
      "1141/1141 [==============================] - 0s 340us/sample - loss: 0.5704 - val_loss: 0.5774\n",
      "Epoch 18/142\n",
      "1141/1141 [==============================] - 0s 393us/sample - loss: 0.5682 - val_loss: 0.5761\n",
      "Epoch 19/142\n",
      "1141/1141 [==============================] - 0s 411us/sample - loss: 0.5659 - val_loss: 0.5765\n",
      "Epoch 20/142\n",
      "1141/1141 [==============================] - 0s 413us/sample - loss: 0.5642 - val_loss: 0.5769\n",
      "Epoch 21/142\n",
      "1141/1141 [==============================] - 0s 354us/sample - loss: 0.5629 - val_loss: 0.5783\n",
      "Epoch 22/142\n",
      "1141/1141 [==============================] - 0s 341us/sample - loss: 0.5619 - val_loss: 0.5766\n",
      "Epoch 23/142\n",
      "1141/1141 [==============================] - 0s 345us/sample - loss: 0.5602 - val_loss: 0.5772\n",
      "Epoch 24/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.5568 - val_loss: 0.5765\n",
      "Epoch 25/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.5549 - val_loss: 0.5752\n",
      "Epoch 26/142\n",
      "1141/1141 [==============================] - 0s 346us/sample - loss: 0.5524 - val_loss: 0.5782\n",
      "Epoch 27/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.5501 - val_loss: 0.5767\n",
      "Epoch 28/142\n",
      "1141/1141 [==============================] - 0s 347us/sample - loss: 0.5474 - val_loss: 0.5769\n",
      "Epoch 29/142\n",
      "1141/1141 [==============================] - 0s 351us/sample - loss: 0.5462 - val_loss: 0.5784\n",
      "Epoch 30/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.5437 - val_loss: 0.5777\n",
      "Epoch 31/142\n",
      "1141/1141 [==============================] - 0s 345us/sample - loss: 0.5435 - val_loss: 0.5814\n",
      "Epoch 32/142\n",
      "1141/1141 [==============================] - 0s 371us/sample - loss: 0.5405 - val_loss: 0.5753\n",
      "Epoch 33/142\n",
      "1141/1141 [==============================] - 0s 347us/sample - loss: 0.5383 - val_loss: 0.5813\n",
      "Epoch 34/142\n",
      "1141/1141 [==============================] - 0s 336us/sample - loss: 0.5382 - val_loss: 0.5764\n",
      "Epoch 35/142\n",
      "1141/1141 [==============================] - 0s 337us/sample - loss: 0.5354 - val_loss: 0.5827\n",
      "Epoch 36/142\n",
      "1141/1141 [==============================] - 0s 351us/sample - loss: 0.5313 - val_loss: 0.5771\n",
      "Epoch 37/142\n",
      "1141/1141 [==============================] - 0s 353us/sample - loss: 0.5294 - val_loss: 0.5805\n",
      "Epoch 38/142\n",
      "1141/1141 [==============================] - 0s 351us/sample - loss: 0.5263 - val_loss: 0.5796\n",
      "Epoch 39/142\n",
      "1141/1141 [==============================] - 0s 342us/sample - loss: 0.5253 - val_loss: 0.5819\n",
      "Epoch 40/142\n",
      "1141/1141 [==============================] - 0s 384us/sample - loss: 0.5234 - val_loss: 0.5807\n",
      "Epoch 41/142\n",
      "1141/1141 [==============================] - 0s 392us/sample - loss: 0.5207 - val_loss: 0.5813\n",
      "Epoch 42/142\n",
      "1141/1141 [==============================] - 0s 339us/sample - loss: 0.5189 - val_loss: 0.5812\n",
      "Epoch 43/142\n",
      "1141/1141 [==============================] - 0s 350us/sample - loss: 0.5168 - val_loss: 0.5857\n",
      "Epoch 44/142\n",
      "1141/1141 [==============================] - 0s 343us/sample - loss: 0.5172 - val_loss: 0.5801\n",
      "Epoch 45/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.5158 - val_loss: 0.5841\n",
      "Epoch 46/142\n",
      "1141/1141 [==============================] - 0s 345us/sample - loss: 0.5116 - val_loss: 0.5846\n",
      "Epoch 47/142\n",
      "1141/1141 [==============================] - 0s 392us/sample - loss: 0.5090 - val_loss: 0.5821\n",
      "Epoch 48/142\n",
      "1141/1141 [==============================] - 0s 420us/sample - loss: 0.5068 - val_loss: 0.5852\n",
      "Epoch 49/142\n",
      "1141/1141 [==============================] - 0s 407us/sample - loss: 0.5056 - val_loss: 0.5864\n",
      "Epoch 50/142\n",
      "1141/1141 [==============================] - 0s 348us/sample - loss: 0.5027 - val_loss: 0.5845\n",
      "Epoch 51/142\n",
      "1141/1141 [==============================] - 0s 350us/sample - loss: 0.5005 - val_loss: 0.5916\n",
      "Epoch 52/142\n",
      "1141/1141 [==============================] - 0s 354us/sample - loss: 0.4987 - val_loss: 0.5881\n",
      "Epoch 53/142\n",
      "1141/1141 [==============================] - 0s 385us/sample - loss: 0.5013 - val_loss: 0.5935\n",
      "Epoch 54/142\n",
      "1141/1141 [==============================] - 0s 414us/sample - loss: 0.4967 - val_loss: 0.5879\n",
      "Epoch 55/142\n",
      "1141/1141 [==============================] - 0s 397us/sample - loss: 0.4933 - val_loss: 0.5940\n",
      "Epoch 56/142\n",
      "1141/1141 [==============================] - 0s 398us/sample - loss: 0.4966 - val_loss: 0.5912\n",
      "Epoch 57/142\n",
      "1141/1141 [==============================] - 1s 471us/sample - loss: 0.4926 - val_loss: 0.5894\n",
      "Epoch 58/142\n",
      "1141/1141 [==============================] - 0s 418us/sample - loss: 0.4884 - val_loss: 0.5952\n",
      "Epoch 59/142\n",
      "1141/1141 [==============================] - 0s 370us/sample - loss: 0.4862 - val_loss: 0.5952\n",
      "Epoch 60/142\n",
      "1141/1141 [==============================] - 0s 396us/sample - loss: 0.4828 - val_loss: 0.5975\n",
      "Epoch 61/142\n",
      "1141/1141 [==============================] - 0s 378us/sample - loss: 0.4822 - val_loss: 0.5997\n",
      "Epoch 62/142\n",
      "1141/1141 [==============================] - 0s 361us/sample - loss: 0.4812 - val_loss: 0.5929\n",
      "Epoch 63/142\n",
      "1141/1141 [==============================] - 0s 364us/sample - loss: 0.4800 - val_loss: 0.5939\n",
      "Epoch 64/142\n",
      "1141/1141 [==============================] - 0s 365us/sample - loss: 0.4795 - val_loss: 0.6025\n",
      "Epoch 65/142\n",
      "1141/1141 [==============================] - 0s 402us/sample - loss: 0.4768 - val_loss: 0.5937\n",
      "Epoch 66/142\n",
      "1141/1141 [==============================] - 0s 386us/sample - loss: 0.4772 - val_loss: 0.6105\n",
      "Epoch 67/142\n",
      "1141/1141 [==============================] - 0s 423us/sample - loss: 0.4762 - val_loss: 0.5965\n",
      "Epoch 68/142\n",
      "1141/1141 [==============================] - 0s 384us/sample - loss: 0.4683 - val_loss: 0.6083\n",
      "Epoch 69/142\n",
      "1141/1141 [==============================] - 0s 393us/sample - loss: 0.4666 - val_loss: 0.6047\n",
      "Epoch 70/142\n",
      "1141/1141 [==============================] - 0s 303us/sample - loss: 0.4655 - val_loss: 0.6010\n",
      "Epoch 71/142\n",
      "1141/1141 [==============================] - 0s 262us/sample - loss: 0.4642 - val_loss: 0.6064\n",
      "Epoch 72/142\n",
      "1141/1141 [==============================] - 0s 266us/sample - loss: 0.4639 - val_loss: 0.6039\n",
      "Epoch 73/142\n",
      "1141/1141 [==============================] - 0s 278us/sample - loss: 0.4616 - val_loss: 0.6054\n",
      "Epoch 74/142\n",
      "1141/1141 [==============================] - 0s 318us/sample - loss: 0.4589 - val_loss: 0.6063\n",
      "Epoch 75/142\n",
      "1141/1141 [==============================] - 1s 559us/sample - loss: 0.4537 - val_loss: 0.6066\n",
      "Epoch 76/142\n",
      "1141/1141 [==============================] - 0s 259us/sample - loss: 0.4516 - val_loss: 0.6152\n",
      "Epoch 77/142\n",
      "1141/1141 [==============================] - 0s 270us/sample - loss: 0.4512 - val_loss: 0.6149\n",
      "Epoch 78/142\n",
      "1141/1141 [==============================] - 0s 397us/sample - loss: 0.4478 - val_loss: 0.6178\n",
      "Epoch 79/142\n",
      "1141/1141 [==============================] - 0s 331us/sample - loss: 0.4484 - val_loss: 0.6132\n",
      "Epoch 80/142\n",
      "1141/1141 [==============================] - 0s 335us/sample - loss: 0.4440 - val_loss: 0.6187\n",
      "Epoch 81/142\n",
      "1141/1141 [==============================] - 0s 325us/sample - loss: 0.4453 - val_loss: 0.6297\n",
      "Epoch 82/142\n",
      "1141/1141 [==============================] - 0s 277us/sample - loss: 0.4425 - val_loss: 0.6290\n",
      "Epoch 83/142\n",
      "1141/1141 [==============================] - 0s 246us/sample - loss: 0.4401 - val_loss: 0.6119\n",
      "Epoch 84/142\n",
      "1141/1141 [==============================] - 0s 240us/sample - loss: 0.4382 - val_loss: 0.6327\n",
      "Epoch 85/142\n",
      "1141/1141 [==============================] - 0s 244us/sample - loss: 0.4355 - val_loss: 0.6429\n",
      "Epoch 86/142\n",
      "1141/1141 [==============================] - 0s 279us/sample - loss: 0.4318 - val_loss: 0.6339\n",
      "Epoch 87/142\n",
      "1141/1141 [==============================] - 0s 265us/sample - loss: 0.4297 - val_loss: 0.6242\n",
      "Epoch 88/142\n",
      "1141/1141 [==============================] - 0s 340us/sample - loss: 0.4275 - val_loss: 0.6256\n",
      "Epoch 89/142\n",
      "1141/1141 [==============================] - 0s 352us/sample - loss: 0.4268 - val_loss: 0.6217\n",
      "Epoch 90/142\n",
      "1141/1141 [==============================] - 0s 370us/sample - loss: 0.4226 - val_loss: 0.6344\n",
      "Epoch 91/142\n",
      "1141/1141 [==============================] - 0s 349us/sample - loss: 0.4203 - val_loss: 0.6356\n",
      "Epoch 92/142\n",
      "1141/1141 [==============================] - 0s 360us/sample - loss: 0.4186 - val_loss: 0.6489\n",
      "Epoch 93/142\n",
      "1141/1141 [==============================] - 0s 379us/sample - loss: 0.4197 - val_loss: 0.6341\n",
      "Epoch 94/142\n",
      "1141/1141 [==============================] - 0s 396us/sample - loss: 0.4144 - val_loss: 0.6436\n",
      "Epoch 95/142\n",
      "1141/1141 [==============================] - 0s 393us/sample - loss: 0.4126 - val_loss: 0.6308\n",
      "Epoch 96/142\n",
      "1141/1141 [==============================] - 0s 384us/sample - loss: 0.4107 - val_loss: 0.6313\n",
      "Epoch 97/142\n",
      "1141/1141 [==============================] - 0s 386us/sample - loss: 0.4081 - val_loss: 0.6615\n",
      "Epoch 98/142\n",
      "1141/1141 [==============================] - 0s 375us/sample - loss: 0.4064 - val_loss: 0.6430\n",
      "Epoch 99/142\n",
      "1141/1141 [==============================] - 0s 373us/sample - loss: 0.4025 - val_loss: 0.6446\n",
      "Epoch 100/142\n",
      "1141/1141 [==============================] - 0s 399us/sample - loss: 0.4007 - val_loss: 0.6430\n",
      "Epoch 101/142\n",
      "1141/1141 [==============================] - 0s 366us/sample - loss: 0.4008 - val_loss: 0.6667\n",
      "Epoch 102/142\n",
      "1141/1141 [==============================] - 0s 388us/sample - loss: 0.3987 - val_loss: 0.6558\n",
      "Epoch 103/142\n",
      "1141/1141 [==============================] - 0s 383us/sample - loss: 0.3952 - val_loss: 0.6606\n",
      "Epoch 104/142\n",
      "1141/1141 [==============================] - 0s 380us/sample - loss: 0.3934 - val_loss: 0.6499\n",
      "Epoch 105/142\n",
      "1141/1141 [==============================] - 0s 385us/sample - loss: 0.3916 - val_loss: 0.6445\n",
      "Epoch 106/142\n",
      "1141/1141 [==============================] - 0s 375us/sample - loss: 0.3922 - val_loss: 0.6604\n",
      "Epoch 107/142\n",
      "1141/1141 [==============================] - 0s 385us/sample - loss: 0.3860 - val_loss: 0.6549\n",
      "Epoch 108/142\n",
      "1141/1141 [==============================] - 0s 382us/sample - loss: 0.3859 - val_loss: 0.6729\n",
      "Epoch 109/142\n",
      "1141/1141 [==============================] - 0s 390us/sample - loss: 0.3841 - val_loss: 0.6685\n",
      "Epoch 110/142\n",
      "1141/1141 [==============================] - 0s 383us/sample - loss: 0.3800 - val_loss: 0.6825\n",
      "Epoch 111/142\n",
      "1141/1141 [==============================] - 0s 352us/sample - loss: 0.3800 - val_loss: 0.6680\n",
      "Epoch 112/142\n",
      "1141/1141 [==============================] - 0s 385us/sample - loss: 0.3767 - val_loss: 0.6824\n",
      "Epoch 113/142\n",
      "1141/1141 [==============================] - 0s 387us/sample - loss: 0.3784 - val_loss: 0.6554\n",
      "Epoch 114/142\n",
      "1141/1141 [==============================] - 0s 378us/sample - loss: 0.3798 - val_loss: 0.6877\n",
      "Epoch 115/142\n",
      "1141/1141 [==============================] - 0s 362us/sample - loss: 0.3714 - val_loss: 0.6725\n",
      "Epoch 116/142\n",
      "1141/1141 [==============================] - 0s 385us/sample - loss: 0.3716 - val_loss: 0.6853\n",
      "Epoch 117/142\n",
      "1141/1141 [==============================] - 0s 389us/sample - loss: 0.3672 - val_loss: 0.6852\n",
      "Epoch 118/142\n",
      "1141/1141 [==============================] - 0s 378us/sample - loss: 0.3646 - val_loss: 0.6700\n",
      "Epoch 119/142\n",
      "1141/1141 [==============================] - 0s 403us/sample - loss: 0.3669 - val_loss: 0.6957\n",
      "Epoch 120/142\n",
      "1141/1141 [==============================] - 0s 393us/sample - loss: 0.3617 - val_loss: 0.6755\n",
      "Epoch 121/142\n",
      "1141/1141 [==============================] - 0s 378us/sample - loss: 0.3609 - val_loss: 0.6780\n",
      "Epoch 122/142\n",
      "1141/1141 [==============================] - 0s 394us/sample - loss: 0.3577 - val_loss: 0.7079\n",
      "Epoch 123/142\n",
      "1141/1141 [==============================] - 0s 377us/sample - loss: 0.3593 - val_loss: 0.7180\n",
      "Epoch 124/142\n",
      "1141/1141 [==============================] - 0s 391us/sample - loss: 0.3611 - val_loss: 0.7134\n",
      "Epoch 125/142\n",
      "1141/1141 [==============================] - 0s 390us/sample - loss: 0.3571 - val_loss: 0.6834\n",
      "Epoch 126/142\n",
      "1141/1141 [==============================] - 0s 410us/sample - loss: 0.3527 - val_loss: 0.6898\n",
      "Epoch 127/142\n",
      "1141/1141 [==============================] - 0s 383us/sample - loss: 0.3503 - val_loss: 0.6822\n",
      "Epoch 128/142\n",
      "1141/1141 [==============================] - 0s 401us/sample - loss: 0.3511 - val_loss: 0.6968\n",
      "Epoch 129/142\n",
      "1141/1141 [==============================] - 0s 382us/sample - loss: 0.3488 - val_loss: 0.6822\n",
      "Epoch 130/142\n",
      "1141/1141 [==============================] - 0s 355us/sample - loss: 0.3458 - val_loss: 0.7141\n",
      "Epoch 131/142\n",
      "1141/1141 [==============================] - 0s 416us/sample - loss: 0.3446 - val_loss: 0.7056\n",
      "Epoch 132/142\n",
      "1141/1141 [==============================] - 0s 398us/sample - loss: 0.3440 - val_loss: 0.6835\n",
      "Epoch 133/142\n",
      "1141/1141 [==============================] - 0s 389us/sample - loss: 0.3431 - val_loss: 0.7023\n",
      "Epoch 134/142\n",
      "1141/1141 [==============================] - 0s 393us/sample - loss: 0.3428 - val_loss: 0.7035\n",
      "Epoch 135/142\n",
      "1141/1141 [==============================] - 0s 382us/sample - loss: 0.3419 - val_loss: 0.7150\n",
      "Epoch 136/142\n",
      "1141/1141 [==============================] - 0s 401us/sample - loss: 0.3372 - val_loss: 0.7061\n",
      "Epoch 137/142\n",
      "1141/1141 [==============================] - 0s 386us/sample - loss: 0.3349 - val_loss: 0.7228\n",
      "Epoch 138/142\n",
      "1141/1141 [==============================] - 0s 394us/sample - loss: 0.3342 - val_loss: 0.7062\n",
      "Epoch 139/142\n",
      "1141/1141 [==============================] - 0s 387us/sample - loss: 0.3350 - val_loss: 0.7394\n",
      "Epoch 140/142\n",
      "1141/1141 [==============================] - 0s 380us/sample - loss: 0.3340 - val_loss: 0.6926\n",
      "Epoch 141/142\n",
      "1141/1141 [==============================] - 0s 387us/sample - loss: 0.3336 - val_loss: 0.7143\n",
      "Epoch 142/142\n",
      "1141/1141 [==============================] - 0s 395us/sample - loss: 0.3312 - val_loss: 0.7261\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "        performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce28104208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(performance.history)[['auc', 'val_auc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6795504715435102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009909623778221152"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>SIDER</td>\n",
       "      <td>0.869852</td>\n",
       "      <td>0.710903</td>\n",
       "      <td>0.670950</td>\n",
       "      <td>ROC</td>\n",
       "      <td>773563</td>\n",
       "      <td>141</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>SIDER</td>\n",
       "      <td>0.870380</td>\n",
       "      <td>0.684427</td>\n",
       "      <td>0.690387</td>\n",
       "      <td>ROC</td>\n",
       "      <td>773563</td>\n",
       "      <td>141</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SIDER</td>\n",
       "      <td>0.885529</td>\n",
       "      <td>0.643739</td>\n",
       "      <td>0.677315</td>\n",
       "      <td>ROC</td>\n",
       "      <td>773563</td>\n",
       "      <td>141</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0     SIDER   0.869852   0.710903  0.670950    ROC              773563   \n",
       "1     SIDER   0.870380   0.684427  0.690387    ROC              773563   \n",
       "2     SIDER   0.885529   0.643739  0.677315    ROC              773563   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0         141         128  0.0001             0  \n",
       "1         141         128  0.0001             0  \n",
       "2         141         128  0.0001             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
