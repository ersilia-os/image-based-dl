{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "#tmp_feature_dir = './tmpignore'\n",
    "tmp_feature_dir = '/raid/shenwanxiang/tempignore'\n",
    "\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: PCBA number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'PCBA'\n",
    "from chembench import load_data\n",
    "df, induces = load_data(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350343 43793 43793 (437929, 129)\n"
     ]
    }
   ],
   "source": [
    "print(len(induces[0][0]), len(induces[0][1]), len(induces[0][2]), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_idx = df[df.smiles.isna()].index.to_list()\n",
    "nan_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float32').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    Y = Y.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437929, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "xs = np.array_split(df.smiles.to_list(), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1_name_all = os.path.join(tmp_feature_dir, 'X1_%s.data' % (task_name))\n",
    "X2_name_all = os.path.join(tmp_feature_dir, 'X2_%s.data' % (task_name))\n",
    "\n",
    "if os.path.exists(X1_name_all) & os.path.exists(X2_name_all):\n",
    "    X1 = load(X1_name_all)\n",
    "    X2 = load(X2_name_all)\n",
    "else:\n",
    "    ## descriptors\n",
    "    X1s = []\n",
    "    for i, batch_smiles in tqdm(enumerate(xs), ascii=True):\n",
    "        ii = str(i).zfill(2)\n",
    "        X1_name = os.path.join(tmp_feature_dir, 'X1_%s_%s.data' % (task_name, ii))\n",
    "        print('save to %s' % X1_name)\n",
    "        if not os.path.exists(X1_name):\n",
    "            X1 = mp1.batch_transform(batch_smiles, n_jobs = 8)\n",
    "            X1 = X1.astype('float32')\n",
    "            dump(X1, X1_name)\n",
    "            \n",
    "        else:\n",
    "            X1 = load(X1_name)\n",
    "        X1s.append(X1)\n",
    "        del X1\n",
    "        \n",
    "    X1 = np.concatenate(X1s)\n",
    "    del X1s\n",
    "    \n",
    "    dump(X1, X1_name_all)\n",
    "    \n",
    "    ## fingerprint\n",
    "    X2s = []      \n",
    "    for i, batch_smiles in tqdm(enumerate(xs), ascii=True):\n",
    "        ii = str(i).zfill(2)\n",
    "        X2_name = os.path.join(tmp_feature_dir, 'X2_%s_%s.data' % (task_name, ii))\n",
    "        if not os.path.exists(X2_name):\n",
    "            X2 = mp2.batch_transform(batch_smiles, n_jobs = 8)\n",
    "            X2 = X2.astype('float32')\n",
    "            dump(X2, X2_name)\n",
    "            \n",
    "        else:\n",
    "            X2 = load(X2_name)\n",
    "            \n",
    "        X2s.append(X2)\n",
    "        del X2\n",
    "        \n",
    "    X2 = np.concatenate(X2s)\n",
    "    del X2s\n",
    "    dump(X2, X2_name_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437929, 37, 37, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA'] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 20 #early stopping, dual to large computation cost, the larger dataset  set small waitig patience for early stopping\n",
    "dense_layers = [512]  #128 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_auc'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350343 43793 43793\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    \n",
    "    train_idx = list(set(train_idx) - set(nan_idx))\n",
    "    valid_idx = list(set(valid_idx) - set(nan_idx))\n",
    "    test_idx = list(set(test_idx) - set(nan_idx))\n",
    "    \n",
    "    \n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, neg_weights, MASK = MASK)\n",
    "    #loss = molmodel.loss.cross_entropy\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.001,learning_rate=0.001)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "\n",
    "    performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                   (validX, validY), \n",
    "                                                                   patience = patience, \n",
    "                                                                   criteria = monitor,\n",
    "                                                                   metric = metric,\n",
    "                                                                  )\n",
    "    model.fit(trainX, trainY, batch_size=batch_size, \n",
    "          epochs=epochs, verbose= 0, shuffle = True, \n",
    "          validation_data = (validX, validY), \n",
    "          callbacks=[performance]) \n",
    "\n",
    "\n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    print(final_res)\n",
    "    del model, trainX, validX, testX\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
