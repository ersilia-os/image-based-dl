{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [22:28:17] Enabling RDKit 2019.09.2 jupyter extensions\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: BBBP number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'BBBP'\n",
    "from chembench import load_data\n",
    "df, induces = load_data(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').values\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256, 128, 32]\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_loss'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1631 204 204\n",
      "WARNING:tensorflow:From /home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "epoch: 0001, loss: 0.2381 - val_loss: 0.3407; auc: 0.8527 - val_auc: 0.9491                                                                                                    \n",
      "epoch: 0002, loss: 0.2183 - val_loss: 0.2887; auc: 0.8535 - val_auc: 0.9448                                                                                                    \n",
      "epoch: 0003, loss: 0.1960 - val_loss: 0.2180; auc: 0.8580 - val_auc: 0.9444                                                                                                    \n",
      "epoch: 0004, loss: 0.1766 - val_loss: 0.1839; auc: 0.8693 - val_auc: 0.9465                                                                                                    \n",
      "epoch: 0005, loss: 0.1645 - val_loss: 0.1530; auc: 0.8831 - val_auc: 0.9540                                                                                                    \n",
      "epoch: 0006, loss: 0.1528 - val_loss: 0.1602; auc: 0.9022 - val_auc: 0.9604                                                                                                    \n",
      "epoch: 0007, loss: 0.1499 - val_loss: 0.1350; auc: 0.9121 - val_auc: 0.9627                                                                                                    \n",
      "epoch: 0008, loss: 0.1399 - val_loss: 0.1335; auc: 0.9237 - val_auc: 0.9636                                                                                                    \n",
      "epoch: 0009, loss: 0.1342 - val_loss: 0.1271; auc: 0.9306 - val_auc: 0.9653                                                                                                    \n",
      "epoch: 0010, loss: 0.1303 - val_loss: 0.1606; auc: 0.9392 - val_auc: 0.9643                                                                                                    \n",
      "epoch: 0011, loss: 0.1253 - val_loss: 0.1398; auc: 0.9429 - val_auc: 0.9645                                                                                                    \n",
      "epoch: 0012, loss: 0.1185 - val_loss: 0.1297; auc: 0.9495 - val_auc: 0.9647                                                                                                    \n",
      "epoch: 0013, loss: 0.1119 - val_loss: 0.1262; auc: 0.9525 - val_auc: 0.9648                                                                                                    \n",
      "epoch: 0014, loss: 0.1065 - val_loss: 0.1188; auc: 0.9562 - val_auc: 0.9652                                                                                                    \n",
      "epoch: 0015, loss: 0.1043 - val_loss: 0.1167; auc: 0.9607 - val_auc: 0.9648                                                                                                    \n",
      "epoch: 0016, loss: 0.1028 - val_loss: 0.1807; auc: 0.9629 - val_auc: 0.9625                                                                                                    \n",
      "epoch: 0017, loss: 0.1073 - val_loss: 0.1496; auc: 0.9645 - val_auc: 0.9640                                                                                                    \n",
      "epoch: 0018, loss: 0.0985 - val_loss: 0.1160; auc: 0.9671 - val_auc: 0.9657                                                                                                    \n",
      "epoch: 0019, loss: 0.0928 - val_loss: 0.1506; auc: 0.9703 - val_auc: 0.9630                                                                                                    \n",
      "epoch: 0020, loss: 0.0872 - val_loss: 0.1172; auc: 0.9727 - val_auc: 0.9653                                                                                                    \n",
      "epoch: 0021, loss: 0.0815 - val_loss: 0.1253; auc: 0.9757 - val_auc: 0.9638                                                                                                    \n",
      "epoch: 0022, loss: 0.0776 - val_loss: 0.1369; auc: 0.9776 - val_auc: 0.9631                                                                                                    \n",
      "epoch: 0023, loss: 0.0847 - val_loss: 0.1193; auc: 0.9771 - val_auc: 0.9652                                                                                                    \n",
      "epoch: 0024, loss: 0.0795 - val_loss: 0.1328; auc: 0.9802 - val_auc: 0.9636                                                                                                    \n",
      "epoch: 0025, loss: 0.0752 - val_loss: 0.1578; auc: 0.9823 - val_auc: 0.9613                                                                                                    \n",
      "epoch: 0026, loss: 0.0671 - val_loss: 0.1499; auc: 0.9841 - val_auc: 0.9622                                                                                                    \n",
      "epoch: 0027, loss: 0.0690 - val_loss: 0.1172; auc: 0.9840 - val_auc: 0.9649                                                                                                    \n",
      "epoch: 0028, loss: 0.0697 - val_loss: 0.1463; auc: 0.9859 - val_auc: 0.9618                                                                                                    \n",
      "epoch: 0029, loss: 0.0587 - val_loss: 0.1542; auc: 0.9877 - val_auc: 0.9619                                                                                                    \n",
      "epoch: 0030, loss: 0.0564 - val_loss: 0.1424; auc: 0.9883 - val_auc: 0.9621                                                                                                    \n",
      "epoch: 0031, loss: 0.0534 - val_loss: 0.1470; auc: 0.9895 - val_auc: 0.9615                                                                                                    \n",
      "epoch: 0032, loss: 0.0514 - val_loss: 0.1449; auc: 0.9903 - val_auc: 0.9609                                                                                                    \n",
      "epoch: 0033, loss: 0.0481 - val_loss: 0.1562; auc: 0.9909 - val_auc: 0.9611                                                                                                    \n",
      "epoch: 0034, loss: 0.0475 - val_loss: 0.1572; auc: 0.9918 - val_auc: 0.9596                                                                                                    \n",
      "epoch: 0035, loss: 0.0447 - val_loss: 0.1560; auc: 0.9924 - val_auc: 0.9602                                                                                                    \n",
      "epoch: 0036, loss: 0.0427 - val_loss: 0.1754; auc: 0.9930 - val_auc: 0.9596                                                                                                    \n",
      "epoch: 0037, loss: 0.0436 - val_loss: 0.1413; auc: 0.9931 - val_auc: 0.9611                                                                                                    \n",
      "epoch: 0038, loss: 0.0436 - val_loss: 0.2505; auc: 0.9936 - val_auc: 0.9586                                                                                                    \n",
      "epoch: 0039, loss: 0.0496 - val_loss: 0.1224; auc: 0.9934 - val_auc: 0.9639                                                                                                    \n",
      "epoch: 0040, loss: 0.0438 - val_loss: 0.2228; auc: 0.9944 - val_auc: 0.9570                                                                                                    \n",
      "epoch: 0041, loss: 0.0365 - val_loss: 0.1552; auc: 0.9945 - val_auc: 0.9620                                                                                                    \n",
      "epoch: 0042, loss: 0.0360 - val_loss: 0.2163; auc: 0.9952 - val_auc: 0.9572                                                                                                    \n",
      "epoch: 0043, loss: 0.0317 - val_loss: 0.1671; auc: 0.9954 - val_auc: 0.9601                                                                                                    \n",
      "epoch: 0044, loss: 0.0343 - val_loss: 0.2291; auc: 0.9959 - val_auc: 0.9565                                                                                                    \n",
      "epoch: 0045, loss: 0.0296 - val_loss: 0.1770; auc: 0.9959 - val_auc: 0.9610                                                                                                    \n",
      "epoch: 0046, loss: 0.0270 - val_loss: 0.2142; auc: 0.9965 - val_auc: 0.9564                                                                                                    \n",
      "epoch: 0047, loss: 0.0269 - val_loss: 0.2036; auc: 0.9968 - val_auc: 0.9568                                                                                                    \n",
      "epoch: 0048, loss: 0.0259 - val_loss: 0.2183; auc: 0.9971 - val_auc: 0.9568                                                                                                    \n",
      "epoch: 0049, loss: 0.0249 - val_loss: 0.2694; auc: 0.9973 - val_auc: 0.9560                                                                                                    \n",
      "epoch: 0050, loss: 0.0233 - val_loss: 0.2331; auc: 0.9975 - val_auc: 0.9566                                                                                                    \n",
      "epoch: 0051, loss: 0.0235 - val_loss: 0.2318; auc: 0.9980 - val_auc: 0.9552                                                                                                    \n",
      "epoch: 0052, loss: 0.0244 - val_loss: 0.1784; auc: 0.9977 - val_auc: 0.9584                                                                                                    \n",
      "epoch: 0053, loss: 0.0217 - val_loss: 0.2339; auc: 0.9981 - val_auc: 0.9556                                                                                                    \n",
      "epoch: 0054, loss: 0.0206 - val_loss: 0.3090; auc: 0.9985 - val_auc: 0.9518                                                                                                    \n",
      "epoch: 0055, loss: 0.0199 - val_loss: 0.2076; auc: 0.9984 - val_auc: 0.9575                                                                                                    \n",
      "epoch: 0056, loss: 0.0224 - val_loss: 0.2614; auc: 0.9983 - val_auc: 0.9569                                                                                                    \n",
      "epoch: 0057, loss: 0.0245 - val_loss: 0.3234; auc: 0.9982 - val_auc: 0.9531                                                                                                    \n",
      "epoch: 0058, loss: 0.0213 - val_loss: 0.1822; auc: 0.9984 - val_auc: 0.9561                                                                                                    \n",
      "epoch: 0059, loss: 0.0229 - val_loss: 0.3756; auc: 0.9986 - val_auc: 0.9510                                                                                                    \n",
      "epoch: 0060, loss: 0.0201 - val_loss: 0.1790; auc: 0.9985 - val_auc: 0.9566                                                                                                    \n",
      "epoch: 0061, loss: 0.0186 - val_loss: 0.3355; auc: 0.9989 - val_auc: 0.9514                                                                                                    \n",
      "epoch: 0062, loss: 0.0195 - val_loss: 0.2284; auc: 0.9989 - val_auc: 0.9547                                                                                                    \n",
      "epoch: 0063, loss: 0.0229 - val_loss: 0.3627; auc: 0.9990 - val_auc: 0.9499                                                                                                    \n",
      "epoch: 0064, loss: 0.0164 - val_loss: 0.2207; auc: 0.9990 - val_auc: 0.9553                                                                                                    \n",
      "epoch: 0065, loss: 0.0162 - val_loss: 0.3783; auc: 0.9990 - val_auc: 0.9499                                                                                                    \n",
      "epoch: 0066, loss: 0.0148 - val_loss: 0.2832; auc: 0.9991 - val_auc: 0.9521                                                                                                    \n",
      "epoch: 0067, loss: 0.0141 - val_loss: 0.2863; auc: 0.9992 - val_auc: 0.9521                                                                                                    \n",
      "epoch: 0068, loss: 0.0140 - val_loss: 0.3135; auc: 0.9992 - val_auc: 0.9508                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00068: early stopping\n",
      "1631 204 204\n",
      "Train on 1631 samples, validate on 204 samples\n",
      "Epoch 1/18\n",
      "1631/1631 [==============================] - 2s 922us/sample - loss: 0.2441 - val_loss: 0.3755\n",
      "Epoch 2/18\n",
      "1631/1631 [==============================] - 0s 242us/sample - loss: 0.2298 - val_loss: 0.3615\n",
      "Epoch 3/18\n",
      "1631/1631 [==============================] - 0s 253us/sample - loss: 0.2123 - val_loss: 0.2936\n",
      "Epoch 4/18\n",
      "1631/1631 [==============================] - 0s 253us/sample - loss: 0.1936 - val_loss: 0.2757\n",
      "Epoch 5/18\n",
      "1631/1631 [==============================] - 0s 251us/sample - loss: 0.1745 - val_loss: 0.2098\n",
      "Epoch 6/18\n",
      "1631/1631 [==============================] - 0s 256us/sample - loss: 0.1689 - val_loss: 0.1823\n",
      "Epoch 7/18\n",
      "1631/1631 [==============================] - 0s 261us/sample - loss: 0.1655 - val_loss: 0.1801\n",
      "Epoch 8/18\n",
      "1631/1631 [==============================] - 0s 254us/sample - loss: 0.1469 - val_loss: 0.1630\n",
      "Epoch 9/18\n",
      "1631/1631 [==============================] - 0s 247us/sample - loss: 0.1377 - val_loss: 0.1655\n",
      "Epoch 10/18\n",
      "1631/1631 [==============================] - 0s 249us/sample - loss: 0.1313 - val_loss: 0.1600\n",
      "Epoch 11/18\n",
      "1631/1631 [==============================] - 0s 240us/sample - loss: 0.1253 - val_loss: 0.1531\n",
      "Epoch 12/18\n",
      "1631/1631 [==============================] - 0s 253us/sample - loss: 0.1220 - val_loss: 0.1290\n",
      "Epoch 13/18\n",
      "1631/1631 [==============================] - 0s 238us/sample - loss: 0.1134 - val_loss: 0.1409\n",
      "Epoch 14/18\n",
      "1631/1631 [==============================] - 0s 246us/sample - loss: 0.1113 - val_loss: 0.1259\n",
      "Epoch 15/18\n",
      "1631/1631 [==============================] - 0s 235us/sample - loss: 0.1043 - val_loss: 0.1476\n",
      "Epoch 16/18\n",
      "1631/1631 [==============================] - 0s 252us/sample - loss: 0.1018 - val_loss: 0.1253\n",
      "Epoch 17/18\n",
      "1631/1631 [==============================] - 0s 243us/sample - loss: 0.0978 - val_loss: 0.1229\n",
      "Epoch 18/18\n",
      "1631/1631 [==============================] - 0s 265us/sample - loss: 0.0920 - val_loss: 0.1195\n",
      "1631 204 204\n",
      "Train on 1631 samples, validate on 204 samples\n",
      "Epoch 1/18\n",
      "1631/1631 [==============================] - 2s 985us/sample - loss: 0.2416 - val_loss: 0.3640\n",
      "Epoch 2/18\n",
      "1631/1631 [==============================] - 0s 241us/sample - loss: 0.2294 - val_loss: 0.3375\n",
      "Epoch 3/18\n",
      "1631/1631 [==============================] - 0s 247us/sample - loss: 0.2104 - val_loss: 0.2701\n",
      "Epoch 4/18\n",
      "1631/1631 [==============================] - 0s 249us/sample - loss: 0.1882 - val_loss: 0.2051\n",
      "Epoch 5/18\n",
      "1631/1631 [==============================] - 0s 251us/sample - loss: 0.1732 - val_loss: 0.1907\n",
      "Epoch 6/18\n",
      "1631/1631 [==============================] - 0s 266us/sample - loss: 0.1643 - val_loss: 0.1680\n",
      "Epoch 7/18\n",
      "1631/1631 [==============================] - 0s 260us/sample - loss: 0.1517 - val_loss: 0.1612\n",
      "Epoch 8/18\n",
      "1631/1631 [==============================] - 0s 256us/sample - loss: 0.1465 - val_loss: 0.1438\n",
      "Epoch 9/18\n",
      "1631/1631 [==============================] - 0s 247us/sample - loss: 0.1388 - val_loss: 0.1349\n",
      "Epoch 10/18\n",
      "1631/1631 [==============================] - 0s 245us/sample - loss: 0.1366 - val_loss: 0.1722\n",
      "Epoch 11/18\n",
      "1631/1631 [==============================] - 0s 245us/sample - loss: 0.1296 - val_loss: 0.1709\n",
      "Epoch 12/18\n",
      "1631/1631 [==============================] - 0s 237us/sample - loss: 0.1243 - val_loss: 0.1377\n",
      "Epoch 13/18\n",
      "1631/1631 [==============================] - 0s 252us/sample - loss: 0.1199 - val_loss: 0.1245\n",
      "Epoch 14/18\n",
      "1631/1631 [==============================] - 0s 236us/sample - loss: 0.1122 - val_loss: 0.1256\n",
      "Epoch 15/18\n",
      "1631/1631 [==============================] - 0s 246us/sample - loss: 0.1071 - val_loss: 0.1265\n",
      "Epoch 16/18\n",
      "1631/1631 [==============================] - 0s 236us/sample - loss: 0.1026 - val_loss: 0.1273\n",
      "Epoch 17/18\n",
      "1631/1631 [==============================] - 0s 247us/sample - loss: 0.0978 - val_loss: 0.1440\n",
      "Epoch 18/18\n",
      "1631/1631 [==============================] - 0s 239us/sample - loss: 0.0983 - val_loss: 0.1205\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "        performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391848925715387"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003263068546464242"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>valid_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>metric</th>\n",
       "      <th># trainable params</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BBBP</td>\n",
       "      <td>0.967070</td>\n",
       "      <td>0.965741</td>\n",
       "      <td>0.737065</td>\n",
       "      <td>ROC</td>\n",
       "      <td>803681</td>\n",
       "      <td>17</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BBBP</td>\n",
       "      <td>0.968289</td>\n",
       "      <td>0.962539</td>\n",
       "      <td>0.742942</td>\n",
       "      <td>ROC</td>\n",
       "      <td>803681</td>\n",
       "      <td>17</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BBBP</td>\n",
       "      <td>0.966114</td>\n",
       "      <td>0.961665</td>\n",
       "      <td>0.737547</td>\n",
       "      <td>ROC</td>\n",
       "      <td>803681</td>\n",
       "      <td>17</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  train_auc  valid_auc  test_auc metric  # trainable params  \\\n",
       "0      BBBP   0.967070   0.965741  0.737065    ROC              803681   \n",
       "1      BBBP   0.968289   0.962539  0.742942    ROC              803681   \n",
       "2      BBBP   0.966114   0.961665  0.737547    ROC              803681   \n",
       "\n",
       "   best_epoch  batch_size      lr  weight_decay  \n",
       "0          17         128  0.0001             0  \n",
       "1          17         128  0.0001             0  \n",
       "2          17         128  0.0001             0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
