{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### !/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from molmap.model import RegressionEstimator, MultiClassEstimator, MultiLabelEstimator\n",
    "from molmap import loadmap\n",
    "from molmap.show import imshow_wrap\n",
    "import molmap\n",
    "from molmap import MolMap\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "from joblib import load, dump\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from chembench import dataset\n",
    "\n",
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimized hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optimized hyper-parameters\n",
    "n_neighbors = 15\n",
    "min_dist = 0.75\n",
    "input_feature_maps = 'fingerprint'\n",
    "batch_size = 32\n",
    "dense_layers = [128, 32] #\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 1513\n"
     ]
    }
   ],
   "source": [
    "## random\n",
    "data = dataset.load_BACE()\n",
    "task_name = data.task_name\n",
    "task_type = data.task_type\n",
    "\n",
    "Y = data.y\n",
    "df = data.df\n",
    "n_outputs = Y.shape[1]\n",
    "\n",
    "gpuid = 5 # which gpu to use\n",
    "\n",
    "random_seeds = [2, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(metric='precomputed', min_dist=0.75, random_state=1, verbose=2)\n",
      "Construct fuzzy simplicial set\n",
      "Thu Sep 24 13:14:15 2020 Finding Nearest Neighbors\n",
      "Thu Sep 24 13:14:15 2020 Finished Nearest Neighbor Search\n",
      "Thu Sep 24 13:14:17 2020 Construct embedding\n",
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Thu Sep 24 13:14:21 2020 Finished embedding\n",
      "2020-09-24 13:14:21,167 - \u001b[32mINFO\u001b[0m - [bidd-molmap]\u001b[0m - Applying grid feature map(assignment), this may take several minutes(1~30 min)\u001b[0m\n",
      "2020-09-24 13:14:23,244 - \u001b[32mINFO\u001b[0m - [bidd-molmap]\u001b[0m - Finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mp = loadmap('../../fingerprint.mp')\n",
    "mp.fit(method = 'umap', min_dist = min_dist, n_neighbors = n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1513/1513 [00:34<00:00, 44.42it/s]\n"
     ]
    }
   ],
   "source": [
    "X = mp.batch_transform(df.smiles.tolist(),  scale=False, n_jobs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 36, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmap_shape1= X.shape[1:]\n",
    "fmap_shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/raid/shenwanxiang/08_Robustness/dataset_induces/split\" #split\n",
    "\n",
    "result_file = 'OPT_%s.csv' % task_name\n",
    "\n",
    "with open(result_file, 'w+') as f:\n",
    "    f.write('task_name, seed, valid_auc, test_auc\\n')\n",
    "\n",
    "# the dense layers for these multi outputs tasks\n",
    "\n",
    "res = []\n",
    "for seed in random_seeds:\n",
    "\n",
    "    train_path = os.path.join(file_path, task_name,\"%s\" % seed, \"train.csv\")\n",
    "    valid_path = os.path.join(file_path, task_name,\"%s\" % seed, \"val.csv\")\n",
    "    test_path = os.path.join(file_path, task_name,\"%s\" % seed, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_idx = df[df.smiles.isin(train_df.smiles)].index.tolist()\n",
    "    valid_idx = df[df.smiles.isin(valid_df.smiles)].index.tolist()\n",
    "    test_idx = df[df.smiles.isin(test_df.smiles)].index.tolist()\n",
    "\n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    y_train = Y[train_idx]\n",
    "\n",
    "    X_valid = X[valid_idx]\n",
    "    y_valid = Y[valid_idx]\n",
    "\n",
    "    X_test = X[test_idx]\n",
    "    y_test = Y[test_idx]    \n",
    "\n",
    "    clf = MultiLabelEstimator(n_outputs,\n",
    "                              fmap_shape1,lr = lr,\n",
    "                              batch_size = batch_size,\n",
    "                              dense_layers = dense_layers,\n",
    "                              gpuid = gpuid, \n",
    "                              monitor = 'val_auc',\n",
    "                             ) \n",
    "    clf.fit(X_train,y_train, X_valid, y_valid)\n",
    "\n",
    "\n",
    "    train_aucs = clf._performance.evaluate(X_train,y_train)\n",
    "    valid_aucs = clf._performance.evaluate(X_valid,y_valid)            \n",
    "    test_aucs = clf._performance.evaluate(X_test,y_test)\n",
    "\n",
    "    train_auc = np.nanmean(train_aucs)\n",
    "    valid_auc = np.nanmean(valid_aucs)\n",
    "    test_auc = np.nanmean(test_aucs)\n",
    "\n",
    "    final_res = {'seed': seed,\n",
    "                 \"task_name\": task_name,\n",
    "                 'train_auc':train_auc, \n",
    "                 'valid_auc':valid_auc,                      \n",
    "                 'test_auc':test_auc,}\n",
    "\n",
    "    print(final_res)\n",
    "\n",
    "    with open(result_file, 'a+') as f:\n",
    "        f.write('%s, %s, %s, %s\\n' % (task_name, seed, valid_auc, test_auc))\n",
    "\n",
    "    res.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
