{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/sxh/Research/AttentiveFP/code',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set_style(\"darkgrid\")\n",
    "from IPython.display import SVG, display\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  9999\n",
      "number of successfully processed smiles:  9999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = ['Loge EC50']\n",
    "\n",
    "raw_filename = \"/home/sxh/Research/AttentiveFP/data/malaria-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename, header = None)\n",
    "smiles_tasks_df.columns = ['Loge EC50', 'smiles']\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "# print(smiles_tasks_df)\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 68\n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 800\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 200\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not processed items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loge EC50</th>\n",
       "      <th>smiles</th>\n",
       "      <th>cano_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Loge EC50, smiles, cano_smiles]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "print(\"not processed items\")\n",
    "uncovered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, loss_function):\n",
    "    model.train()\n",
    "    np.random.seed(epoch)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        atoms_prediction, mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "def eval(model, dataset):\n",
    "    model.eval()\n",
    "    test_MAE_list = []\n",
    "    test_MSE_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "    \n",
    "    try:\n",
    "        for counter, test_batch in enumerate(batch_list):\n",
    "            batch_df = dataset.loc[test_batch,:]\n",
    "            smiles_list = batch_df.cano_smiles.values\n",
    "    #         print(batch_df)\n",
    "            y_val = batch_df[tasks[0]].values\n",
    "\n",
    "            x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "            atoms_prediction, mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "            MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')        \n",
    "            MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "    #         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "\n",
    "            test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "            test_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    except:pass\n",
    "    return np.array(test_MAE_list).mean(), np.array(test_MSE_list).mean()\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, dataset):\n",
    "    model.eval()\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "        \n",
    "    preds = []\n",
    "    for counter, test_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[test_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        atoms_prediction, mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "        \n",
    "        preds.append(mol_prediction.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    return np.concatenate(preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Malaria\n",
      "8001 999 999\n",
      "0 1.2280173 1.249661\n",
      "1 1.224023 1.2429434\n",
      "2 1.2123165 1.2437472\n",
      "3 1.218979 1.2578886\n",
      "4 1.2040884 1.2378714\n",
      "5 1.1952045 1.2204531\n",
      "6 1.1916541 1.2117571\n",
      "7 1.1782123 1.2103775\n",
      "8 1.1688482 1.1934291\n",
      "9 1.1588559 1.1884705\n",
      "10 1.2771977 1.2618192\n",
      "11 1.1726156 1.2055337\n",
      "12 1.1588364 1.1833348\n",
      "13 1.152611 1.1826605\n",
      "14 1.1731627 1.1804516\n",
      "15 1.1455301 1.1766757\n",
      "16 1.1474288 1.1852576\n",
      "17 1.141565 1.1721272\n",
      "18 1.1365047 1.163044\n",
      "19 1.1346749 1.1662105\n",
      "20 1.1315064 1.1607332\n",
      "21 1.1295086 1.1589222\n",
      "22 1.1312778 1.1665053\n",
      "23 1.1267121 1.1571592\n",
      "24 1.1262385 1.1478894\n",
      "25 1.1296579 1.1647712\n",
      "26 1.1234508 1.1563395\n",
      "27 1.1215832 1.1553711\n",
      "28 1.1180586 1.144673\n",
      "29 1.1165222 1.1436833\n",
      "30 1.1253166 1.1638986\n",
      "31 1.1152207 1.140604\n",
      "32 1.1128373 1.1453542\n",
      "33 1.11364 1.1502875\n",
      "34 1.1199429 1.1624535\n",
      "35 1.1124774 1.1484196\n",
      "36 1.1086583 1.1418033\n",
      "37 1.1072657 1.1397709\n",
      "38 1.1053222 1.1384939\n",
      "39 1.1235437 1.1709476\n",
      "40 1.1055138 1.1373227\n",
      "41 1.103139 1.1365916\n",
      "42 1.1043915 1.129678\n",
      "43 1.1126008 1.1574992\n",
      "44 1.1017493 1.137943\n",
      "45 1.0983642 1.1329231\n",
      "46 1.1059291 1.1504107\n",
      "47 1.1153507 1.1658254\n",
      "48 1.1007334 1.1376132\n",
      "49 1.1076306 1.1514416\n",
      "50 1.1150578 1.1348376\n",
      "51 1.1075388 1.1541965\n",
      "52 1.0983392 1.1363775\n",
      "best epoch: 42 \n",
      " valid RMSE: 1.129678\n",
      "16 Malaria\n",
      "8001 999 999\n",
      "0 1.500228 1.51427\n",
      "1 1.2138608 1.2503471\n",
      "2 1.2173046 1.2579075\n",
      "3 1.2160094 1.2566206\n",
      "4 1.2019229 1.2411106\n",
      "5 1.1929258 1.2304155\n",
      "6 1.1948832 1.2298648\n",
      "7 1.1777229 1.2159013\n",
      "8 1.1742734 1.2142451\n",
      "9 1.1615583 1.199679\n",
      "10 1.2072963 1.2357503\n",
      "11 1.1580471 1.1959358\n",
      "12 1.1484258 1.1870519\n",
      "13 1.1415813 1.1781154\n",
      "14 1.1584362 1.1887344\n",
      "15 1.1417341 1.1735634\n",
      "16 1.1354549 1.1678681\n",
      "17 1.1385807 1.1703689\n",
      "18 1.1310139 1.1627232\n",
      "19 1.1285224 1.1595169\n",
      "20 1.1231654 1.1536945\n",
      "21 1.1251616 1.1544293\n",
      "22 1.1210896 1.1496925\n",
      "23 1.1201206 1.1468486\n",
      "24 1.1212864 1.1479725\n",
      "25 1.1254051 1.1544101\n",
      "26 1.1139796 1.1412408\n",
      "27 1.1131554 1.1408558\n",
      "28 1.1102977 1.1379749\n",
      "29 1.1096544 1.1353676\n",
      "30 1.1194406 1.1515621\n",
      "31 1.110929 1.1407503\n",
      "32 1.1080935 1.1383158\n",
      "33 1.103985 1.1338683\n",
      "34 1.1110272 1.142843\n",
      "35 1.1040215 1.1361666\n",
      "36 1.1025218 1.1362206\n",
      "37 1.0990766 1.130559\n",
      "38 1.1022494 1.1349369\n",
      "39 1.1183997 1.1495421\n",
      "40 1.101164 1.1309052\n",
      "41 1.0980201 1.1314058\n",
      "42 1.097054 1.1265819\n",
      "43 1.1053294 1.1402383\n",
      "44 1.0981436 1.1331643\n",
      "45 1.0914532 1.1253418\n",
      "46 1.0935088 1.1290206\n",
      "47 1.1059 1.1424874\n",
      "48 1.097102 1.1334621\n",
      "49 1.1080365 1.1423125\n",
      "50 1.1048611 1.1368877\n",
      "51 1.1042626 1.1345861\n",
      "52 1.0945258 1.127145\n",
      "53 1.0996876 1.1357715\n",
      "54 1.0982282 1.1333522\n",
      "55 1.0904552 1.1250067\n",
      "56 1.0885612 1.122148\n",
      "57 1.0858159 1.120659\n",
      "58 1.0840882 1.1189352\n",
      "59 1.0834707 1.1216104\n",
      "60 1.0872363 1.1247137\n",
      "61 1.0814862 1.118805\n",
      "62 1.0803937 1.1181753\n",
      "63 1.0879681 1.1236526\n",
      "64 1.1014822 1.1416433\n",
      "65 1.0890368 1.1276932\n",
      "66 1.0942506 1.1275631\n",
      "67 1.1016642 1.134912\n",
      "68 1.0890472 1.1257093\n",
      "69 1.0836513 1.1195973\n",
      "70 1.0855156 1.1230776\n",
      "71 1.089163 1.1278497\n",
      "72 1.0826526 1.121247\n",
      "73 1.0792116 1.1177233\n",
      "74 1.0777097 1.1171664\n",
      "75 1.0765803 1.1148067\n",
      "76 1.0746963 1.1139401\n",
      "77 1.0760714 1.115901\n",
      "78 1.0863968 1.1285043\n",
      "79 1.074956 1.114615\n",
      "80 1.0741563 1.1134415\n",
      "81 1.0707177 1.1114087\n",
      "82 1.0700712 1.1103877\n",
      "83 1.0685768 1.1110675\n",
      "84 1.067475 1.1089572\n",
      "85 1.0680972 1.1112725\n",
      "86 1.0657334 1.1079187\n",
      "87 1.065638 1.1081171\n",
      "88 1.0647098 1.1072258\n",
      "89 1.0681056 1.110952\n",
      "90 1.0631136 1.1068302\n",
      "91 1.0620204 1.1051102\n",
      "92 1.0616548 1.1064454\n",
      "93 1.0628604 1.106999\n",
      "94 1.062023 1.1063929\n",
      "95 1.0623378 1.1070611\n",
      "96 1.0761814 1.1236802\n",
      "97 1.0634896 1.1102774\n",
      "98 1.0597622 1.1065439\n",
      "99 1.0794165 1.1281408\n",
      "100 1.0645059 1.1088873\n",
      "101 1.0575405 1.1010945\n",
      "102 1.0582895 1.1022801\n",
      "103 1.054385 1.1002144\n",
      "104 1.059997 1.1081696\n",
      "105 1.0839006 1.1250303\n",
      "106 1.0558068 1.1018801\n",
      "107 1.0548558 1.1010227\n",
      "108 1.0525006 1.0991639\n",
      "109 1.0526931 1.0992867\n",
      "110 1.0495696 1.0977331\n",
      "111 1.0479119 1.0962069\n",
      "112 1.0512207 1.1031542\n",
      "113 1.0509821 1.1004658\n",
      "114 1.0502483 1.0988011\n",
      "115 1.0531375 1.1019479\n",
      "116 1.0483857 1.0996705\n",
      "117 1.0483358 1.1006823\n",
      "118 1.045441 1.0987419\n",
      "119 1.0456902 1.0981214\n",
      "120 1.0459388 1.0945953\n",
      "121 1.0920074 1.1446784\n",
      "122 1.0519062 1.099015\n",
      "123 1.0596715 1.1115159\n",
      "124 1.0552433 1.1016026\n",
      "125 1.0497411 1.1019111\n",
      "126 1.042581 1.0938866\n",
      "127 1.040958 1.0936283\n",
      "128 1.0422299 1.0956689\n",
      "129 1.0394769 1.0932375\n",
      "130 1.0466285 1.0998955\n",
      "131 1.0404392 1.0936471\n",
      "132 1.0394703 1.0924518\n",
      "133 1.0386997 1.0926005\n",
      "134 1.0509976 1.1045991\n",
      "135 1.0355002 1.0903952\n",
      "136 1.0354272 1.090842\n",
      "137 1.0373522 1.0924026\n",
      "138 1.0335896 1.0884511\n",
      "139 1.032902 1.0903428\n",
      "140 1.0317618 1.0901167\n",
      "141 1.045189 1.1054792\n",
      "142 1.0372291 1.0970911\n",
      "143 1.0331048 1.0939418\n",
      "144 1.0323395 1.0956535\n",
      "145 1.0339725 1.095592\n",
      "146 1.029573 1.0920928\n",
      "147 1.0494313 1.1157476\n",
      "148 1.0438584 1.1032648\n",
      "best epoch: 138 \n",
      " valid RMSE: 1.0884511\n",
      "32 Malaria\n",
      "8001 999 999\n",
      "0 1.2435266 1.1803312\n",
      "1 1.211152 1.1568342\n",
      "2 1.2074397 1.1601695\n",
      "3 1.2167203 1.1751478\n",
      "4 1.2005798 1.1542624\n",
      "5 1.1962332 1.1458937\n",
      "6 1.2001632 1.146806\n",
      "7 1.1923609 1.1501005\n",
      "8 1.1850204 1.1370319\n",
      "9 1.1805787 1.1372149\n",
      "10 1.2089207 1.1558492\n",
      "11 1.1781834 1.1356934\n",
      "12 1.1728103 1.1256644\n",
      "13 1.1637253 1.1211106\n",
      "14 1.173463 1.1248256\n",
      "15 1.1552992 1.1108268\n",
      "16 1.1550486 1.1173053\n",
      "17 1.1471316 1.104748\n",
      "18 1.1421585 1.1018701\n",
      "19 1.1391904 1.1011721\n",
      "20 1.1339115 1.0950333\n",
      "21 1.1415578 1.1057363\n",
      "22 1.1388047 1.101188\n",
      "23 1.1329454 1.090102\n",
      "24 1.1270689 1.0868565\n",
      "25 1.1247245 1.0858182\n",
      "26 1.1217427 1.0815262\n",
      "27 1.1203668 1.0812155\n",
      "28 1.117645 1.0777303\n",
      "29 1.1179897 1.0772815\n",
      "30 1.1291819 1.0965916\n",
      "31 1.11424 1.0764943\n",
      "32 1.1129965 1.0765692\n",
      "33 1.1090099 1.0726694\n",
      "34 1.1166198 1.0824226\n",
      "35 1.1098475 1.0711944\n",
      "36 1.105248 1.0688375\n",
      "37 1.1074634 1.0731119\n",
      "38 1.1024201 1.0662622\n",
      "39 1.1166089 1.0858632\n",
      "40 1.1031891 1.069758\n",
      "41 1.1001785 1.0672592\n",
      "42 1.0998389 1.0643187\n",
      "43 1.0975211 1.0636345\n",
      "44 1.0975913 1.0669627\n",
      "45 1.0934101 1.0595745\n",
      "46 1.102054 1.0726\n",
      "47 1.1121522 1.0873406\n",
      "48 1.0962929 1.0639101\n",
      "49 1.0929763 1.0622191\n",
      "50 1.0929666 1.0598919\n",
      "51 1.0923408 1.0627413\n",
      "52 1.0899034 1.0588887\n",
      "53 1.1099297 1.0831857\n",
      "54 1.0885801 1.0563904\n",
      "55 1.0889295 1.0586998\n",
      "56 1.0886146 1.0580217\n",
      "57 1.0840405 1.0530235\n",
      "58 1.0834612 1.0544344\n",
      "59 1.0817809 1.0509872\n",
      "60 1.0808792 1.0502111\n",
      "61 1.0794343 1.0495142\n",
      "62 1.0787021 1.0492678\n",
      "63 1.0801572 1.0496405\n",
      "64 1.0804416 1.0520899\n",
      "65 1.0786426 1.0505662\n",
      "66 1.1135832 1.0883621\n",
      "67 1.0895348 1.0625046\n",
      "68 1.0829517 1.0549756\n",
      "69 1.0805143 1.0534307\n",
      "70 1.0886526 1.0644104\n",
      "71 1.083636 1.061244\n",
      "72 1.0807521 1.0554276\n",
      "73 1.0752115 1.048889\n",
      "74 1.0738147 1.0479636\n",
      "75 1.0806744 1.0533954\n",
      "76 1.0719819 1.0455635\n",
      "77 1.0742822 1.0453715\n",
      "78 1.070102 1.0456697\n",
      "79 1.069581 1.0436975\n",
      "80 1.0712922 1.0440967\n",
      "81 1.0680972 1.0429615\n",
      "82 1.0705552 1.0478873\n",
      "83 1.0689944 1.0449774\n",
      "84 1.0663251 1.0413356\n",
      "85 1.0717602 1.0488688\n",
      "86 1.0672526 1.0424271\n",
      "87 1.0655153 1.0418041\n",
      "88 1.0652303 1.04068\n",
      "89 1.0636706 1.0404971\n",
      "90 1.0620215 1.0381176\n",
      "91 1.06229 1.0367472\n",
      "92 1.0627928 1.0400552\n",
      "93 1.0588906 1.0349424\n",
      "94 1.0573577 1.0346576\n",
      "95 1.0571359 1.0334984\n",
      "96 1.0886254 1.0727385\n",
      "97 1.064046 1.0447211\n",
      "98 1.0611601 1.0436882\n",
      "99 1.0711224 1.0524385\n",
      "100 1.0620034 1.0426065\n",
      "101 1.0551351 1.0367099\n",
      "102 1.0533098 1.0326682\n",
      "103 1.0525542 1.029837\n",
      "104 1.0529882 1.034272\n",
      "105 1.0838908 1.060978\n",
      "106 1.056671 1.0344336\n",
      "107 1.0579225 1.036392\n",
      "108 1.0511637 1.0322663\n",
      "109 1.0506034 1.0313535\n",
      "110 1.0476699 1.0290118\n",
      "111 1.0474074 1.0286967\n",
      "112 1.0469096 1.0308123\n",
      "113 1.0755574 1.0601145\n",
      "114 1.055704 1.0361522\n",
      "115 1.0475614 1.0284463\n",
      "116 1.0449973 1.0284145\n",
      "117 1.0444453 1.0249066\n",
      "118 1.042711 1.0263457\n",
      "119 1.0474315 1.0275415\n",
      "120 1.046308 1.026463\n",
      "121 1.0771043 1.0601363\n",
      "122 1.0667773 1.0416355\n",
      "123 1.0481974 1.0326178\n",
      "124 1.0464473 1.0291603\n",
      "125 1.0438071 1.0274584\n",
      "126 1.0442759 1.0273583\n",
      "127 1.0432473 1.0257242\n",
      "best epoch: 117 \n",
      " valid RMSE: 1.0249066\n",
      "64 Malaria\n",
      "8001 999 999\n",
      "0 1.8593456 1.9335446\n",
      "1 1.2145822 1.2281826\n",
      "2 1.2114677 1.2207919\n",
      "3 1.2082963 1.2155342\n",
      "4 1.2073482 1.2127204\n",
      "5 1.2000009 1.2115487\n",
      "6 1.2047659 1.2235507\n",
      "7 1.1913388 1.2003889\n",
      "8 1.1866682 1.1953974\n",
      "9 1.1794441 1.1905216\n",
      "10 1.2102284 1.2366476\n",
      "11 1.1752183 1.1821202\n",
      "12 1.1675854 1.1711879\n",
      "13 1.1597866 1.1622896\n",
      "14 1.1671975 1.1787049\n",
      "15 1.1505796 1.1558245\n",
      "16 1.1744012 1.1571146\n",
      "17 1.1492803 1.1443729\n",
      "18 1.1418734 1.1426291\n",
      "19 1.1376818 1.133208\n",
      "20 1.1336409 1.1302956\n",
      "21 1.1427486 1.1306164\n",
      "22 1.1372179 1.1315366\n",
      "23 1.1333647 1.1413516\n",
      "24 1.1277019 1.1318284\n",
      "25 1.1254157 1.124493\n",
      "26 1.1223286 1.1224729\n",
      "27 1.1193644 1.1214951\n",
      "28 1.1182941 1.1255581\n",
      "29 1.116543 1.1254117\n",
      "30 1.1338185 1.1274095\n",
      "31 1.1154191 1.1253383\n",
      "32 1.1109507 1.1165776\n",
      "33 1.1097155 1.1137339\n",
      "34 1.1112357 1.1132948\n",
      "35 1.1102468 1.1136475\n",
      "36 1.1056153 1.1122952\n",
      "37 1.1033969 1.1123633\n",
      "38 1.1022588 1.1162016\n",
      "39 1.1120203 1.1137563\n",
      "40 1.1042502 1.1187072\n",
      "41 1.0997418 1.1115865\n",
      "42 1.1099852 1.131833\n",
      "43 1.1159748 1.1260097\n",
      "44 1.1048455 1.113929\n",
      "45 1.0965217 1.1137983\n",
      "46 1.1016239 1.1104934\n",
      "47 1.0992965 1.1088346\n",
      "48 1.0972533 1.1065305\n",
      "49 1.100078 1.1109843\n",
      "50 1.1092216 1.1363705\n",
      "51 1.101547 1.111544\n",
      "52 1.0925401 1.1079886\n",
      "53 1.0945407 1.1073333\n",
      "54 1.0968232 1.1084329\n",
      "55 1.0946428 1.116949\n",
      "56 1.0904878 1.1097239\n",
      "57 1.0881 1.1067697\n",
      "58 1.0873413 1.1048803\n",
      "59 1.0860994 1.1049361\n",
      "60 1.0865605 1.1032498\n",
      "61 1.0872518 1.1132563\n",
      "62 1.0880898 1.1044914\n",
      "63 1.0830765 1.1068913\n",
      "64 1.087561 1.1037228\n",
      "65 1.0809753 1.1037005\n",
      "66 1.1002693 1.1157001\n",
      "67 1.0942267 1.1229439\n",
      "68 1.0844208 1.1062392\n",
      "69 1.0817741 1.1043602\n",
      "70 1.0870345 1.1033274\n",
      "71 1.0833033 1.0978243\n",
      "72 1.0826188 1.1003541\n",
      "73 1.0796149 1.1024532\n",
      "74 1.077292 1.1056378\n",
      "75 1.0769604 1.1062618\n",
      "76 1.0749446 1.1022005\n",
      "77 1.0741538 1.1026036\n",
      "78 1.0765675 1.1006446\n",
      "79 1.0756152 1.0977039\n",
      "80 1.0748879 1.1018463\n",
      "81 1.0725926 1.101214\n",
      "82 1.0724286 1.1047387\n",
      "83 1.0728675 1.0977662\n",
      "84 1.0712137 1.0976686\n",
      "85 1.0694848 1.099179\n",
      "86 1.0679048 1.0996971\n",
      "87 1.0670022 1.0981784\n",
      "88 1.0663505 1.0977228\n",
      "89 1.0715533 1.0990671\n",
      "90 1.0650338 1.0970469\n",
      "91 1.0719191 1.1143943\n",
      "92 1.0675948 1.0982896\n",
      "93 1.0658492 1.099649\n",
      "94 1.0622021 1.0993723\n",
      "95 1.0607839 1.0978996\n",
      "96 1.0994458 1.1241757\n",
      "97 1.0667549 1.1011268\n",
      "98 1.0749166 1.1052294\n",
      "99 1.1153548 1.1418083\n",
      "100 1.0648484 1.0928416\n",
      "101 1.061412 1.0934011\n",
      "102 1.059278 1.0988374\n",
      "103 1.0571227 1.097319\n",
      "104 1.0587271 1.0950819\n",
      "105 1.066969 1.116085\n",
      "106 1.0655953 1.0999451\n",
      "107 1.0634817 1.1033312\n",
      "108 1.0586257 1.093492\n",
      "109 1.0568929 1.0951173\n",
      "110 1.0621197 1.1093665\n",
      "best epoch: 100 \n",
      " valid RMSE: 1.0928416\n",
      "128 Malaria\n",
      "8001 999 999\n",
      "0 1.6046404 1.5666095\n",
      "1 1.2099049 1.2276703\n",
      "2 1.2096874 1.233993\n",
      "3 1.2121733 1.2398329\n",
      "4 1.200838 1.2189085\n",
      "5 1.201835 1.2124842\n",
      "6 1.1973058 1.207695\n",
      "7 1.1938366 1.217405\n",
      "8 1.185314 1.201624\n",
      "9 1.1788063 1.1978564\n",
      "10 1.1911402 1.1909189\n",
      "11 1.1692235 1.1835731\n",
      "12 1.1594516 1.177482\n",
      "13 1.1496468 1.1648141\n",
      "14 1.196466 1.1845331\n",
      "15 1.1521889 1.1634876\n",
      "16 1.1463736 1.1701007\n",
      "17 1.1403924 1.1633463\n",
      "18 1.1379634 1.14946\n",
      "19 1.1332414 1.1454601\n",
      "20 1.136343 1.1569791\n",
      "21 1.133385 1.1581395\n",
      "22 1.1291155 1.1450242\n",
      "23 1.125121 1.1450447\n",
      "24 1.1253101 1.1383342\n",
      "25 1.1323873 1.1632328\n",
      "26 1.1200217 1.1400576\n",
      "27 1.1176634 1.1376209\n",
      "28 1.1156597 1.1387717\n",
      "29 1.1141864 1.1391354\n",
      "30 1.1239218 1.1591485\n",
      "31 1.1128343 1.130223\n",
      "32 1.1104428 1.1360718\n",
      "33 1.1078006 1.1293712\n",
      "34 1.1069465 1.1299483\n",
      "35 1.1109577 1.1410526\n",
      "36 1.1039757 1.127848\n",
      "37 1.1030954 1.1290973\n",
      "38 1.1003495 1.1219318\n",
      "39 1.1301944 1.1728268\n",
      "40 1.1014413 1.1242732\n",
      "41 1.102202 1.1301717\n",
      "42 1.102551 1.1168404\n",
      "43 1.1233552 1.1662858\n",
      "44 1.1013087 1.129563\n",
      "45 1.0968655 1.1196159\n",
      "46 1.1053597 1.1384163\n",
      "47 1.0945443 1.1195203\n",
      "48 1.0961024 1.1250625\n",
      "49 1.1036829 1.1417555\n",
      "50 1.1057315 1.1128361\n",
      "51 1.0992492 1.1323321\n",
      "52 1.0910656 1.117584\n",
      "53 1.0950944 1.1269842\n",
      "54 1.1271281 1.1766708\n",
      "55 1.1251343 1.1416539\n",
      "56 1.106162 1.1316681\n",
      "57 1.0995666 1.1226339\n",
      "58 1.0951836 1.1168926\n",
      "59 1.0933793 1.118065\n",
      "60 1.0966543 1.1269131\n",
      "best epoch: 50 \n",
      " valid RMSE: 1.1128361\n",
      "256 Malaria\n",
      "8001 999 999\n",
      "0 1.2129196 1.1654528\n",
      "1 1.2035304 1.1619378\n",
      "2 1.2026396 1.1705301\n",
      "3 1.2162439 1.1931894\n",
      "4 1.2036481 1.1773251\n",
      "5 1.1868503 1.1539205\n",
      "6 1.1834713 1.1484879\n",
      "7 1.1732756 1.1505753\n",
      "8 1.167778 1.1476749\n",
      "9 1.1586876 1.1422101\n",
      "10 1.1829675 1.1557399\n",
      "11 1.1548564 1.1371497\n",
      "12 1.1460971 1.1351621\n",
      "13 1.1402502 1.1344823\n",
      "14 1.1571915 1.145541\n",
      "15 1.1367089 1.133542\n",
      "16 1.1426342 1.1549606\n",
      "17 1.1337166 1.1418084\n",
      "18 1.1263589 1.1324155\n",
      "19 1.1232051 1.1316876\n",
      "20 1.1200846 1.130552\n",
      "21 1.1350338 1.1538712\n",
      "22 1.1309704 1.1413529\n",
      "23 1.1222162 1.1258056\n",
      "24 1.1186175 1.1240319\n",
      "25 1.1240003 1.1366799\n",
      "26 1.1129161 1.1221105\n",
      "27 1.1103731 1.1222684\n",
      "28 1.1086067 1.1221058\n",
      "29 1.1062062 1.1199523\n",
      "30 1.1146263 1.1315544\n",
      "31 1.1034395 1.1139232\n",
      "32 1.102074 1.113123\n",
      "33 1.1033765 1.1171802\n",
      "34 1.1186508 1.1381578\n",
      "35 1.1006343 1.1156102\n",
      "36 1.0966529 1.1083112\n",
      "37 1.0940278 1.1041615\n",
      "38 1.0970879 1.104086\n",
      "39 1.1196003 1.1354464\n",
      "40 1.0955211 1.1039164\n",
      "41 1.0919178 1.1013597\n",
      "42 1.0960482 1.1025075\n",
      "43 1.097187 1.1118579\n",
      "44 1.0941142 1.1060537\n",
      "45 1.0892344 1.0998418\n",
      "46 1.0960782 1.110809\n",
      "47 1.0932745 1.1075383\n",
      "48 1.0884997 1.0979434\n",
      "49 1.0859138 1.0983529\n",
      "50 1.0915008 1.099554\n",
      "51 1.0907701 1.1053195\n",
      "52 1.0825465 1.0950226\n",
      "53 1.0884197 1.1063024\n",
      "54 1.0854895 1.1009244\n",
      "55 1.0837417 1.0931269\n",
      "56 1.0809906 1.0927639\n",
      "57 1.0791354 1.0901374\n",
      "58 1.0775326 1.0905157\n",
      "59 1.0765235 1.0876784\n",
      "60 1.084247 1.100274\n",
      "61 1.0757736 1.0856361\n",
      "62 1.0765597 1.0894626\n",
      "63 1.0730655 1.084552\n",
      "64 1.0772899 1.0906291\n",
      "65 1.0742716 1.0897079\n",
      "66 1.1342734 1.154915\n",
      "67 1.083638 1.0865473\n",
      "68 1.0779363 1.0820847\n",
      "69 1.0749296 1.0819572\n",
      "70 1.0816649 1.094674\n",
      "71 1.08193 1.0911577\n",
      "72 1.0765675 1.0865861\n",
      "73 1.0717082 1.079747\n",
      "74 1.071447 1.0828918\n",
      "75 1.0730094 1.0792675\n",
      "76 1.0676459 1.07897\n",
      "77 1.0665537 1.0778975\n",
      "78 1.0701764 1.0839429\n",
      "79 1.0701977 1.0850283\n",
      "80 1.0712696 1.0800294\n",
      "81 1.0650668 1.0770187\n",
      "82 1.0646589 1.078109\n",
      "83 1.0633507 1.0770903\n",
      "84 1.0619788 1.0741094\n",
      "85 1.0627569 1.0760536\n",
      "86 1.0606384 1.0747882\n",
      "87 1.0612533 1.0770867\n",
      "88 1.0583096 1.0726224\n",
      "89 1.0588943 1.0759642\n",
      "90 1.0570965 1.0723294\n",
      "91 1.056316 1.0709016\n",
      "92 1.0568593 1.0696932\n",
      "93 1.0543307 1.0715742\n",
      "94 1.0543754 1.0681409\n",
      "95 1.0551056 1.0710208\n",
      "96 1.0777214 1.1002256\n",
      "97 1.0572891 1.0747565\n",
      "98 1.0579951 1.0807847\n",
      "99 1.0651999 1.0879838\n",
      "100 1.0570714 1.0744992\n",
      "101 1.0528897 1.0741034\n",
      "102 1.0520192 1.0725274\n",
      "103 1.0494525 1.0696366\n",
      "104 1.0488955 1.0680919\n",
      "105 1.0630553 1.0765502\n",
      "106 1.0529665 1.0756412\n",
      "107 1.0497814 1.0733198\n",
      "108 1.0496386 1.073971\n",
      "109 1.0459018 1.0668101\n",
      "110 1.0472392 1.0640125\n",
      "111 1.0448121 1.0668133\n",
      "112 1.043349 1.0643293\n",
      "113 1.0608791 1.0869803\n",
      "114 1.0491301 1.0654598\n",
      "115 1.0443567 1.0633017\n",
      "116 1.0451657 1.068738\n",
      "117 1.0434761 1.062939\n",
      "118 1.040628 1.0610195\n",
      "119 1.044149 1.064068\n",
      "120 1.0468336 1.0599464\n",
      "121 1.0828828 1.110306\n",
      "122 1.0499038 1.0671895\n",
      "123 1.0444157 1.0672678\n",
      "124 1.0418903 1.06209\n",
      "125 1.039436 1.0624721\n",
      "126 1.0372717 1.0578079\n",
      "127 1.0365887 1.0583001\n",
      "128 1.037962 1.0638181\n",
      "129 1.0371612 1.0614007\n",
      "130 1.0381547 1.0634477\n",
      "131 1.0373998 1.0543387\n",
      "132 1.0363337 1.0536928\n",
      "133 1.0324818 1.0544041\n",
      "134 1.0325004 1.0540199\n",
      "135 1.0317067 1.0525987\n",
      "136 1.0476544 1.0744202\n",
      "137 1.0431668 1.0566387\n",
      "138 1.0341094 1.0558298\n",
      "139 1.0320289 1.0555232\n",
      "140 1.0292437 1.0525061\n",
      "141 1.038836 1.0676209\n",
      "142 1.0314503 1.0602474\n",
      "143 1.0368687 1.0570575\n",
      "144 1.0409735 1.0622289\n",
      "145 1.0352485 1.0574659\n",
      "146 1.0325887 1.0554416\n",
      "147 1.0313618 1.0559754\n",
      "148 1.0282234 1.0487685\n",
      "149 1.0279622 1.0549088\n",
      "150 1.0381163 1.0701393\n",
      "151 1.0387588 1.0589942\n",
      "152 1.0339996 1.0616449\n",
      "153 1.0246339 1.0519865\n",
      "154 1.0224886 1.0492584\n",
      "155 1.0202076 1.0474818\n",
      "156 1.0231433 1.0501034\n",
      "157 1.030894 1.0606489\n",
      "158 1.0265408 1.0534958\n",
      "159 1.0250454 1.0519116\n",
      "160 1.0247432 1.0486808\n",
      "161 1.0357136 1.0658371\n",
      "162 1.0267341 1.0491089\n",
      "163 1.0206177 1.0457557\n",
      "164 1.0209869 1.0454457\n",
      "165 1.0177904 1.0430105\n",
      "166 1.0160972 1.0440736\n",
      "167 1.0437802 1.0636781\n",
      "168 1.0203762 1.0492707\n",
      "169 1.0183799 1.049849\n",
      "170 1.014587 1.0433687\n",
      "171 1.0162636 1.0450647\n",
      "172 1.0132073 1.0453748\n",
      "173 1.0108335 1.0430425\n",
      "174 1.011349 1.0453434\n",
      "175 1.0160878 1.0423824\n",
      "176 1.0235407 1.0484822\n",
      "177 1.0193299 1.0429221\n",
      "178 1.0120207 1.0398508\n",
      "179 1.0115641 1.0416892\n",
      "180 1.0115004 1.0440875\n",
      "181 1.0142508 1.0490592\n",
      "182 1.0228117 1.0515119\n",
      "183 1.0102032 1.0406952\n",
      "184 1.0066365 1.0370843\n",
      "185 1.0074013 1.0416746\n",
      "186 1.0047197 1.0364718\n",
      "187 1.0076013 1.0366888\n",
      "188 1.0033025 1.0412335\n",
      "189 1.0051532 1.0410788\n",
      "190 1.0063481 1.0369524\n",
      "191 1.0076764 1.0379132\n",
      "192 1.0019962 1.034658\n",
      "193 1.003373 1.0377194\n",
      "194 1.0016433 1.037987\n",
      "195 1.0034124 1.0373014\n",
      "196 0.998908 1.0351096\n",
      "197 0.9978952 1.0327462\n",
      "198 0.997438 1.0357157\n",
      "199 0.9969897 1.0359102\n",
      "200 0.99696875 1.0385635\n",
      "201 0.994785 1.0312626\n",
      "202 0.99478006 1.0338491\n",
      "203 0.9948197 1.0341271\n",
      "204 0.99293524 1.0356824\n",
      "205 1.0312872 1.0536456\n",
      "206 1.0491439 1.0571519\n",
      "207 1.0366371 1.0705899\n",
      "208 1.0343428 1.0591576\n",
      "209 1.0141231 1.0505296\n",
      "210 1.0056 1.0434884\n",
      "211 1.0090866 1.0434277\n",
      "best epoch: 201 \n",
      " valid RMSE: 1.0312626\n",
      "512 Malaria\n",
      "8001 999 999\n",
      "0 1.7115978 1.8295516\n",
      "1 1.1882956 1.3075187\n",
      "2 1.1875048 1.3008505\n",
      "3 1.1869615 1.2964307\n",
      "4 1.1791412 1.290895\n",
      "5 1.174988 1.2863535\n",
      "6 1.177665 1.2909877\n",
      "7 1.1648512 1.2719908\n",
      "8 1.1599946 1.263752\n",
      "9 1.1509051 1.2484131\n",
      "10 1.1860217 1.2865963\n",
      "11 1.1475409 1.2453296\n",
      "12 1.1381758 1.229787\n",
      "13 1.1326358 1.2206274\n",
      "14 1.1762953 1.2657707\n",
      "15 1.1387429 1.22125\n",
      "16 1.1343815 1.2127761\n",
      "17 1.1271485 1.2099186\n",
      "18 1.1242033 1.2049856\n",
      "19 1.122042 1.1999345\n",
      "20 1.117537 1.195784\n",
      "21 1.1240066 1.198305\n",
      "22 1.1170737 1.1946697\n",
      "23 1.1143335 1.1920904\n",
      "24 1.1159408 1.1949698\n",
      "25 1.1157151 1.1913218\n",
      "26 1.1085039 1.1866813\n",
      "27 1.1068923 1.1857196\n",
      "28 1.1051046 1.1840266\n",
      "29 1.102187 1.181488\n",
      "30 1.1167452 1.190917\n",
      "31 1.0993506 1.1805339\n",
      "32 1.0978196 1.177217\n",
      "33 1.094513 1.1759133\n",
      "34 1.096851 1.1763141\n",
      "35 1.1107455 1.1868358\n",
      "36 1.0934933 1.1745945\n",
      "37 1.0878608 1.1722949\n",
      "38 1.0906806 1.1779357\n",
      "39 1.1197103 1.1963513\n",
      "40 1.087799 1.1740638\n",
      "41 1.0852108 1.1762146\n",
      "42 1.085624 1.176878\n",
      "43 1.0873353 1.1748646\n",
      "44 1.0885977 1.1751535\n",
      "45 1.0783457 1.1695397\n",
      "46 1.0861742 1.1740756\n",
      "47 1.0966499 1.1828018\n",
      "48 1.0802083 1.173162\n",
      "49 1.0764208 1.1673499\n",
      "50 1.0829489 1.1745055\n",
      "51 1.0837455 1.1716172\n",
      "52 1.0750781 1.1671306\n",
      "53 1.0828252 1.1713501\n",
      "54 1.0801336 1.1697444\n",
      "55 1.0760773 1.1687971\n",
      "56 1.0732628 1.1684142\n",
      "57 1.0711179 1.1657113\n",
      "58 1.0698307 1.1651855\n",
      "59 1.0684618 1.1638559\n",
      "60 1.0681629 1.1637194\n",
      "61 1.0705767 1.1686996\n",
      "62 1.0721427 1.1657633\n",
      "63 1.0675884 1.1613218\n",
      "64 1.0727253 1.1635355\n",
      "65 1.065619 1.1591605\n",
      "66 1.0839278 1.1727256\n",
      "67 1.0747447 1.1696649\n",
      "68 1.066976 1.1616672\n",
      "69 1.064159 1.1594857\n",
      "70 1.0671422 1.1600661\n",
      "71 1.0644736 1.160839\n",
      "72 1.0620939 1.1560677\n",
      "73 1.0605396 1.154636\n",
      "74 1.058085 1.1541897\n",
      "75 1.0649894 1.1651026\n",
      "76 1.0564736 1.1567189\n",
      "77 1.0622846 1.16461\n",
      "78 1.0571722 1.1555043\n",
      "79 1.054632 1.1547563\n",
      "80 1.055844 1.1578237\n",
      "81 1.0525823 1.152762\n",
      "82 1.0524644 1.155895\n",
      "83 1.052383 1.1565455\n",
      "84 1.0542119 1.1593837\n",
      "85 1.0534937 1.1530042\n",
      "86 1.0512866 1.1509295\n",
      "87 1.0469694 1.1498328\n",
      "88 1.0470159 1.1526724\n",
      "89 1.0507225 1.1513565\n",
      "90 1.0506368 1.1499226\n",
      "91 1.0489712 1.1562483\n",
      "92 1.0526803 1.1520436\n",
      "93 1.0470415 1.1467731\n",
      "94 1.0436788 1.1512089\n",
      "95 1.0409032 1.148101\n",
      "96 1.0819615 1.1738813\n",
      "97 1.0451624 1.1507207\n",
      "98 1.0431644 1.1487845\n",
      "99 1.0608865 1.1605031\n",
      "100 1.0457957 1.1447484\n",
      "101 1.0415651 1.1435117\n",
      "102 1.0394505 1.1462843\n",
      "103 1.0371604 1.1451416\n",
      "104 1.0370034 1.1425406\n",
      "105 1.0861578 1.2010828\n",
      "106 1.0398906 1.145192\n",
      "107 1.0365496 1.1447444\n",
      "108 1.0346297 1.1462601\n",
      "109 1.0331454 1.1447186\n",
      "110 1.0315726 1.1431167\n",
      "111 1.0311946 1.1415371\n",
      "112 1.0302943 1.1421437\n",
      "113 1.0671626 1.1694336\n",
      "114 1.0430068 1.1433765\n",
      "115 1.0343087 1.1400554\n",
      "116 1.0363058 1.1425843\n",
      "117 1.034006 1.1494342\n",
      "118 1.0305066 1.1455559\n",
      "119 1.0287335 1.1403157\n",
      "120 1.0286676 1.1422809\n",
      "121 1.0673113 1.1686413\n",
      "122 1.0353101 1.1453106\n",
      "123 1.0412621 1.1546222\n",
      "124 1.0392233 1.146555\n",
      "125 1.0290892 1.1380011\n",
      "126 1.0264788 1.1376299\n",
      "127 1.0327996 1.1410972\n",
      "128 1.0262471 1.1362079\n",
      "129 1.0230625 1.1351693\n",
      "130 1.0351204 1.1421188\n",
      "131 1.0390261 1.1568154\n",
      "132 1.0294721 1.1470453\n",
      "133 1.0246046 1.1408564\n",
      "134 1.0334433 1.15539\n",
      "135 1.0240877 1.1432486\n",
      "136 1.0253259 1.1412004\n",
      "137 1.0243732 1.1458985\n",
      "138 1.0217527 1.1426284\n",
      "139 1.0181074 1.1377864\n",
      "best epoch: 129 \n",
      " valid RMSE: 1.1351693\n",
      "1024 Malaria\n",
      "8001 999 999\n",
      "0 1.2145813 1.2691567\n",
      "1 1.2083875 1.2621672\n",
      "2 1.1998264 1.2542977\n",
      "3 1.218575 1.2727723\n",
      "4 1.2005917 1.2541693\n",
      "5 1.1913291 1.2443229\n",
      "6 1.1889049 1.2418404\n",
      "7 1.1813453 1.2334931\n",
      "8 1.1809925 1.2310011\n",
      "9 1.1725948 1.2204287\n",
      "10 1.1888565 1.2368608\n",
      "11 1.1675819 1.2164165\n",
      "12 1.1551716 1.20399\n",
      "13 1.1462932 1.1952679\n",
      "14 1.1651973 1.2085489\n",
      "15 1.1449685 1.1904496\n",
      "16 1.1535438 1.2038525\n",
      "17 1.1364717 1.1839209\n",
      "18 1.1232344 1.1697147\n",
      "19 1.1200178 1.1659046\n",
      "20 1.117247 1.1641126\n",
      "21 1.1258293 1.172386\n",
      "22 1.1275048 1.175248\n",
      "23 1.1144736 1.1606488\n",
      "24 1.1125945 1.1588547\n",
      "25 1.1106806 1.1590849\n",
      "26 1.1098843 1.1568508\n",
      "27 1.10741 1.1552706\n",
      "28 1.1088557 1.1558627\n",
      "29 1.1045479 1.1512063\n",
      "30 1.1380868 1.1904899\n",
      "31 1.1067256 1.154689\n",
      "32 1.1047875 1.1540124\n",
      "33 1.1011629 1.1489234\n",
      "34 1.1112456 1.1609596\n",
      "35 1.1076146 1.1575357\n",
      "36 1.1004325 1.1496153\n",
      "37 1.09687 1.1446223\n",
      "38 1.0959808 1.1418984\n",
      "39 1.107981 1.1581895\n",
      "40 1.0968602 1.1455003\n",
      "41 1.0944543 1.1412791\n",
      "42 1.0944148 1.1434447\n",
      "43 1.0940455 1.1421039\n",
      "44 1.0914483 1.1399139\n",
      "45 1.0903007 1.1385001\n",
      "46 1.1020547 1.1505005\n",
      "47 1.1222894 1.1711657\n",
      "48 1.100554 1.1519258\n",
      "49 1.1085352 1.1611006\n",
      "50 1.1032907 1.1549202\n",
      "51 1.1001219 1.1544166\n",
      "52 1.0867348 1.139779\n",
      "53 1.1007105 1.1570089\n",
      "54 1.0879961 1.1398755\n",
      "55 1.0875553 1.1404804\n",
      "56 1.0834935 1.137022\n",
      "57 1.0814573 1.1345826\n",
      "58 1.0800705 1.1340362\n",
      "59 1.0817255 1.1368725\n",
      "60 1.0810633 1.1304117\n",
      "61 1.0802767 1.1325814\n",
      "62 1.0776285 1.1274909\n",
      "63 1.0766771 1.1280419\n",
      "64 1.0782735 1.1325442\n",
      "65 1.0754837 1.1280072\n",
      "66 1.1253829 1.1938772\n",
      "67 1.0826261 1.1308748\n",
      "68 1.0776709 1.1294314\n",
      "69 1.0757098 1.1311514\n",
      "70 1.0765668 1.1306087\n",
      "71 1.1014967 1.1596818\n",
      "72 1.0797985 1.1316818\n",
      "73 1.0745217 1.1263093\n",
      "74 1.0713005 1.1247743\n",
      "75 1.107228 1.1549472\n",
      "76 1.1015401 1.1492088\n",
      "77 1.080273 1.1281435\n",
      "78 1.0814039 1.132699\n",
      "79 1.0747391 1.1262354\n",
      "80 1.0753988 1.1274614\n",
      "81 1.0717037 1.124725\n",
      "82 1.0704716 1.1243353\n",
      "83 1.0694594 1.1239417\n",
      "84 1.0682522 1.1234233\n",
      "85 1.0730329 1.1290604\n",
      "86 1.066253 1.1196947\n",
      "87 1.0652572 1.1205497\n",
      "88 1.0659053 1.1217483\n",
      "89 1.0659673 1.1207803\n",
      "90 1.0659002 1.1246532\n",
      "91 1.0622699 1.1175005\n",
      "92 1.0618182 1.1182265\n",
      "93 1.0610228 1.1175638\n",
      "94 1.0623257 1.116717\n",
      "95 1.0628285 1.1193141\n",
      "96 1.069862 1.1281701\n",
      "97 1.0626487 1.1182008\n",
      "98 1.0580264 1.1136627\n",
      "99 1.0789766 1.1394387\n",
      "100 1.060154 1.1193551\n",
      "101 1.0568161 1.1141984\n",
      "102 1.0577976 1.1144525\n",
      "103 1.0553263 1.1120744\n",
      "104 1.0569154 1.1121694\n",
      "105 1.0890741 1.1434957\n",
      "106 1.059107 1.1178954\n",
      "107 1.0690415 1.1294336\n",
      "108 1.0593413 1.1179485\n",
      "109 1.0575166 1.115721\n",
      "110 1.0540891 1.1175297\n",
      "111 1.0520494 1.1139399\n",
      "112 1.0538443 1.1095785\n",
      "113 1.0745634 1.1376622\n",
      "114 1.0631952 1.1229035\n",
      "115 1.0535662 1.1146828\n",
      "116 1.0517066 1.1130656\n",
      "117 1.053216 1.1107632\n",
      "118 1.0498468 1.1088947\n",
      "119 1.0539355 1.1218402\n",
      "120 1.0519127 1.1158041\n",
      "121 1.0476389 1.1136432\n",
      "122 1.0511767 1.1135526\n",
      "123 1.0626646 1.1263419\n",
      "124 1.0517083 1.1162975\n",
      "125 1.0471812 1.1108723\n",
      "126 1.0449169 1.109795\n",
      "127 1.0439117 1.1076587\n",
      "128 1.046102 1.1140155\n",
      "129 1.0444081 1.1098258\n",
      "130 1.0521559 1.1167717\n",
      "131 1.049298 1.1091723\n",
      "132 1.0474358 1.1039308\n",
      "133 1.0446041 1.1023644\n",
      "134 1.0461714 1.1052759\n",
      "135 1.0437106 1.1074638\n",
      "136 1.0429771 1.1033058\n",
      "137 1.0399158 1.101546\n",
      "138 1.0386544 1.1023653\n",
      "139 1.0396336 1.104731\n",
      "140 1.0386182 1.1061218\n",
      "141 1.0371277 1.1042186\n",
      "142 1.0474645 1.1192098\n",
      "143 1.0757118 1.139525\n",
      "144 1.0538119 1.1178812\n",
      "145 1.0483419 1.1128439\n",
      "146 1.0491965 1.1245013\n",
      "147 1.0434613 1.1089078\n",
      "best epoch: 137 \n",
      " valid RMSE: 1.101546\n",
      "2048 Malaria\n",
      "8001 999 999\n",
      "0 1.639766 1.7284669\n",
      "1 1.1964991 1.2924052\n",
      "2 1.1931121 1.2900391\n",
      "3 1.196152 1.2917336\n",
      "4 1.1869729 1.2862201\n",
      "5 1.1868504 1.2871922\n",
      "6 1.1820263 1.2827812\n",
      "7 1.1811064 1.2776906\n",
      "8 1.1745095 1.2736651\n",
      "9 1.1692971 1.2667136\n",
      "10 1.1779836 1.2790521\n",
      "11 1.1635461 1.2596501\n",
      "12 1.154853 1.2517886\n",
      "13 1.147119 1.2437813\n",
      "14 1.180096 1.275594\n",
      "15 1.1481718 1.2407631\n",
      "16 1.1409087 1.2330174\n",
      "17 1.1343274 1.2268629\n",
      "18 1.13238 1.2251394\n",
      "19 1.1267122 1.2156228\n",
      "20 1.1212835 1.2111446\n",
      "21 1.127952 1.2159188\n",
      "22 1.1213772 1.2090434\n",
      "23 1.1159581 1.2042421\n",
      "24 1.1189008 1.2079282\n",
      "25 1.1233294 1.2106308\n",
      "26 1.1116371 1.2009449\n",
      "27 1.1127098 1.2037761\n",
      "28 1.1092656 1.1982118\n",
      "29 1.1061865 1.197096\n",
      "30 1.1238123 1.211781\n",
      "31 1.1099598 1.198374\n",
      "32 1.1044598 1.196183\n",
      "33 1.1023613 1.1945678\n",
      "34 1.1147736 1.20816\n",
      "35 1.1031432 1.1899178\n",
      "36 1.0987678 1.1887599\n",
      "37 1.0958136 1.1900132\n",
      "38 1.0942144 1.1901643\n",
      "39 1.1029321 1.1952882\n",
      "40 1.0929687 1.1894199\n",
      "41 1.0915309 1.1874142\n",
      "42 1.0951692 1.1923504\n",
      "43 1.102718 1.199125\n",
      "44 1.0972257 1.1965979\n",
      "45 1.0899489 1.189418\n",
      "46 1.0952812 1.1921713\n",
      "47 1.0905784 1.1867832\n",
      "48 1.0937998 1.1918013\n",
      "49 1.0900022 1.1863774\n",
      "50 1.0953453 1.1934439\n",
      "51 1.0914147 1.1855007\n",
      "52 1.0863259 1.1835264\n",
      "53 1.0935106 1.1929771\n",
      "54 1.0899283 1.1897938\n",
      "55 1.0868939 1.1832829\n",
      "56 1.082296 1.1806033\n",
      "57 1.0804162 1.1789929\n",
      "58 1.0809288 1.1808553\n",
      "59 1.079844 1.1777695\n",
      "60 1.0776429 1.1785258\n",
      "61 1.0768543 1.1798073\n",
      "62 1.0789849 1.1790746\n",
      "63 1.0746113 1.1791571\n",
      "64 1.0790144 1.176715\n",
      "65 1.0729227 1.1727157\n",
      "66 1.1043352 1.2187862\n",
      "67 1.0830071 1.1838882\n",
      "68 1.0776379 1.179352\n",
      "69 1.0765346 1.1811522\n",
      "70 1.0771304 1.1818246\n",
      "71 1.0738518 1.1811227\n",
      "72 1.0810194 1.1787335\n",
      "73 1.0718844 1.1737951\n",
      "74 1.068583 1.1740848\n",
      "75 1.070558 1.17431\n",
      "76 1.067491 1.1689587\n",
      "77 1.0665762 1.1692802\n",
      "78 1.0686926 1.1712089\n",
      "79 1.0657953 1.1682228\n",
      "80 1.066525 1.1712171\n",
      "81 1.0650728 1.1686441\n",
      "82 1.0639584 1.1705486\n",
      "83 1.0618471 1.1661485\n",
      "84 1.0662766 1.1697628\n",
      "85 1.0631917 1.169371\n",
      "86 1.0595142 1.1660001\n",
      "87 1.0589845 1.1639677\n",
      "88 1.0588273 1.1648544\n",
      "89 1.057789 1.1626065\n",
      "90 1.0699587 1.1801205\n",
      "91 1.0560572 1.1621265\n",
      "92 1.0549258 1.1638051\n",
      "93 1.0541046 1.16525\n",
      "94 1.0542287 1.161439\n",
      "95 1.0540128 1.1632425\n",
      "96 1.0665247 1.178709\n",
      "97 1.0587662 1.1717033\n",
      "98 1.0591601 1.1680584\n",
      "99 1.0779153 1.1860015\n",
      "100 1.0554001 1.1695731\n",
      "101 1.0505757 1.1630551\n",
      "102 1.0521653 1.161313\n",
      "103 1.0477777 1.1589848\n",
      "104 1.048408 1.1577083\n",
      "105 1.0833921 1.194725\n",
      "106 1.04849 1.1577804\n",
      "107 1.0466348 1.1560985\n",
      "108 1.0478358 1.157668\n",
      "109 1.0453466 1.1552582\n",
      "110 1.0433077 1.1536555\n",
      "111 1.0450275 1.1563929\n",
      "112 1.0443561 1.1508801\n",
      "113 1.0788339 1.1927345\n",
      "114 1.0597739 1.1730407\n",
      "115 1.0474883 1.1593397\n",
      "116 1.0449678 1.1543984\n",
      "117 1.0453653 1.1539829\n",
      "118 1.0407132 1.1526096\n",
      "119 1.0452976 1.1560531\n",
      "120 1.0428921 1.1547017\n",
      "121 1.0690485 1.1822983\n",
      "122 1.0558783 1.1682326\n",
      "best epoch: 112 \n",
      " valid RMSE: 1.1508801\n",
      "4096 Malaria\n",
      "8001 999 999\n",
      "0 1.2298399 1.1902158\n",
      "1 1.2219297 1.1839061\n",
      "2 1.2272488 1.1995555\n",
      "3 1.2240317 1.1974095\n",
      "4 1.2108781 1.1790218\n",
      "5 1.2031103 1.1677653\n",
      "6 1.2056869 1.1668168\n",
      "7 1.1884539 1.1551273\n",
      "8 1.1808176 1.1486207\n",
      "9 1.1693242 1.1400135\n",
      "10 1.2594798 1.2162793\n",
      "11 1.1632304 1.1326506\n",
      "12 1.1502781 1.1249896\n",
      "13 1.1454543 1.1171333\n",
      "14 1.1654897 1.1327722\n",
      "15 1.1465954 1.1179466\n",
      "16 1.1327336 1.1109037\n",
      "17 1.137562 1.1220815\n",
      "18 1.1298475 1.112548\n",
      "19 1.1267859 1.1105582\n",
      "20 1.1233466 1.1069272\n",
      "21 1.1321509 1.1200404\n",
      "22 1.1191052 1.1033814\n",
      "23 1.1164873 1.1002957\n",
      "24 1.11493 1.097715\n",
      "25 1.1166958 1.1047113\n",
      "26 1.1128832 1.0970896\n",
      "27 1.1102582 1.0957322\n",
      "28 1.1089664 1.0949394\n",
      "29 1.1067833 1.092931\n",
      "30 1.1265708 1.12037\n",
      "31 1.108419 1.0942302\n",
      "32 1.1056083 1.0937234\n",
      "33 1.1029533 1.0889819\n",
      "34 1.1077142 1.098535\n",
      "35 1.1041019 1.0940886\n",
      "36 1.1020328 1.0927502\n",
      "37 1.0998771 1.0906459\n",
      "38 1.0974216 1.0853405\n",
      "39 1.1120098 1.1066738\n",
      "40 1.1017895 1.0889952\n",
      "41 1.0961686 1.0883112\n",
      "42 1.0946931 1.083866\n",
      "43 1.1088085 1.1068281\n",
      "44 1.1001576 1.0949872\n",
      "45 1.0966054 1.0853357\n",
      "46 1.0991102 1.0949162\n",
      "47 1.0922014 1.0857502\n",
      "48 1.0906906 1.0837092\n",
      "49 1.0957328 1.0911342\n",
      "50 1.1061169 1.0920148\n",
      "51 1.096293 1.0925424\n",
      "52 1.0881641 1.0819539\n",
      "53 1.0915108 1.0886288\n",
      "54 1.0929857 1.0897901\n",
      "55 1.0898324 1.0828156\n",
      "56 1.087459 1.0808043\n",
      "57 1.0850502 1.079074\n",
      "58 1.0836414 1.0783174\n",
      "59 1.0823106 1.0778679\n",
      "60 1.0832859 1.0804675\n",
      "61 1.0864952 1.0777801\n",
      "62 1.085054 1.0802566\n",
      "63 1.0804309 1.0745729\n",
      "64 1.08607 1.0859046\n",
      "65 1.0778583 1.0741271\n",
      "66 1.1361512 1.144492\n",
      "67 1.1042978 1.0877291\n",
      "68 1.0882635 1.0755051\n",
      "69 1.0844712 1.0731288\n",
      "70 1.0858538 1.0793291\n",
      "71 1.0865065 1.0824708\n",
      "72 1.085097 1.0827335\n",
      "73 1.0794413 1.075392\n",
      "74 1.0767286 1.0718217\n",
      "75 1.0759745 1.069766\n",
      "76 1.0749171 1.0693604\n",
      "77 1.0747918 1.0723721\n",
      "78 1.0759968 1.0753325\n",
      "79 1.0744289 1.0679582\n",
      "80 1.0726457 1.0685323\n",
      "81 1.0726676 1.0679173\n",
      "82 1.0733606 1.0729923\n",
      "83 1.070553 1.0670741\n",
      "84 1.0689733 1.0664585\n",
      "85 1.0732336 1.076046\n",
      "86 1.0696312 1.0694032\n",
      "87 1.0685463 1.0709754\n",
      "88 1.0665735 1.0665977\n",
      "89 1.0661824 1.0671144\n",
      "90 1.065977 1.0709386\n",
      "91 1.0655365 1.0667554\n",
      "92 1.0638592 1.0676568\n",
      "93 1.0647386 1.0649073\n",
      "94 1.0637382 1.067952\n",
      "95 1.0635461 1.0682489\n",
      "96 1.0702099 1.0778859\n",
      "97 1.0614789 1.0660337\n",
      "98 1.0604167 1.0644957\n",
      "99 1.0747427 1.0859048\n",
      "100 1.0619531 1.0668468\n",
      "101 1.0610131 1.0651741\n",
      "102 1.0609908 1.0627086\n",
      "103 1.0586352 1.0628831\n",
      "104 1.0600774 1.067313\n",
      "105 1.0796713 1.0774461\n",
      "106 1.0643864 1.071872\n",
      "107 1.0596699 1.0637034\n",
      "108 1.0563618 1.0626886\n",
      "109 1.0559278 1.0618715\n",
      "110 1.0638599 1.0667188\n",
      "111 1.0548909 1.0634933\n",
      "112 1.054466 1.0622376\n",
      "113 1.0645534 1.0764381\n",
      "114 1.065988 1.0672367\n",
      "115 1.0564744 1.0595673\n",
      "116 1.0608724 1.0714378\n",
      "117 1.0530796 1.0580204\n",
      "118 1.0592136 1.0706931\n",
      "119 1.0537492 1.0630445\n",
      "120 1.0551943 1.0613586\n",
      "121 1.0637419 1.0786355\n",
      "122 1.0540416 1.0611774\n",
      "123 1.0486993 1.0573819\n",
      "124 1.0500739 1.0566334\n",
      "125 1.0490625 1.0558527\n",
      "126 1.0468241 1.0552896\n",
      "127 1.0465593 1.056368\n",
      "128 1.0515846 1.0658377\n",
      "129 1.0453018 1.056675\n",
      "130 1.043805 1.0553566\n",
      "131 1.0452182 1.0547719\n",
      "132 1.0464361 1.0622449\n",
      "133 1.0454783 1.0579536\n",
      "134 1.0468775 1.0550033\n",
      "135 1.0418439 1.0527015\n",
      "136 1.0430481 1.0553075\n",
      "137 1.0677031 1.0712901\n",
      "138 1.0570512 1.0555583\n",
      "139 1.0461012 1.0510991\n",
      "140 1.0443048 1.0532646\n",
      "141 1.0610458 1.0783497\n",
      "142 1.0448157 1.055694\n",
      "143 1.0484608 1.0582703\n",
      "144 1.0467595 1.0591786\n",
      "145 1.0442405 1.0594027\n",
      "146 1.0390586 1.0531678\n",
      "147 1.0459996 1.0648578\n",
      "148 1.0447528 1.0550913\n",
      "149 1.0409762 1.0526749\n",
      "best epoch: 139 \n",
      " valid RMSE: 1.0510991\n"
     ]
    }
   ],
   "source": [
    "remained_df = remained_df.reset_index(drop=True)\n",
    "\n",
    "file_path = \"/raid/shenwanxiang/08_Robustness/dataset_induces/split\"\n",
    "random_seeds = [2, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "task_name = 'Malaria'\n",
    "\n",
    "\n",
    "for seed in random_seeds:\n",
    "    \n",
    "    train_path = os.path.join(file_path, task_name,\"%s\" % seed, \"train.csv\")\n",
    "    valid_path = os.path.join(file_path, task_name,\"%s\" % seed, \"val.csv\")\n",
    "    test_path = os.path.join(file_path, task_name,\"%s\" % seed, \"test.csv\")\n",
    "\n",
    "    pred_valid_path = os.path.join(file_path, task_name,\"%s\" % seed, \"attfp_pred_val.csv\")\n",
    "    pred_test_path = os.path.join(file_path, task_name,\"%s\" % seed, \"attfp_pred_test.csv\")\n",
    "    \n",
    "    saved_valid_path = os.path.join(file_path, task_name,\"%s\" % seed, \"attfp_saved_val.csv\")\n",
    "    saved_test_path = os.path.join(file_path, task_name,\"%s\" % seed, \"attfp_saved_test.csv\")\n",
    "    \n",
    "    \n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_valid = pd.read_csv(valid_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    print(seed, task_name)\n",
    "    if (os.path.exists(pred_test_path)) & (os.path.exists(pred_valid_path)):\n",
    "        continue\n",
    "    test_df = remained_df[remained_df.smiles.isin(df_test.smiles)].reset_index(drop=True)\n",
    "    valid_df = remained_df[remained_df.smiles.isin(df_valid.smiles)].reset_index(drop=True)\n",
    "    train_df = remained_df[remained_df.smiles.isin(df_train.smiles)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print(len(train_df),len(valid_df),len(test_df),)\n",
    "\n",
    "    x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([canonical_smiles_list[0]],feature_dicts)\n",
    "    num_atom_features = x_atom.shape[-1]\n",
    "    num_bond_features = x_bonds.shape[-1]\n",
    "    loss_function = nn.MSELoss()\n",
    "    model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "                fingerprint_dim, output_units_num, p_dropout)\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "    best_param ={}\n",
    "    best_param[\"train_epoch\"] = 0\n",
    "    best_param[\"valid_epoch\"] = 0\n",
    "    best_param[\"train_MSE\"] = 9e8\n",
    "    best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "    for epoch in range(800):\n",
    "        train_MAE, train_MSE = eval(model, train_df)\n",
    "        valid_MAE, valid_MSE = eval(model, valid_df)\n",
    "        if train_MSE < best_param[\"train_MSE\"]:\n",
    "            best_param[\"train_epoch\"] = epoch\n",
    "            best_param[\"train_MSE\"] = train_MSE\n",
    "        if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "            best_param[\"valid_epoch\"] = epoch\n",
    "            best_param[\"valid_MSE\"] = valid_MSE\n",
    "            if valid_MSE < 2:\n",
    "                 torch.save(model, '/raid/shenwanxiang/08_Robustness/saved_models/model_%s' % task_name +prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "        if (epoch - best_param[\"valid_epoch\"]) > 18:        \n",
    "            break\n",
    "        print(epoch, np.sqrt(train_MSE), np.sqrt(valid_MSE))\n",
    "\n",
    "        train(model, train_df, optimizer, loss_function)\n",
    "\n",
    "    # evaluate model\n",
    "    best_model = torch.load('/raid/shenwanxiang/08_Robustness/saved_models/model_%s' % task_name+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "    best_model_dict = best_model.state_dict()\n",
    "    best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    (best_model.align[0].weight == model.align[0].weight).all()\n",
    "    _, valid_MSE = eval(model, valid_df)\n",
    "    print(\"best epoch:\",best_param[\"valid_epoch\"],\"\\n\",\"valid RMSE:\",np.sqrt(valid_MSE))\n",
    "\n",
    "    pred_test = predict(model, test_df)\n",
    "    pd.DataFrame(pred_test, index = test_df['smiles'],\n",
    "                 columns = [tasks]).to_csv(pred_test_path)\n",
    "\n",
    "    pd.DataFrame(test_df[tasks].values, index = test_df['smiles'],\n",
    "                 columns = [tasks]).to_csv(saved_test_path)\n",
    "\n",
    "    pred_valid = predict(best_model, valid_df)\n",
    "    pd.DataFrame(pred_valid, index = valid_df['smiles'], \n",
    "                 columns = [tasks]).to_csv(pred_valid_path)\n",
    "\n",
    "    pd.DataFrame(valid_df[tasks].values, index = valid_df['smiles'],\n",
    "                 columns = [tasks]).to_csv(saved_valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_gpu5_run_Malaria.ipynb  attentivefp.random.final.csv  ZZZ_eva_rd5fcv.ipynb\n",
      "06_gpu7_HIV.ipynb          \u001b[0m\u001b[01;34mfinished\u001b[0m/\n",
      "11_gpu4_ToxCast.ipynb      \u001b[01;34mpending\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
